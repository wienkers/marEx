{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Daily SST Analysis: Identifying Marine Extremes with `MarEx-Detect`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d21965",
   "metadata": {},
   "source": [
    "### `MarEx-Detect` Processing Pipeline:\n",
    "\n",
    "1. **Anomaly Generation**\n",
    "   - Removes polynomial trends (user-configurable orders)\n",
    "   - Eliminates seasonal cycle via annual and semi-annual harmonics\n",
    "   - Optionally standardises by day-of-year temporal variability\n",
    "\n",
    "2. **Extreme Event Identification**\n",
    "   - Computes adaptive local thresholds using percentile-based approach\n",
    "   - Creates boolean masks identifying extreme events\n",
    "   - Uses histogram-based approximation for efficiency on large datasets\n",
    "\n",
    "3. **Results Assembly**\n",
    "   - Attaches spatial metadata (connectivity, cell areas) if provided\n",
    "   - Optimises chunking for subsequent analyses\n",
    "\n",
    "The pipeline leverages **dask** for distributed parallel computation and **flox** for optimised groupby operations, enabling efficient processing of large datasets. \\\n",
    "A 40-year global daily analysis at 0.25Â° resolution completes in ~2 minutes on 128 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "import intake\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import marEx\n",
    "import marEx.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.start_local_cluster(n_workers=64, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 40 years of Daily ICON data (ref. EERIE project)\n",
    "\n",
    "cat = intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "expid = 'eerie-control-1950'\n",
    "version = 'v20240618'\n",
    "model = 'icon-esm-er'\n",
    "gridspec = 'gr025'\n",
    "\n",
    "dat = cat['dkrz.disk.model-output'][model][expid][version]['ocean'][gridspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5631e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal chunk size & load data\n",
    "#   N.B.: This is crucial for dask (not only for performance, but also to make the problem tractable)\n",
    "#         The operations in this package eventually require global-in-time operations,\n",
    "#         therefore, a larger time chunksize is beneficial.\n",
    "\n",
    "time_chunksize = 1000\n",
    "sst = dat['2d_daily_mean'](chunks={}).to_dask().to.isel(depth=0).drop_vars('depth').chunk({'time':time_chunksize, 'lat':'auto', 'lon':'auto'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the chunks are appropriately-sized\n",
    "#  N.B.: The intermediate chunk size is the global-in-time memory footprint\n",
    "#        It is good for each worker to have a few times more memory than this value\n",
    "\n",
    "chunk_shape = sst.data.chunksize\n",
    "intermediate_chunk_shape = (sst.sizes['time'],) + chunk_shape[1:]\n",
    "\n",
    "print(f\"Data Chunking (time, lat, lon): {chunk_shape}\")\n",
    "print(f\"Initial Chunk Size: {np.prod(chunk_shape) * sst.data.dtype.itemsize / (1024**2):.2f} MB\")\n",
    "print(f\"Intermediate Chunk Size: {np.prod(intermediate_chunk_shape) * sst.data.dtype.itemsize / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389349a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data using `MarEx-Detect` helper functions:\n",
    "\n",
    "extremes_ds = marEx.preprocess_data(sst, \n",
    "                                std_normalise = False,        # Don't Standardise the data (this is standard)\n",
    "                                threshold_percentile = 95,    # Use the 95th percentile as the extremes threshold\n",
    "                                detrend_orders = [1, 2],      # Detrend the data using 1st & 2nd order polynomials (in addition to removing the mean & seasonal cycle/sub-cycle)\n",
    "                                dimensions = {'time':'time',\n",
    "                                              'xdim':'lon',\n",
    "                                              'ydim':'lat'},  # Define the dimensions of the data -- if 'ydim' exists, then MarEx-Detect knows this is a gridded dataset\n",
    "                                dask_chunks = {'time': 25})   # Dask chunks for *output* data (this is much smaller than the input chunks because the Tracking/ID is more memory-intensive)\n",
    "extremes_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Extremes Data to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extremes_binary_gridded.zarr'\n",
    "extremes_ds.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
