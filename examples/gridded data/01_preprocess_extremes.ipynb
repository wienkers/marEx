{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Daily SST Analysis: Identifying Marine Extremes with `MarEx-Detect`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d21965",
   "metadata": {},
   "source": [
    "### `MarEx-Detect` Processing Pipeline:\n",
    "\n",
    "1. **Anomaly Generation**\n",
    "   - Removes polynomial trends (user-configurable orders)\n",
    "   - Eliminates seasonal cycle via annual and semi-annual harmonics\n",
    "   - Optionally standardises by day-of-year temporal variability\n",
    "\n",
    "2. **Extreme Event Identification**\n",
    "   - Computes adaptive local thresholds using percentile-based approach\n",
    "   - Creates boolean masks identifying extreme events\n",
    "   - Uses histogram-based approximation for efficiency on large datasets\n",
    "\n",
    "3. **Results Assembly**\n",
    "   - Attaches spatial metadata (connectivity, cell areas) if provided\n",
    "   - Optimises chunking for subsequent analyses\n",
    "\n",
    "The pipeline leverages **dask** for distributed parallel computation and **flox** for optimised groupby operations, enabling efficient processing of large datasets. \\\n",
    "A 40-year global daily analysis at 0.25Â° resolution completes in ~4 minutes on 128 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "import intake\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import marEx\n",
    "import marEx.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lustre Scratch Directory\n",
    "scratch_dir = Path('/scratch') / getuser()[0] / getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a714371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.start_local_cluster(n_workers=32, threads_per_worker=1,\n",
    "                                 scratch_dir = scratch_dir / 'clients')  # Specify temporary scratch directory for dask to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7399531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Distributed Dask Cluster\n",
    "# client = hpc.start_distributed_cluster(n_workers=256, workers_per_node=32, runtime=29, \n",
    "#                                        scratch_dir=scratch_dir / 'clients', account='bk1377')    # Specify temporary scratch directory for dask to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 40 years of Daily ICON data (ref. EERIE project)\n",
    "\n",
    "cat = intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "expid = 'eerie-control-1950'\n",
    "version = 'v20240618'\n",
    "model = 'icon-esm-er'\n",
    "gridspec = 'gr025'\n",
    "\n",
    "dat = cat['dkrz.disk.model-output'][model][expid][version]['ocean'][gridspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5631e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "sst = dat['2d_daily_mean'](chunks={}).to_dask().to.isel(depth=0).drop_vars('depth').sel(time=slice('1991-01-01', '2030-12-31')).isel(lat=slice(100,600), lon=slice(100,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data using `MarEx-Detect` helper functions:\n",
    "\n",
    "extremes_ds = marEx.preprocess_data(sst, \n",
    "                                method_anomaly = 'shifting_baseline', # Anomalies from a rolling climatology using previous window_year years -- more \"correct\", but shortens time series by window_year years\n",
    "                                method_extreme = 'hobday_extreme',    # Local day-of-year specific thresholds with windowing\n",
    "                                threshold_percentile = 95,            # Use the 95th percentile as the extremes threshold\n",
    "                                window_year_baseline = 15, \n",
    "                                smooth_days_baseline = 21,            # Defines the rolling climatology window (15 years) and smoothing window (21 days) for determining the anomalies\n",
    "                                window_days_hobday = 11,              # Defines the window (11 days) of compiled samples collected for the extremes detection\n",
    "                                dimensions = {'time':'time',\n",
    "                                              'xdim':'lon',\n",
    "                                              'ydim':'lat'},  # Define the dimensions of the data -- if 'ydim' exists, then MarEx-Detect knows this is a gridded dataset\n",
    "                                dask_chunks = {'time': 25})   # Dask chunks for *output* data\n",
    "extremes_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Extremes Data to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = scratch_dir / 'mhws' / 'extremes_binary_gridded.zarr'\n",
    "extremes_ds.dat_detrend.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
