{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ec1808",
   "metadata": {},
   "source": [
    "# Global Daily SST Analysis: Identifying Marine Extremes with `MarEx-Detect`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fdcc3",
   "metadata": {},
   "source": [
    "### `MarEx-Detect` Processing Pipeline:\n",
    "\n",
    "1. **Anomaly Generation**\n",
    "   - Removes polynomial trends (user-configurable orders)\n",
    "   - Eliminates seasonal cycle via annual and semi-annual harmonics\n",
    "   - Optionally standardises by day-of-year temporal variability\n",
    "\n",
    "2. **Extreme Event Identification**\n",
    "   - Computes adaptive local thresholds using percentile-based approach\n",
    "   - Creates boolean masks identifying extreme events\n",
    "   - Uses histogram-based approximation for efficiency on large datasets\n",
    "\n",
    "3. **Results Assembly**\n",
    "   - Attaches spatial metadata (connectivity, cell areas) if provided\n",
    "   - Optimises chunking for subsequent analyses\n",
    "\n",
    "The pipeline leverages **dask** for distributed parallel computation and **flox** for optimised groupby operations, enabling efficient processing of large datasets. \\\n",
    "A 40-year global daily analysis at 5km resolution on the _unstructured grid_ (15 million cells) completes in ~10 minutes on 512 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "import intake\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import marEx\n",
    "import marEx.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Distributed Dask Cluster\n",
    "client = hpc.start_distributed_cluster(n_workers=512, workers_per_node=64, runtime=20, node_memory=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 40 years of Daily Native-Grid ICON data (ref. EERIE project)\n",
    "\n",
    "cat = intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "expid = 'eerie-control-1950'\n",
    "version = 'v20240618'\n",
    "model = 'icon-esm-er'\n",
    "gridspec = 'native'\n",
    "\n",
    "dat = cat['dkrz.disk.model-output'][model][expid][version]['ocean'][gridspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal chunk size & load data\n",
    "#   N.B.: This is crucial for dask (not only for performance, but also to make the problem tractable)\n",
    "#         The operations in this package eventually require global-in-time operations,\n",
    "#         therefore, a larger time chunksize is beneficial.\n",
    "\n",
    "time_chunksize = 200\n",
    "sst = dat['2d_daily_mean'](chunks={}).to_dask().to.isel(depth=0).chunk({'time':200, 'ncells':'auto'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11256d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the chunks are appropriately-sized\n",
    "#  N.B.: The intermediate chunk size is the global-in-time memory footprint\n",
    "#        It is good for each worker to have a few times more memory than this value\n",
    "\n",
    "chunk_shape = sst.data.chunksize\n",
    "intermediate_chunk_shape = (sst.sizes['time'],) + chunk_shape[1:]\n",
    "\n",
    "print(f\"Data Chunking (time, ncells): {chunk_shape}\")\n",
    "print(f\"Initial Chunk Size: {np.prod(chunk_shape) * sst.data.dtype.itemsize / (1024**2):.2f} MB\")\n",
    "print(f\"Intermediate Chunk Size: {np.prod(intermediate_chunk_shape) * sst.data.dtype.itemsize / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grid & neighbours\n",
    "\n",
    "grid2d = dat['2d_grid'](chunks={}).to_dask().rename({'cell':'ncells'})\n",
    "neighbours = grid2d.neighbor_cell_index.rename({'clat':'lat', 'clon':'lon'})\n",
    "areas = grid2d.cell_area.rename({'clat':'lat', 'clon':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389349a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data using `MarEx Detect` helper functions:\n",
    "\n",
    "extremes_ds = marEx.preprocess_data(sst, \n",
    "                                std_normalise = False,            # Don't Standardise the data (this is standard)\n",
    "                                threshold_percentile = 95,        # Use the 95th percentile as the extremes threshold\n",
    "                                exact_percentile = False,         # Use a histogram-based method to estimate the percentile value (within 0.025C)\n",
    "                                dask_chunks = {'time': 2},        # Dask chunks for *output* data (this is much smaller than the input chunks because the Tracking/ID is more memory-intensive)\n",
    "                                neighbours = neighbours,          # Pass information about neighbours to be used in subsequent processing\n",
    "                                cell_areas = areas,               # Pass information about each Unstructured Grid's cell area (in metres) to be used in subsequent processing\n",
    "                                dimensions = {'time':'time', \n",
    "                                              'xdim':'ncells'})   # Not specifying 'ydim' tells MarEx-Detect that it is an Unstructured Grid\n",
    "extremes_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extremes_binary_unstruct.zarr'\n",
    "extremes_ds.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
