{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Daily Event Analysis: Marine Heatwave ID & Tracking using `MarEx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb47467",
   "metadata": {},
   "source": [
    "### `MarEx` Processing Pipeline for Unstructured Datasets:\n",
    "\n",
    "1. **Morphological Pre-Processing**\n",
    "    - Performs binary morphological closing using highly-threaded binary dilation matrix operations to fill small spatial holes up to `R_fill` elements in radius \n",
    "    - Executes binary opening to remove isolated small features of order `R_fill`\n",
    "    - Fills gaps in time to maintain event continuity for interruptions up to `T_fill` time steps\n",
    "    - Filters out smallest objects below the `area_filter_quartile` percentile threshold\n",
    "\n",
    "2. **Blob Identification**\n",
    "    - Labels spatially connected components using a highly efficient Unstructured Union-Find (Disjoint Set Union) Clustering Algorithm\n",
    "    - Computes blob properties (area, centroid, boundaries)\n",
    "\n",
    "3. **Temporal Tracking**\n",
    "    - Identifies blob overlaps between consecutive time frames\n",
    "    - Connects objects across time, applying the following criteria for splitting, merging, & persistence:\n",
    "        - Connected objects must overlap by at least fraction `overlap_threshold` of the smaller area\n",
    "        - Merged objects retain their original ID, but partition the child area based on the parent of the _nearest-neighbour_ cell (or centroid distance)\n",
    "\n",
    "4. **Graph Reduction & Finalisation**\n",
    "    - Constructs the complete temporal graph of object evolution through time\n",
    "    - Resolves object connectivity graph using `scipy.sparse.csgraph.connected_components`\n",
    "    - Creates globally unique IDs for each tracked extreme event\n",
    "    - Maps objects into efficient ID-time space for convenient analysis\n",
    "    - Computes comprehensive statistics about the lifecycle of each event\n",
    "\n",
    "The pipeline leverages **dask** for distributed parallel computation, enabling efficient processing of large datasets. \\\n",
    "A 40-year global daily analysis at 5km resolution on the _unstructured grid_ (15 million cells) using 240 cores takes ~40 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b381",
   "metadata": {},
   "source": [
    "#### N.B.: The following `dask` config may be necessary on particular systems:\n",
    "```python\n",
    "dask.config.set({\n",
    "    'distributed.comm.timeouts.connect': '120s',  # Increase from default\n",
    "    'distributed.comm.timeouts.tcp': '240s',      # Double the connection timeout\n",
    "    'distributed.comm.retry.count': 10,           # More retries before giving up\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "\n",
    "import marEx\n",
    "import marEx.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lustre Scratch Directory\n",
    "scratch_dir = Path(\"/scratch\") / getuser()[0] / getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b131e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Distributed Dask Cluster\n",
    "# client_cluster = hpc.start_distributed_cluster(n_workers=2048, workers_per_node=128, runtime=59, node_memory=256,\n",
    "#                                  scratch_dir = scratch_dir / 'clients')  # Specify temporary scratch directory for dask to use\n",
    "client = hpc.start_local_cluster(\n",
    "    n_workers=50, threads_per_worker=1, scratch_dir=scratch_dir / \"clients\"\n",
    ")  # Specify temporary scratch directory for dask to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal chunk size & load data\n",
    "#   N.B.: This is crucial for dask (not only for performance, but also to make the problem tractable)\n",
    "#         The operations are eventually global-in-space, and so requires the spatial dimension to be contiguous/unchunked\n",
    "#         We can adjust the chunk size in time depending on available system memory; however,\n",
    "#         note that the performance of the parallel iterative merging algorithm increases with larger chunks in time.\n",
    "\n",
    "chunk_size = {\"time\": 4, \"ncells\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = scratch_dir / \"mhws\" / \"extremes_binary_unstruct_shifting_hobday.zarr\"\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size).isel(time=slice(0, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID, Tracking, & Merging\n",
    "\n",
    "tracker = marEx.tracker(\n",
    "    ds.extreme_events,\n",
    "    ds.mask,\n",
    "    area_filter_quartile=0.8,  # Remove the smallest 80% of the identified coherent extreme areas. N.B.: With increasing resolution, the filter quartile should be increased.\n",
    "    R_fill=32,  # Fill small holes with radius < 32 elements, i.e. ~100 km,\n",
    "    T_fill=2,  # Allow gaps of 2 days and still continue the event tracking with the same ID\n",
    "    allow_merging=True,  # Allow extreme events to split/merge. Keeps track of merge events & unique IDs.\n",
    "    overlap_threshold=0.5,  # Overlap threshold for merging events. If overlap < threshold, events keep independent IDs.\n",
    "    nn_partitioning=True,  # Use new NN method to partition merged children areas. If False, reverts to old method of Di Sun et al. 2023.\n",
    "    temp_dir=str(\n",
    "        scratch_dir / \"mhws\" / \"TEMP/\"\n",
    "    ),  # Temporary Scratch Directory needed for Dask\n",
    "    checkpoint=\"save\",  # Make checkpoint of binary pre-processed data\n",
    "    verbose=True,  # Enable detailed logging\n",
    "    # -- Unstructured Grid Options --\n",
    "    unstructured_grid=True,  # Use Unstructured Grid\n",
    "    dimensions={\"x\": \"ncells\"}   # Need to tell MarEx the new Unstructured dimension\n",
    "    coordinates={\"x\": \"lon\", \"y\": \"lat\"},  # Coordinates for Unstructured Grid\n",
    "    neighbours=ds.neighbours,  # Connectivity array for the Unstructured Grid Cells\n",
    "    cell_areas=ds.cell_areas,\n",
    ")  # Cell areas for each Unstructured Grid Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689c24a",
   "metadata": {},
   "source": [
    "## Split the Coherent Area Pre-processing & ID/Tracking/Merging Steps:\n",
    "- Coherent Area Pre-Processing Requires _Many Workers_\n",
    "- ID/Tracking Requires _Lots of Memory per Worker_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd018bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Spatial Pre-processing on the Distributed Cluster (above)\n",
    "data_bin_preprocessed, object_stats = tracker.run_preprocess(checkpoint=\"save\")\n",
    "# client_cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Small Local Dask Cluster\n",
    "#  N.B.: Need ~8 GB per worker (for 5km data // 15 million points)\n",
    "client = hpc.start_local_cluster(\n",
    "    n_workers=50, threads_per_worker=1, scratch_dir=scratch_dir / \"clients\"\n",
    ")  # Specify temporary scratch directory for dask to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_events_ds, merges_ds = tracker.run(\n",
    "    return_merges=True, checkpoint=\"load\"\n",
    ")  # This first loads the processed data, then tracks the events\n",
    "extreme_events_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6403e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IDed/Tracked/Merged Events to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = scratch_dir / \"mhws\" / \"extreme_events_merged_unstruct.zarr\"\n",
    "extreme_events_ds.to_zarr(file_name, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70fbcd7",
   "metadata": {},
   "source": [
    "### Use Centroid-based Partitioning Method for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID, Tracking, & Merging\n",
    "\n",
    "tracker = marEx.tracker(\n",
    "    ds.extreme_events,\n",
    "    ds.mask,\n",
    "    area_filter_quartile=0.8,  # Remove the smallest 80% of the identified coherent extreme areas. N.B.: With increasing resolution, the filter quartile should be increased.\n",
    "    R_fill=32,  # Fill small holes with radius < 32 elements, i.e. ~100 km,\n",
    "    T_fill=2,  # Allow gaps of 2 days and still continue the event tracking with the same ID\n",
    "    allow_merging=True,  # Allow extreme events to split/merge. Keeps track of merge events & unique IDs.\n",
    "    overlap_threshold=0.5,  # Overlap threshold for merging events. If overlap < threshold, events keep independent IDs.\n",
    "    nn_partitioning=False,  # Use old Centroid-based partitioning method (Di Sun et al. 2023).\n",
    "    temp_dir=str(\n",
    "        scratch_dir / \"mhws\" / \"TEMP/\"\n",
    "    ),  # Temporary Scratch Directory for Dask\n",
    "    checkpoint=\"save\",  # Make checkpoint of binary pre-processed data\n",
    "    verbose=True,  # Enable detailed logging\n",
    "    # -- Unstructured Grid Options --\n",
    "    unstructured_grid=True,  # Use Unstructured Grid\n",
    "    dimensions={\"x\": \"ncells\"}   # Need to tell MarEx the new Unstructured dimension\n",
    "    coordinates={\"x\": \"lon\", \"y\": \"lat\"},  # Coordinates for Unstructured Grid\n",
    "    neighbours=ds.neighbours,  # Connectivity array for the Unstructured Grid Cells\n",
    "    cell_areas=ds.cell_areas,\n",
    ")  # Cell areas for each Unstructured Grid Cell\n",
    "\n",
    "extreme_events_ds, merges_ds = tracker.run(return_merges=True)\n",
    "extreme_events_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91515920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IDed/Tracked/Merged Events to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = scratch_dir / \"mhws\" / \"extreme_events_merged_centroid_unstruct.zarr\"\n",
    "extreme_events_ds.to_zarr(file_name, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
