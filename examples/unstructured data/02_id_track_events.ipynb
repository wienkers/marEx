{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves on _Unstructured Grid_ using `MarEx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill spatial holes in the binary data, using highly-threaded binary dilation matrix operations -- up to `R_fill` elements in radius.\n",
    "2. Fill gaps in time -- permitting up to `T_fill` missing time slices, while keeping the same event ID.\n",
    "3. Filter out small objects -- areas less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "4. Identify objects in the binary data, using a highly efficient Unstructured Union-Find (Disjoint Set Union) Clustering Algorithm.\n",
    "5. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller area.\n",
    "    - Merged Blobs retain their original ID, but partition the child area based on the parent of the _nearest-neighbour_ cell. \n",
    "6. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "7. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 5km resolution on an _Unstructured Grid_ (15 million cells) using 240 cores takes ~40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b381",
   "metadata": {},
   "source": [
    "#### N.B.: The following `dask` config may be necessary on particular systems:\n",
    "```python\n",
    "dask.config.set({\n",
    "    'distributed.comm.timeouts.connect': '120s',  # Increase from default\n",
    "    'distributed.comm.timeouts.tcp': '240s',      # Double the connection timeout\n",
    "    'distributed.comm.retry.count': 10,           # More retries before giving up\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "#  N.B.: Need ~ 8 GB per worker (for 5km data // 15 million points)\n",
    "client = hpc.StartLocalCluster(n_workers=80, n_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "scratch_dir = Path('/scratch') / getuser()[0] / getuser() / 'mhws'\n",
    "\n",
    "file_name = scratch_dir / 'extremes_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}  # Adjust chunksize depending on system memory (too small make parallel iterative algorithm very slow)\n",
    "ds = xr.open_zarr(str(file_name), chunks={}).isel(time=slice(0, 1825)).chunk(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID, Tracking, & Merging\n",
    "\n",
    "tracker = blob.Spotter(ds.extreme_events, \n",
    "                       ds.mask,                                 \n",
    "                       area_filter_quartile = 0.8,          # Remove the smallest 80% of the identified coherent extreme areas. N.B.: With increasing resolution, the filter quartile should be increased.\n",
    "                       R_fill = 32,                         # Fill small holes with radius < 32 elements, i.e. ~100 km, \n",
    "                       T_fill = 2,                          # Allow gaps of 2 days and still continue the event tracking with the same ID\n",
    "                       allow_merging = True,                # Allow extreme events to split/merge. Keeps track of merge events & unique IDs.\n",
    "                       overlap_threshold = 0.5,             # Overlap threshold for merging events. If overlap < threshold, events keep independent IDs.\n",
    "                       nn_partitioning = True,              # Use new NN method to partition merged children areas. If False, reverts to old method of Di Sun et al. 2023.\n",
    "                       temp_dir = str(scratch_dir/'TEMP/'), # Temporary Scratch Directory for Dask\n",
    "                       checkpoint = 'load',                 # Load binary pre-processed data\n",
    "                       verbosity = 1,                       # Choose Verbosity Level (0=None, 1=Basic, 2=Advanced/Timing)\n",
    "                       # Unstructured Grid Options -- \n",
    "                       unstructured_grid = True,            # Use Unstructured Grid\n",
    "                       xdim = 'ncells',                     # Need to tell MarEx the new Unstructured dimension\n",
    "                       neighbours = ds.neighbours,          # Connectivity array for the Unstructured Grid Cells\n",
    "                       cell_areas = ds.cell_areas)          # Cell areas for each Unstructured Grid Cell\n",
    "\n",
    "extreme_events_ds = tracker.run(return_merges=True)\n",
    "extreme_events_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736e582",
   "metadata": {},
   "source": [
    "## Split the Processing & Tracking Steps:\n",
    "- Processing Requires Many Workers\n",
    "- Tracking Requires Lots of Memory per Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing: Use a Distributed Dask Cluster\n",
    "#client_cluster = hpc.StartDistributedCluster(n_workers=120, workers_per_node=40, node_memory=256, runtime=19)\n",
    "data_bin_preprocessed, blob_stats = tracker.run_preprocess(checkpoint='save')\n",
    "#client_cluster.close()\n",
    "\n",
    "### N.B.: 480 time steps takes ~17 minutes with 40 workers (512Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking: Use Local Cluster\n",
    "extreme_events_ds = tracker.run(return_merges=False)  # This first loads the processed data, then tracks the blobs\n",
    "\n",
    "### N.B.: 480 time steps takes ~40 minutes with 40 workers (512Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5e48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IDed/Tracked/Merged Events to `zarr` for more efficient parallel I/O\n",
    "file_name = scratch_dir / 'extreme_events_merged_unstruct.zarr'\n",
    "extreme_events_ds.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
