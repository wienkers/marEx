{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Daily Event Analysis: Marine Heatwave ID & Tracking using `MarEx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb47467",
   "metadata": {},
   "source": [
    "### `MarEx` Processing Pipeline for Unstructured Datasets:\n",
    "\n",
    "1. **Morphological Pre-Processing**\n",
    "    - Performs binary morphological closing using highly-threaded binary dilation matrix operations to fill small spatial holes up to `R_fill` elements in radius \n",
    "    - Executes binary opening to remove isolated small features of order `R_fill`\n",
    "    - Fills gaps in time to maintain event continuity for interruptions up to `T_fill` time steps\n",
    "    - Filters out smallest objects below the `area_filter_quartile` percentile threshold\n",
    "\n",
    "2. **Blob Identification**\n",
    "    - Labels spatially connected components using a highly efficient Unstructured Union-Find (Disjoint Set Union) Clustering Algorithm\n",
    "    - Computes blob properties (area, centroid, boundaries)\n",
    "\n",
    "3. **Temporal Tracking**\n",
    "    - Identifies blob overlaps between consecutive time frames\n",
    "    - Connects objects across time, applying the following criteria for splitting, merging, & persistence:\n",
    "        - Connected objects must overlap by at least fraction `overlap_threshold` of the smaller area\n",
    "        - Merged objects retain their original ID, but partition the child area based on the parent of the _nearest-neighbour_ cell (or centroid distance)\n",
    "\n",
    "4. **Graph Reduction & Finalisation**\n",
    "    - Constructs the complete temporal graph of object evolution through time\n",
    "    - Resolves object connectivity graph using `scipy.sparse.csgraph.connected_components`\n",
    "    - Creates globally unique IDs for each tracked extreme event\n",
    "    - Maps objects into efficient ID-time space for convenient analysis\n",
    "    - Computes comprehensive statistics about the lifecycle of each event\n",
    "\n",
    "The pipeline leverages **dask** for distributed parallel computation, enabling efficient processing of large datasets. \\\n",
    "A 40-year global daily analysis at 5km resolution on the _unstructured grid_ (15 million cells) using 240 cores takes ~40 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b381",
   "metadata": {},
   "source": [
    "#### N.B.: The following `dask` config may be necessary on particular systems:\n",
    "```python\n",
    "dask.config.set({\n",
    "    'distributed.comm.timeouts.connect': '120s',  # Increase from default\n",
    "    'distributed.comm.timeouts.tcp': '240s',      # Double the connection timeout\n",
    "    'distributed.comm.retry.count': 10,           # More retries before giving up\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import marEx\n",
    "import marEx.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "#  N.B.: Need ~ 8 GB per worker (for 5km data // 15 million points) -- Adjust as needed\n",
    "client = hpc.start_local_cluster(n_workers=80, n_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "scratch_dir = Path('/scratch') / getuser()[0] / getuser() / 'mhws'\n",
    "\n",
    "file_name = scratch_dir / 'extremes_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}  # Adjust chunksize depending on system memory, still globally-chunked in space (too small chunks makes the parallel iterative algorithm very slow)\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size).isel(time=slice(0, 1825))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID, Tracking, & Merging\n",
    "\n",
    "tracker = marEx.tracker(ds.extreme_events, \n",
    "                       ds.mask,                                 \n",
    "                       area_filter_quartile = 0.8,          # Remove the smallest 80% of the identified coherent extreme areas. N.B.: With increasing resolution, the filter quartile should be increased.\n",
    "                       R_fill = 32,                         # Fill small holes with radius < 32 elements, i.e. ~100 km, \n",
    "                       T_fill = 2,                          # Allow gaps of 2 days and still continue the event tracking with the same ID\n",
    "                       allow_merging = True,                # Allow extreme events to split/merge. Keeps track of merge events & unique IDs.\n",
    "                       overlap_threshold = 0.5,             # Overlap threshold for merging events. If overlap < threshold, events keep independent IDs.\n",
    "                       nn_partitioning = True,              # Use new NN method to partition merged children areas. If False, reverts to old method of Di Sun et al. 2023.\n",
    "                       temp_dir = str(scratch_dir/'TEMP/'), # Temporary Scratch Directory for Dask\n",
    "                       checkpoint = 'save',                 # Make checkpoint of binary pre-processed data\n",
    "                       verbosity = 1,                       # Choose Verbosity Level (0=None, 1=Basic, 2=Advanced/Timing)\n",
    "                       # -- Unstructured Grid Options -- \n",
    "                       unstructured_grid = True,            # Use Unstructured Grid\n",
    "                       xdim = 'ncells',                     # Need to tell MarEx the new Unstructured dimension\n",
    "                       neighbours = ds.neighbours,          # Connectivity array for the Unstructured Grid Cells\n",
    "                       cell_areas = ds.cell_areas)          # Cell areas for each Unstructured Grid Cell\n",
    "\n",
    "extreme_events_ds, merges_ds = tracker.run(return_merges=True)\n",
    "extreme_events_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6403e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the Coherent Area Pre-processing & ID/Tracking/Merging Steps:\n",
    "#   - Coherent Area Pre-Processing Requires _Many Workers_\n",
    "#   - ID/Tracking Requires _Lots of Memory per Worker_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing: Use a Distributed Dask Cluster\n",
    "#client_cluster = hpc.start_distributed_cluster(n_workers=120, workers_per_node=40, node_memory=256, runtime=19)\n",
    "# data_bin_preprocessed, object_stats = tracker.run_preprocess(checkpoint='save')\n",
    "#client_cluster.close()\n",
    "\n",
    "### N.B.: 480 time steps takes ~17 minutes with 40 workers (512Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking: Use Local Cluster\n",
    "# extreme_events_ds = tracker.run(return_merges=True, checkpoint='load')  # This first loads the processed data, then tracks the events\n",
    "\n",
    "### N.B.: 480 time steps takes ~40 minutes with 40 workers (512Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5e48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IDed/Tracked/Merged Events to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = scratch_dir / 'extreme_events_merged_unstruct.zarr'\n",
    "extreme_events_ds.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
