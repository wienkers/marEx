# Performance Testing Workflow for MarEx
# This workflow runs performance tests to catch performance regressions

name: Performance Tests

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'marEx/**'
      - 'tests/test_performance.py'
      - 'pyproject.toml'
  push:
    branches: [ main ]
    paths:
      - 'marEx/**'
      - 'tests/test_performance.py'
      - 'pyproject.toml'
  schedule:
    # Run performance tests weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:  # Allow manual triggering

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  performance-tests:
    name: Quick Performance Tests
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: 'pyproject.toml'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libhdf5-dev libnetcdf-dev libproj-dev proj-data proj-bin libgeos-dev

    - name: Install package with dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,full]"

    - name: Configure Dask for performance testing
      run: |
        mkdir -p ~/.dask
        cat > ~/.dask/config.yaml << 'EOF'
        distributed:
          worker:
            memory:
              target: 0.8
              spill: 0.85
              pause: 0.9
              terminate: 0.95
          admin:
            log-format: '%(name)s - %(levelname)s - %(message)s'
        array:
          chunk-size: 128MiB
        EOF

    - name: Run quick performance tests
      run: |
        python run_performance_tests.py --quick --report --verbose
      timeout-minutes: 30

    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ matrix.python-version }}
        path: |
          performance_*.txt
          .pytest_cache/
        retention-days: 30

  full-performance-tests:
    name: Full Performance Test Suite
    # Only run full performance tests on main branch or when specifically requested
    if: github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-perf]') || github.event_name == 'schedule'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'
        cache-dependency-path: 'pyproject.toml'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libhdf5-dev libnetcdf-dev libproj-dev proj-data proj-bin libgeos-dev

    - name: Install package with dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,full]"

    - name: Configure Dask for performance testing
      run: |
        mkdir -p ~/.dask
        cat > ~/.dask/config.yaml << 'EOF'
        distributed:
          worker:
            memory:
              target: 0.8
              spill: 0.85
              pause: 0.9
              terminate: 0.95
          admin:
            log-format: '%(name)s - %(levelname)s - %(message)s'
        array:
          chunk-size: 128MiB
        EOF

    - name: Run full performance test suite
      run: |
        python run_performance_tests.py --report --verbose
      timeout-minutes: 90

    - name: Upload full performance results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: full-performance-results
        path: |
          performance_*.txt
          .pytest_cache/
        retention-days: 30

    - name: Performance regression check
      if: github.event_name == 'pull_request'
      run: |
        echo "Checking for performance regressions..."
        # You can add logic here to compare performance results
        # with previous benchmarks and fail if regression is detected
