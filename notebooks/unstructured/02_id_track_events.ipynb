{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves on _Unstructured Grid_ using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill spatial holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Fill gaps in time -- permitting up to `T_fill` missing time slices, while keeping the same blob ID.\n",
    "3. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "4. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "5. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "6. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "7. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 5km resolution on an Unstructured Grid (15 million cells) using 32 cores takes \n",
    "- Full Split/Merge Thresholding & Merge Tracking:  ~40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks={}).isel(time=slice(0,32)).chunk(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.8  # Remove the smallest 80% of the identified blobs\n",
    "hole_filling_radius = 32   # Fill small holes with radius < 8 elements\n",
    "time_gap_fill = 2         # Allow gaps of 2 days and still continue the blob tracking with the same ID\n",
    "allow_merging = True      # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5   # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True    # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpOt & Track the Blobs & Merger Events\n",
    "\n",
    "tracker = blob.Spotter(ds.extreme_events, ds.mask, R_fill=hole_filling_radius, T_fill = time_gap_fill, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning, \n",
    "                       xdim='ncells',               # Need to tell spot_the_blOb the new Unstructured dimension\n",
    "                       unstructured_grid=True,      # Use Unstructured Grid\n",
    "                       neighbours=ds.neighbours,    # Connectivity array for the Unstructured Grid\n",
    "                       cell_areas=ds.cell_areas)      # Cell areas for each Unstructured Grid cell\n",
    "blobs = tracker.run(return_merges=False)\n",
    "\n",
    "# blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a5a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyicon as pyic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes(tracker.data_bin).persist()\n",
    "data_bin_gap_filled = tracker.fill_time_gaps(data_bin_filled).persist()\n",
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_gap_filled)\n",
    "data_bin_filtered = data_bin_filtered.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Track_blObs....\n",
    "\n",
    "blob_id_field, _ = tracker.identify_blobs(data_bin_filtered, time_connectivity=False)\n",
    "blob_id_field = blob_id_field.persist()\n",
    "\n",
    "cumsum_ids = (blob_id_field.max(dim=tracker.xdim)).cumsum(tracker.timedim).shift({tracker.timedim: 1}, fill_value=0)\n",
    "blob_id_field = xr.where(blob_id_field > 0, blob_id_field + cumsum_ids, 0)\n",
    "\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fa8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing as binary_closing_dask\n",
    "from dask_image.ndmorph import binary_opening as binary_opening_dask\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from scipy.sparse import coo_matrix, csr_matrix, eye\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask import delayed\n",
    "from dask import compute as dask_compute\n",
    "import dask.array as dsa\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, njit, int64, int32, prange\n",
    "import jax.numpy as jnp\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0004bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list, merge_events = tracker.split_and_merge_blobs_parallel(blob_id_field, blob_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995ab74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = split_merged_blob_id_field_unique\n",
    "blobs_props = merged_blobs_props\n",
    "overlap_blobs_list = split_merged_blobs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique IDs from the overlap pairs\n",
    "IDs = np.unique(overlap_blobs_list) # 1D sorted unique\n",
    "        \n",
    "# Create a mapping from ID to indices\n",
    "ID_to_index = {ID: index for index, ID in enumerate(IDs)}\n",
    "\n",
    "# Convert overlap pairs to indices\n",
    "overlap_pairs_indices = np.array([(ID_to_index[pair[0]], ID_to_index[pair[1]]) for pair in overlap_blobs_list])\n",
    "\n",
    "# Create a sparse matrix representation of the graph\n",
    "n = len(IDs)\n",
    "row_indices, col_indices = overlap_pairs_indices.T\n",
    "data = np.ones(len(overlap_pairs_indices), dtype=np.bool_)\n",
    "graph = csr_matrix((data, (row_indices, col_indices)), shape=(n, n), dtype=np.bool_)\n",
    "\n",
    "# Clear temporary arrays\n",
    "del row_indices\n",
    "del col_indices\n",
    "del data\n",
    "\n",
    "# Solve the graph to determine connected components\n",
    "num_components, component_IDs = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "\n",
    "del graph\n",
    "\n",
    "# Group IDs by their component index\n",
    "ID_clusters = [[] for _ in range(num_components)]\n",
    "for ID, component_ID in zip(IDs, component_IDs):\n",
    "    ID_clusters[component_ID].append(ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ID_clusters now is a list of lists of equivalent blob IDs that have been tracked across time\n",
    "#  We now need to replace all IDs in blob_id_field_unique that match the equivalent_IDs with the list index:  This is the new/final ID field.\n",
    "\n",
    "# Create a dictionary to map IDs to the new cluster indices\n",
    "min_int32 = np.iinfo(np.int32).min\n",
    "max_old_ID = blob_id_field_unique.max().compute().data\n",
    "ID_to_cluster_index_array = np.full(max_old_ID + 1, min_int32, dtype=np.int32)\n",
    "\n",
    "# Fill the lookup array with cluster indices\n",
    "for index, cluster in enumerate(ID_clusters):\n",
    "    for ID in cluster:\n",
    "        ID_to_cluster_index_array[ID] = np.int32(index+1) # Because these are the connected IDs, there are many fewer!\n",
    "                                                            #  Add 1 so that ID = 0 is still invalid/no object\n",
    "\n",
    "# N.B.: **Need to pass da into apply_ufunc, otherwise it doesn't manage the memory correctly with large shared-mem numpy arrays**\n",
    "ID_to_cluster_index_da = xr.DataArray(ID_to_cluster_index_array, dims='ID', coords={'ID': np.arange(max_old_ID + 1)})\n",
    "\n",
    "def map_IDs_to_indices(block, ID_to_cluster_index_array):\n",
    "    mask = block > 0\n",
    "    new_block = np.zeros_like(block, dtype=np.int32)\n",
    "    new_block[mask] = ID_to_cluster_index_array[block[mask]]\n",
    "    return new_block\n",
    "\n",
    "input_dims = [tracker.xdim] if tracker.unstructured_grid else [tracker.ydim, tracker.xdim]\n",
    "split_merged_relabeled_blob_id_field = xr.apply_ufunc(\n",
    "    map_IDs_to_indices,\n",
    "    blob_id_field_unique, \n",
    "    ID_to_cluster_index_da,\n",
    "    input_core_dims=[input_dims,['ID']],\n",
    "    output_core_dims=[input_dims],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[np.int32]\n",
    ").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610560d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relabel the blobs_props to match the new IDs (and add time dimension!)\n",
    "\n",
    "max_new_ID = num_components + 1  # New IDs range from 0 to max_new_ID...\n",
    "new_ids = np.arange(1, max_new_ID+1, dtype=np.int32)\n",
    "\n",
    "# New blobs_props DataSet Structure\n",
    "blobs_props_extended = xr.Dataset(coords={\n",
    "    'ID': new_ids,\n",
    "    tracker.timedim: blob_id_field_unique[tracker.timedim]\n",
    "})\n",
    "\n",
    "## Create a mapping from new IDs to the original IDs _at the corresponding time_\n",
    "valid_new_ids = (split_merged_relabeled_blob_id_field > 0)      \n",
    "original_ids_field = blob_id_field_unique.where(valid_new_ids)\n",
    "new_ids_field = split_merged_relabeled_blob_id_field.where(valid_new_ids)\n",
    "\n",
    "if not tracker.unstructured_grid:\n",
    "    original_ids_field = original_ids_field.stack(z=(tracker.ydim, tracker.xdim), create_index=False)\n",
    "    new_ids_field = new_ids_field.stack(z=(tracker.ydim, tracker.xdim), create_index=False)\n",
    "\n",
    "new_id_to_idx = {id_val: idx for idx, id_val in enumerate(new_ids)}\n",
    "\n",
    "def process_timestep(orig_ids, new_ids_t):\n",
    "    \"\"\"Process a single timestep to create ID mapping.\"\"\"\n",
    "    result = np.zeros(len(new_id_to_idx), dtype=np.int32)\n",
    "    \n",
    "    valid_mask = new_ids_t > 0\n",
    "    \n",
    "    # Get valid points for this timestep\n",
    "    if not valid_mask.any():\n",
    "        return result\n",
    "        \n",
    "    orig_valid = orig_ids[valid_mask]\n",
    "    new_valid = new_ids_t[valid_mask]\n",
    "    \n",
    "    if len(orig_valid) == 0:\n",
    "        return result\n",
    "        \n",
    "    unique_pairs = np.unique(np.column_stack((orig_valid, new_valid)), axis=0)\n",
    "    \n",
    "    # Create mapping\n",
    "    for orig_id, new_id in unique_pairs:\n",
    "        if new_id in new_id_to_idx:\n",
    "            result[new_id_to_idx[new_id]] = orig_id\n",
    "            \n",
    "    return result\n",
    "\n",
    "input_dim = ['ncells'] if tracker.unstructured_grid else ['z']\n",
    "global_id_mapping = xr.apply_ufunc(\n",
    "            process_timestep,\n",
    "            original_ids_field,\n",
    "            new_ids_field,\n",
    "            input_core_dims=[input_dim, input_dim],\n",
    "            output_core_dims=[['ID']],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[np.int32],\n",
    "            dask_gufunc_kwargs={'output_sizes': {'ID': len(new_ids)}}\n",
    "    ).assign_coords(ID=new_ids).compute()\n",
    "\n",
    "\n",
    "blobs_props_extended['global_ID'] = global_id_mapping\n",
    "# N.B.: Now, e.g. global_id_mapping.sel(ID=10) --> Given the new ID (10), returns corresponding original_id at every time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_props.sel(ID=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer and transform all variables from original blobs_props:\n",
    "\n",
    "\n",
    "for var_name in blobs_props.data_vars:\n",
    "    \n",
    "    temp = (blobs_props[var_name]\n",
    "                        .sel(ID=global_id_mapping.rename({'ID':'new_id'}))\n",
    "                        .drop_vars('ID').rename({'new_id':'ID'}))\n",
    "    \n",
    "    if var_name == 'ID':\n",
    "        temp = temp.astype(np.int32)\n",
    "    else:\n",
    "        temp = temp.astype(np.float32)\n",
    "        \n",
    "    blobs_props_extended[var_name] = temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Map the merge_events using the old IDs to be from dimensions (merge_ID, parent_idx) \n",
    "#     --> new merge_ledger with dimensions (time, ID, sibling_ID)\n",
    "# i.e. for each merge_ID --> merge_parent_IDs   gives the old IDs  --> map to new ID using ID_to_cluster_index_da\n",
    "#                   --> merge_time\n",
    "\n",
    "old_parent_IDs = xr.where(merge_events.parent_IDs>0, merge_events.parent_IDs, 0)\n",
    "new_IDs_parents = ID_to_cluster_index_da.sel(ID=old_parent_IDs)\n",
    "\n",
    "# Replace the coordinate merge_ID in new_IDs_parents with merge_time.  merge_events.merge_time gives merge_time for each merge_ID\n",
    "new_IDs_parents_t = new_IDs_parents.assign_coords({'merge_time': merge_events.merge_time}).drop_vars('ID').swap_dims({'merge_ID': 'merge_time'}).persist()  # this now has coordinate merge_time and ID\n",
    "\n",
    "# Map new_IDs_parents_t into a new data array with dimensions time, ID, and sibling_ID\n",
    "merge_ledger = xr.full_like(global_id_mapping, fill_value=-1).chunk({tracker.timedim: split_merged_relabeled_blob_id_field.data.chunksize[0]}).expand_dims({'sibling_ID': new_IDs_parents_t.parent_idx.shape[0]}).copy() # dimesions are time, ID, sibling_ID\n",
    "\n",
    "# Wrapper for processing/mapping mergers in parallel\n",
    "def process_time_group(time_block, IDs_data, IDs_coords):\n",
    "    \"\"\"Process all mergers for a single block of timesteps.\"\"\"\n",
    "    result = xr.full_like(time_block, -1)\n",
    "    \n",
    "    # Get unique times in this block\n",
    "    unique_times = np.unique(time_block[tracker.timedim])\n",
    "    \n",
    "    for time_val in unique_times:\n",
    "        # Get IDs for this time\n",
    "        time_mask = IDs_coords['merge_time'] == time_val\n",
    "        if not np.any(time_mask):\n",
    "            continue\n",
    "            \n",
    "        IDs_at_time = IDs_data[time_mask]\n",
    "        \n",
    "        # Single merger case\n",
    "        if IDs_at_time.ndim == 1:\n",
    "            valid_mask = IDs_at_time > 0\n",
    "            if np.any(valid_mask):\n",
    "                # Create expanded array for each sibling_ID dimension\n",
    "                expanded_IDs = np.broadcast_to(IDs_at_time, (len(time_block.sibling_ID), len(IDs_at_time)))\n",
    "                result.loc[{tracker.timedim: time_val, 'ID': IDs_at_time[valid_mask]}] = expanded_IDs[:, valid_mask]\n",
    "        # Multiple mergers case\n",
    "        else:\n",
    "            for merger_IDs in IDs_at_time:\n",
    "                valid_mask = merger_IDs > 0\n",
    "                if np.any(valid_mask):\n",
    "                    expanded_IDs = np.broadcast_to(merger_IDs, (len(time_block.sibling_ID), len(merger_IDs)))\n",
    "                    result.loc[{tracker.timedim: time_val, 'ID': merger_IDs[valid_mask]}] = expanded_IDs[:, valid_mask]\n",
    "                    \n",
    "    return result\n",
    "\n",
    "merge_ledger = xr.map_blocks(\n",
    "    process_time_group,\n",
    "    merge_ledger,\n",
    "    args=(new_IDs_parents_t.values, new_IDs_parents_t.coords),\n",
    "    template=merge_ledger\n",
    ")\n",
    "\n",
    "# Final formatting\n",
    "merge_ledger = merge_ledger.rename('merge_ledger').transpose(tracker.timedim, 'ID', 'sibling_ID').persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Finish up:\n",
    "# Add start and end time indices for each ID\n",
    "valid_presence = blobs_props_extended['global_ID'] > 0  # Where we have valid data\n",
    "\n",
    "blobs_props_extended['presence'] = valid_presence\n",
    "blobs_props_extended['time_start'] = valid_presence.time[valid_presence.argmax(dim=tracker.timedim)]\n",
    "blobs_props_extended['time_end'] = valid_presence.time[(valid_presence.sizes[tracker.timedim] - 1) - (valid_presence[::-1]).argmax(dim=tracker.timedim)]\n",
    "        \n",
    "# Combine blobs_props_extended with split_merged_relabeled_blob_id_field\n",
    "split_merged_relabeled_blobs_ds = xr.merge([split_merged_relabeled_blob_id_field.rename('ID_field'), \n",
    "                                            blobs_props_extended,\n",
    "                                            merge_ledger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = split_merged_relabeled_blobs_ds.isel(ID=slice(0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.ID_field.max().compute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked_unstruct.zarr'\n",
    "blobs.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
