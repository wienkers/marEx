{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves on _Unstructured Grid_ using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill spatial holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Fill gaps in time -- permitting up to `T_fill` missing time slices, while keeping the same blob ID.\n",
    "3. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "4. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "5. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "6. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "7. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 5km resolution on an Unstructured Grid (15 million cells) using 32 cores takes \n",
    "- Full Split/Merge Thresholding & Merge Tracking:  ~40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 7.86 GB\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks={}).isel(time=slice(0,32)).chunk(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.8  # Remove the smallest 80% of the identified blobs\n",
    "hole_filling_radius = 32   # Fill small holes with radius < 8 elements\n",
    "time_gap_fill = 2         # Allow gaps of 2 days and still continue the blob tracking with the same ID\n",
    "allow_merging = True      # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5   # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True    # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpOt & Track the Blobs & Merger Events\n",
    "\n",
    "tracker = blob.Spotter(ds.extreme_events, ds.mask, R_fill=hole_filling_radius, T_fill = time_gap_fill, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning, \n",
    "                       xdim='ncells',               # Need to tell spot_the_blOb the new Unstructured dimension\n",
    "                       unstructured_grid=True,      # Use Unstructured Grid\n",
    "                       neighbours=ds.neighbours,    # Connectivity array for the Unstructured Grid\n",
    "                       cell_areas=ds.cell_areas)      # Cell areas for each Unstructured Grid cell\n",
    "# blobs = tracker.run(return_merges=False)\n",
    "\n",
    "# blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyicon as pyic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes(tracker.data_bin).persist()\n",
    "data_bin_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_gap_filled = tracker.fill_time_gaps(data_bin_filled).persist()\n",
    "data_bin_gap_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74003e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_gap_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filtered = data_bin_filtered.persist()\n",
    "data_bin_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2d7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Track_blObs....\n",
    "###############\n",
    "\n",
    "\n",
    "blob_id_field, _ = tracker.identify_blobs(data_bin_filtered, time_connectivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field = blob_id_field.persist()\n",
    "blob_id_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "399e32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_ids = (blob_id_field.max(dim=tracker.xdim)).cumsum(tracker.timedim).shift({tracker.timedim: 1}, fill_value=0)\n",
    "blob_id_field = xr.where(blob_id_field > 0, \n",
    "                                        blob_id_field + cumsum_ids, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b42eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list, merge_events = tracker.split_and_merge_blobs(blob_id_field, blob_props, overlap_blobs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41fa8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing as binary_closing_dask\n",
    "from dask_image.ndmorph import binary_opening as binary_opening_dask\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from scipy.sparse import coo_matrix, csr_matrix, eye\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask import delayed\n",
    "from dask import compute as dask_compute\n",
    "import dask.array as dsa\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, njit, int64, int32, prange\n",
    "import jax.numpy as jnp\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = blob_id_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70473eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##################################\n",
    "### Optimised Helper Functions ###\n",
    "##################################\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def wrapped_euclidian_parallel(mask_values, parent_centroids_values, Nx):\n",
    "    \"\"\"\n",
    "    Optimised function for computing wrapped Euclidean distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_values : np.ndarray\n",
    "        2D boolean array where True indicates points to calculate distances for\n",
    "    parent_centroids_values : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (y, x) coordinates of parent centroids\n",
    "    Nx : int\n",
    "        Size of the x-dimension for wrapping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    distances : np.ndarray\n",
    "        Array of shape (n_true_points, n_parents) with minimum distances\n",
    "    \"\"\"\n",
    "    n_parents = len(parent_centroids_values)\n",
    "    half_Nx = Nx / 2\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(mask_values)\n",
    "    n_true = len(y_indices)\n",
    "    \n",
    "    distances = np.empty((n_true, n_parents), dtype=np.float64)\n",
    "    \n",
    "    # Precompute for faster access\n",
    "    parent_y = parent_centroids_values[:, 0]\n",
    "    parent_x = parent_centroids_values[:, 1]\n",
    "    \n",
    "    # Parallel loop over true positions\n",
    "    for idx in prange(n_true):\n",
    "        y, x = y_indices[idx], x_indices[idx]\n",
    "        \n",
    "        # Pre-compute y differences for all parents\n",
    "        dy = y - parent_y\n",
    "        \n",
    "        # Pre-compute x differences for all parents\n",
    "        dx = x - parent_x\n",
    "        \n",
    "        # Wrapping correction\n",
    "        dx = np.where(dx > half_Nx, dx - Nx, dx)\n",
    "        dx = np.where(dx < -half_Nx, dx + Nx, dx)\n",
    "        \n",
    "        distances[idx] = np.sqrt(dy * dy + dx * dx)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def create_grid_index_arrays(points_y, points_x, grid_size, ny, nx):\n",
    "    \"\"\"\n",
    "    Creates a grid-based spatial index using numpy arrays.\n",
    "    \"\"\"\n",
    "    n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "    n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "    max_points_per_cell = len(points_y)\n",
    "    \n",
    "    grid_points = np.full((n_grids_y, n_grids_x, max_points_per_cell), -1, dtype=np.int32)\n",
    "    grid_counts = np.zeros((n_grids_y, n_grids_x), dtype=np.int32)\n",
    "    \n",
    "    for idx in range(len(points_y)):\n",
    "        grid_y = min(points_y[idx] // grid_size, n_grids_y - 1)\n",
    "        grid_x = min(points_x[idx] // grid_size, n_grids_x - 1)\n",
    "        count = grid_counts[grid_y, grid_x]\n",
    "        if count < max_points_per_cell:\n",
    "            grid_points[grid_y, grid_x, count] = idx\n",
    "            grid_counts[grid_y, grid_x] += 1\n",
    "    \n",
    "    return grid_points, grid_counts\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def calculate_wrapped_distance(y1, x1, y2, x2, nx, half_nx):\n",
    "    \"\"\"\n",
    "    Calculate distance with periodic boundary conditions in x dimension.\n",
    "    \"\"\"\n",
    "    dy = y1 - y2\n",
    "    dx = x1 - x2\n",
    "    \n",
    "    if dx > half_nx:\n",
    "        dx -= nx\n",
    "    elif dx < -half_nx:\n",
    "        dx += nx\n",
    "        \n",
    "    return np.sqrt(dy * dy + dx * dx)\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def get_nearest_parent_labels(child_mask, parent_masks, child_ids, parent_centroids, Nx, max_distance=20):\n",
    "    \"\"\"\n",
    "    Assigns labels based on nearest parent blob points.\n",
    "    This is quite computationally-intensive, so we utilise many optimisations here...\n",
    "    \"\"\"\n",
    "    \n",
    "    ny, nx = child_mask.shape\n",
    "    half_Nx = Nx / 2\n",
    "    n_parents = len(parent_masks)\n",
    "    grid_size = max(2, max_distance // 4)\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(child_mask)\n",
    "    n_child_points = len(y_indices)\n",
    "    \n",
    "    min_distances = np.full(n_child_points, np.inf)\n",
    "    parent_assignments = np.zeros(n_child_points, dtype=np.int32)\n",
    "    found_close = np.zeros(n_child_points, dtype=np.bool_)\n",
    "    \n",
    "    for parent_idx in range(n_parents):\n",
    "        py, px = np.nonzero(parent_masks[parent_idx])\n",
    "        \n",
    "        if len(py) == 0:  # Skip empty parents\n",
    "            continue\n",
    "            \n",
    "        # Create grid index for this parent\n",
    "        n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "        n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "        grid_points, grid_counts = create_grid_index_arrays(py, px, grid_size, ny, nx)\n",
    "        \n",
    "        # Process child points in parallel\n",
    "        for child_idx in prange(n_child_points):\n",
    "            if found_close[child_idx]:  # Skip if we already found an exact match\n",
    "                continue\n",
    "                \n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            grid_y = min(child_y // grid_size, n_grids_y - 1)\n",
    "            grid_x = min(child_x // grid_size, n_grids_x - 1)\n",
    "            \n",
    "            min_dist_to_parent = np.inf\n",
    "            \n",
    "            # Check nearby grid cells\n",
    "            for dy in range(-1, 2):\n",
    "                grid_y_check = (grid_y + dy) % n_grids_y\n",
    "                \n",
    "                for dx in range(-1, 2):\n",
    "                    grid_x_check = (grid_x + dx) % n_grids_x\n",
    "                    \n",
    "                    # Process points in this grid cell\n",
    "                    n_points = grid_counts[grid_y_check, grid_x_check]\n",
    "                    \n",
    "                    for p_idx in range(n_points):\n",
    "                        point_idx = grid_points[grid_y_check, grid_x_check, p_idx]\n",
    "                        if point_idx == -1:\n",
    "                            break\n",
    "                        \n",
    "                        dist = calculate_wrapped_distance(\n",
    "                            child_y, child_x,\n",
    "                            py[point_idx], px[point_idx],\n",
    "                            Nx, half_Nx\n",
    "                        )\n",
    "                        \n",
    "                        if dist > max_distance:\n",
    "                            continue\n",
    "                        \n",
    "                        if dist < min_dist_to_parent:\n",
    "                            min_dist_to_parent = dist\n",
    "                            \n",
    "                        if dist < 1e-6:  # Found exact same point (within numerical precision)\n",
    "                            min_dist_to_parent = dist\n",
    "                            found_close[child_idx] = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_close[child_idx]:\n",
    "                        break\n",
    "                \n",
    "                if found_close[child_idx]:\n",
    "                    break\n",
    "            \n",
    "            # Update assignment if this parent is closer\n",
    "            if min_dist_to_parent < min_distances[child_idx]:\n",
    "                min_distances[child_idx] = min_dist_to_parent\n",
    "                parent_assignments[child_idx] = parent_idx\n",
    "    \n",
    "    # Handle any unassigned points using centroids\n",
    "    unassigned = min_distances == np.inf\n",
    "    if np.any(unassigned):\n",
    "        for child_idx in np.nonzero(unassigned)[0]:\n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            min_dist = np.inf\n",
    "            best_parent = 0\n",
    "            \n",
    "            for parent_idx in range(n_parents):\n",
    "                # Calculate distance to centroid with periodic boundary conditions\n",
    "                dist = calculate_wrapped_distance(\n",
    "                    child_y, child_x,\n",
    "                    parent_centroids[parent_idx, 0],\n",
    "                    parent_centroids[parent_idx, 1],\n",
    "                    Nx, half_Nx\n",
    "                )\n",
    "                \n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_parent = parent_idx\n",
    "                    \n",
    "            parent_assignments[child_idx] = best_parent\n",
    "    \n",
    "    # Convert from parent indices to child_ids\n",
    "    new_labels = child_ids[parent_assignments]\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def get_nearest_parent_labels_unstructured(child_mask, parent_masks, child_ids, parent_centroids, neighbours_int, lat, lon, max_distance=20):\n",
    "    \"\"\"\n",
    "    Optimized version of nearest parent label assignment for unstructured grids.\n",
    "    Uses numpy arrays throughout to ensure Numba compatibility.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    child_mask : np.ndarray\n",
    "        1D boolean array where True indicates points in the child blob\n",
    "    parent_masks : np.ndarray\n",
    "        2D boolean array of shape (n_parents, n_points) where True indicates points in each parent blob\n",
    "    child_ids : np.ndarray\n",
    "        1D array containing the IDs to assign to each partition of the child blob\n",
    "    parent_centroids : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (lat, lon) coordinates of parent centroids in degrees\n",
    "    neighbours_int : np.ndarray\n",
    "        2D array of shape (3, n_points) containing indices of neighboring cells for each point\n",
    "    lat / lon : np.ndarray\n",
    "        Latitude/Longitude in degrees\n",
    "    max_distance : int, optional\n",
    "        Maximum number of edge hops to search for parent points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_labels : np.ndarray\n",
    "        1D array containing the assigned child_ids for each True point in child_mask\n",
    "    \"\"\"\n",
    "    n_points = len(child_mask)\n",
    "    n_parents = len(parent_masks)\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    distances = np.full(n_points, np.inf, dtype=np.float32)\n",
    "    parent_assignments = np.full(n_points, -1, dtype=np.int32)\n",
    "    visited = np.zeros((n_parents, n_points), dtype=np.bool_)\n",
    "    \n",
    "    # Initialize with direct overlaps\n",
    "    for parent_idx in range(n_parents):\n",
    "        overlap_mask = parent_masks[parent_idx] & child_mask\n",
    "        if np.any(overlap_mask):\n",
    "            visited[parent_idx, overlap_mask] = True\n",
    "            unclaimed_overlap = distances[overlap_mask] == np.inf\n",
    "            if np.any(unclaimed_overlap):\n",
    "                overlap_points = np.where(overlap_mask)[0]\n",
    "                valid_points = overlap_points[unclaimed_overlap]\n",
    "                distances[valid_points] = 0\n",
    "                parent_assignments[valid_points] = parent_idx\n",
    "    \n",
    "    # Pre-compute trig values\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    cos_lat = np.cos(lat_rad)\n",
    "    \n",
    "    # Graph traversal for remaining points\n",
    "    current_distance = 0\n",
    "    any_unassigned = np.any(child_mask & (parent_assignments == -1))\n",
    "    \n",
    "    while current_distance < max_distance and any_unassigned:\n",
    "        current_distance += 1\n",
    "        updates_made = False\n",
    "        \n",
    "        for parent_idx in range(n_parents):\n",
    "            # Get current frontier points\n",
    "            frontier_mask = visited[parent_idx]\n",
    "            if not np.any(frontier_mask):\n",
    "                continue\n",
    "            \n",
    "            # Process neighbors\n",
    "            for i in range(3):  # For each neighbor direction\n",
    "                neighbors = neighbours_int[i, frontier_mask]\n",
    "                valid_neighbors = neighbors >= 0\n",
    "                if not np.any(valid_neighbors):\n",
    "                    continue\n",
    "                    \n",
    "                valid_points = neighbors[valid_neighbors]\n",
    "                unvisited = ~visited[parent_idx, valid_points]\n",
    "                new_points = valid_points[unvisited]\n",
    "                \n",
    "                if len(new_points) > 0:\n",
    "                    visited[parent_idx, new_points] = True\n",
    "                    update_mask = distances[new_points] > current_distance\n",
    "                    if np.any(update_mask):\n",
    "                        points_to_update = new_points[update_mask]\n",
    "                        distances[points_to_update] = current_distance\n",
    "                        parent_assignments[points_to_update] = parent_idx\n",
    "                        updates_made = True\n",
    "        \n",
    "        if not updates_made:\n",
    "            break\n",
    "            \n",
    "        any_unassigned = np.any(child_mask & (parent_assignments == -1))\n",
    "    \n",
    "    # Handle remaining unassigned points using great circle distances\n",
    "    unassigned_mask = child_mask & (parent_assignments == -1)\n",
    "    if np.any(unassigned_mask):\n",
    "        parent_lat_rad = np.deg2rad(parent_centroids[:, 0])\n",
    "        parent_lon_rad = np.deg2rad(parent_centroids[:, 1])\n",
    "        cos_parent_lat = np.cos(parent_lat_rad)\n",
    "        \n",
    "        unassigned_points = np.where(unassigned_mask)[0]\n",
    "        for point in unassigned_points:\n",
    "            # Vectorized haversine calculation\n",
    "            dlat = parent_lat_rad - lat_rad[point]\n",
    "            dlon = parent_lon_rad - lon_rad[point]\n",
    "            a = np.sin(dlat/2)**2 + cos_lat[point] * cos_parent_lat * np.sin(dlon/2)**2\n",
    "            dist = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "            parent_assignments[point] = np.argmin(dist)\n",
    "    \n",
    "    # Return only the assignments for points in child_mask\n",
    "    child_points = np.where(child_mask)[0]\n",
    "    return child_ids[parent_assignments[child_points]]\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def unstructured_centroid_partition(child_mask, parent_centroids, child_ids, lat, lon):\n",
    "    \"\"\"\n",
    "    Assigns labels to child cells based on closest parent centroid using great circle distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    child_mask : np.ndarray\n",
    "        1D boolean array indicating which cells belong to the child blob\n",
    "    parent_centroids : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (lat, lon) coordinates of parent centroids in degrees\n",
    "    child_ids : np.ndarray\n",
    "        Array of IDs to assign to each partition of the child blob\n",
    "    lat / lon : np.ndarray\n",
    "        Latitude/Longitude in degrees\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    new_labels : np.ndarray\n",
    "        1D array containing assigned child_ids for cells in child_mask\n",
    "    \"\"\"\n",
    "    n_cells = len(child_mask)\n",
    "    n_parents = len(parent_centroids)\n",
    "    \n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    parent_coords_rad = np.deg2rad(parent_centroids)\n",
    "    \n",
    "    new_labels = np.zeros(n_cells, dtype=child_ids.dtype)\n",
    "    \n",
    "    # Process each child cell in parallel\n",
    "    for i in prange(n_cells):\n",
    "        if not child_mask[i]:\n",
    "            continue\n",
    "            \n",
    "        min_dist = np.inf\n",
    "        closest_parent = 0\n",
    "        \n",
    "        # Calculate great circle distance to each parent centroid\n",
    "        for j in range(n_parents):\n",
    "            dlat = parent_coords_rad[j, 0] - lat_rad[i]\n",
    "            dlon = parent_coords_rad[j, 1] - lon_rad[i]\n",
    "            \n",
    "            # Use haversine formula for great circle distance\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat_rad[i]) * np.cos(parent_coords_rad[j, 0]) * np.sin(dlon/2)**2\n",
    "            dist = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_parent = j\n",
    "        \n",
    "        new_labels[i] = child_ids[closest_parent]\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Helper Function for Super Fast Sparse Bool Multiply (*without the scipy+Dask Memory Leak*)\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def sparse_bool_power(vec, sp_data, indices, indptr, exponent):\n",
    "    vec = vec.T\n",
    "    num_rows = indptr.size - 1\n",
    "    num_cols = vec.shape[1]\n",
    "    result = vec.copy()\n",
    "\n",
    "    for _ in range(exponent):\n",
    "        temp_result = np.zeros((num_rows, num_cols), dtype=np.bool_)\n",
    "\n",
    "        for i in prange(num_rows):\n",
    "            for j in range(indptr[i], indptr[i + 1]):\n",
    "                if sp_data[j]:\n",
    "                    for k in range(num_cols):\n",
    "                        if result[indices[j], k]:\n",
    "                            temp_result[i, k] = True\n",
    "\n",
    "        result = temp_result\n",
    "\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "84d1ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_0 = blob_props['area'].sel(ID=overlap_blobs_list[:, 0]).values\n",
    "areas_1 = blob_props['area'].sel(ID=overlap_blobs_list[:, 1]).values\n",
    "min_areas = np.minimum(areas_0, areas_1)\n",
    "overlap_fractions = overlap_blobs_list[:, 2].astype(float) / min_areas\n",
    "\n",
    "## Filter out the overlaps that are too small\n",
    "overlap_blobs_list = overlap_blobs_list[overlap_fractions >= tracker.overlap_threshold]\n",
    "\n",
    "## Initialize merge tracking lists to build DataArray later\n",
    "merge_times = []      # When the merge occurred\n",
    "merge_child_ids = []  # Resulting child ID\n",
    "merge_parent_ids = []  # List of parent IDs that merged\n",
    "merge_areas = []      # Areas of overlap\n",
    "\n",
    "# Find initial merging blobs\n",
    "unique_children, children_counts = np.unique(overlap_blobs_list[:, 1], return_counts=True)\n",
    "merging_blobs = set(unique_children[children_counts > 1])\n",
    "\n",
    "# Pre-compute the child_time_idx for merging_blobs\n",
    "time_index_map = tracker.compute_id_time_dict(blob_id_field_unique, list(merging_blobs), blob_props.ID.max().item() + 1)\n",
    "\n",
    "# Group blobs by time-chunk\n",
    "chunk_boundaries = np.cumsum([0] + list(blob_id_field_unique.chunks[0]))\n",
    "blobs_by_chunk = {}\n",
    "for chunk_idx in range(len(blob_id_field_unique.chunks[0])):\n",
    "    blobs_by_chunk[chunk_idx] = []\n",
    "\n",
    "for blob_id in merging_blobs:\n",
    "    chunk_idx = np.searchsorted(chunk_boundaries, time_index_map[blob_id], side='right') - 1\n",
    "    blobs_by_chunk[chunk_idx].append(blob_id)\n",
    "\n",
    "# Persist input arrays\n",
    "blob_id_field_unique = blob_id_field_unique.persist()\n",
    "blob_props = blob_props.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1e3eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_idx, chunk_blobs, last_timeslice, next_id_start):\n",
    "    '''Process a single chunk of merging blobs.'''\n",
    "    # Extract chunk data\n",
    "    chunk_start = chunk_boundaries[chunk_idx]\n",
    "    chunk_end = chunk_boundaries[chunk_idx + 1] + 1\n",
    "    chunk_data = blob_id_field_unique.isel({tracker.timedim: slice(chunk_start, chunk_end)}).compute()\n",
    "    \n",
    "    # Initialize tracking variables for this chunk\n",
    "    chunk_merge_times = []\n",
    "    chunk_merge_child_ids = []\n",
    "    chunk_merge_parent_ids = []\n",
    "    chunk_merge_areas = []\n",
    "    next_new_id = next_id_start\n",
    "    new_merging_blobs = set()\n",
    "    id_mapping = {}  # Track local to global ID mapping\n",
    "    \n",
    "    # Process each blob in the chunk\n",
    "    blobs_to_process = chunk_blobs.copy()\n",
    "    overlap_blobs_local = overlap_blobs_list.copy()\n",
    "    while blobs_to_process:\n",
    "        child_id = blobs_to_process.pop(0)\n",
    "        child_time_idx = time_index_map[child_id]\n",
    "        relative_time_idx = child_time_idx - chunk_start\n",
    "        \n",
    "        print('Processing chunk', chunk_idx, 'child', child_id, 'at relative time', relative_time_idx)\n",
    "        \n",
    "        # Get time slices\n",
    "        blob_id_time = chunk_data.isel({tracker.timedim: relative_time_idx})\n",
    "        if relative_time_idx + 1 < chunk_data.sizes[tracker.timedim]:\n",
    "            blob_id_time_p1 = chunk_data.isel({tracker.timedim: relative_time_idx + 1})\n",
    "        else:\n",
    "            blob_id_time_p1 = xr.full_like(blob_id_time, 0)\n",
    "            \n",
    "        if relative_time_idx - 1 >= 0:\n",
    "            blob_id_time_m1 = chunk_data.isel({tracker.timedim: relative_time_idx - 1})\n",
    "        elif last_timeslice is not None:\n",
    "            blob_id_time_m1 = last_timeslice\n",
    "        else:\n",
    "            blob_id_time_m1 = xr.full_like(blob_id_time, 0)\n",
    "        \n",
    "        # Process merging logic\n",
    "        child_mask_2d = (blob_id_time == child_id).values\n",
    "        child_mask = overlap_blobs_local[:, 1] == child_id\n",
    "        child_where = np.where(child_mask)[0]\n",
    "        merge_group = overlap_blobs_local[child_mask]\n",
    "        parent_ids = merge_group[:, 0]\n",
    "        \n",
    "        # Create parent masks and get centroids\n",
    "        parent_masks = np.array([(blob_id_time_m1 == pid).values for pid in parent_ids])\n",
    "        parent_blob_props = tracker.calculate_blob_properties(blob_id_time_m1, properties=['area', 'centroid'], full_array=False)\n",
    "        parent_centroids = parent_blob_props.sel(ID=parent_ids).centroid.values\n",
    "        child_ids = np.concatenate((np.array([child_id]), \n",
    "                                np.arange(next_new_id, next_new_id + (len(parent_ids) - 1), dtype=np.int32)))\n",
    "        \n",
    "        # Update ID tracking\n",
    "        for new_id in child_ids[1:]:\n",
    "            id_mapping[new_id] = None  # Will be assigned global ID later\n",
    "        next_new_id += len(parent_ids) - 1\n",
    "        \n",
    "        # Partition child blob based on method\n",
    "        if tracker.nn_partitioning:\n",
    "            \n",
    "            max_area = np.max(parent_blob_props.sel(ID=parent_ids).area.values) / tracker.mean_cell_area\n",
    "            max_distance = int(np.sqrt(max_area) * 2.0)  # Use 2x the max blob radius\n",
    "            \n",
    "            if tracker.unstructured_grid:\n",
    "                new_labels = get_nearest_parent_labels_unstructured(\n",
    "                    child_mask_2d.astype(np.bool_),  # Ensure correct boolean type\n",
    "                    parent_masks.astype(np.bool_),   # Ensure correct boolean type\n",
    "                    child_ids.astype(np.int32),      # Ensure int32 type\n",
    "                    parent_centroids.astype(np.float64),  # Ensure float64 type\n",
    "                    tracker.neighbours_int.values.astype(np.int32),  # Ensure int32 type\n",
    "                    tracker.data_bin.lat.values.astype(np.float64),  # Ensure float64 type\n",
    "                    tracker.data_bin.lon.values.astype(np.float64),  # Ensure float64 type\n",
    "                    max_distance=max(max_distance, 20)*2  # Set minimum threshold, in cells\n",
    "                )\n",
    "            else:\n",
    "                new_labels = get_nearest_parent_labels(\n",
    "                    child_mask_2d.astype(np.bool_),  # Ensure correct boolean type\n",
    "                    parent_masks.astype(np.bool_),   # Ensure correct boolean type\n",
    "                    child_ids.astype(np.int32),      # Ensure int32 type\n",
    "                    parent_centroids.astype(np.float64),  # Ensure float64 type\n",
    "                    tracker.data_bin[tracker.xdim].size,\n",
    "                    max_distance=max(max_distance, 20)  # Set minimum threshold, in pixels\n",
    "                )\n",
    "        else:\n",
    "            if tracker.unstructured_grid:\n",
    "                new_labels = unstructured_centroid_partition(\n",
    "                    child_mask_2d,\n",
    "                    parent_centroids,\n",
    "                    child_ids,\n",
    "                    tracker.data_bin.lat.values,\n",
    "                    tracker.data_bin.lon.values\n",
    "                )\n",
    "            else:\n",
    "                distances = wrapped_euclidian_parallel(child_mask_2d, parent_centroids, tracker.data_bin[tracker.xdim].size)\n",
    "                new_labels = child_ids[np.argmin(distances, axis=1)]\n",
    "        \n",
    "        # Update chunk data\n",
    "        temp = np.zeros_like(blob_id_time)\n",
    "        temp[child_mask_2d] = new_labels\n",
    "        blob_id_time = blob_id_time.where(~child_mask_2d, temp)\n",
    "        chunk_data[{tracker.timedim: relative_time_idx}] = blob_id_time\n",
    "        \n",
    "        # Update tracking\n",
    "        chunk_merge_times.append(chunk_data.isel({tracker.timedim: relative_time_idx}).time.values)\n",
    "        chunk_merge_child_ids.append(child_ids)\n",
    "        chunk_merge_parent_ids.append(parent_ids)\n",
    "        chunk_merge_areas.append(merge_group[:, 2])\n",
    "        \n",
    "        # Check for new merging blobs\n",
    "        overlap_blobs_local = tracker.check_overlap_slice(blob_id_time.values, blob_id_time_p1.values)\n",
    "        if len(overlap_blobs_local) > 0:\n",
    "            new_unique_children, new_children_counts = np.unique(overlap_blobs_local[:, 1], return_counts=True)\n",
    "            current_new_merging = set(new_unique_children[new_children_counts > 1])\n",
    "            \n",
    "            # Add to overall set of new merging blobs for this chunk\n",
    "            new_merging_blobs.update(current_new_merging)\n",
    "            \n",
    "            # Add to processing queue if they're in the next timeslice of this chunk\n",
    "            if relative_time_idx + 1 < chunk_data.sizes[tracker.timedim] - 1:\n",
    "                for new_blob_id in current_new_merging:\n",
    "                    if new_blob_id not in blobs_to_process:  # Avoid duplicates\n",
    "                        blobs_to_process.append(new_blob_id)\n",
    "            \n",
    "    \n",
    "    return (chunk_data, next_new_id, new_merging_blobs,\n",
    "            (chunk_merge_times, chunk_merge_child_ids, chunk_merge_parent_ids, chunk_merge_areas),\n",
    "            id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033986cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "096910aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process chunks iteratively until no new merging blobs remain\n",
    "iteration = 0\n",
    "max_iterations = 10\n",
    "processed_chunks = set()\n",
    "global_id_counter = blob_props.ID.max().item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f6c58d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_futures = []\n",
    "next_id_offset = 0\n",
    "last_timeslice = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "23492a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_idx in sorted(blobs_by_chunk.keys()):\n",
    "    chunk_blobs = [b for b in merging_blobs if chunk_idx == np.searchsorted(chunk_boundaries, time_index_map[b], side='right') - 1]\n",
    "    if not chunk_blobs:\n",
    "        continue\n",
    "    \n",
    "    chunk_start = chunk_boundaries[chunk_idx]\n",
    "    last_timeslice = blob_id_field_unique.isel({tracker.timedim: chunk_start - 1}) if chunk_start > 0 else None\n",
    "\n",
    "        \n",
    "    future = delayed(process_chunk)(chunk_idx, chunk_blobs, last_timeslice,\n",
    "                                global_id_counter + next_id_offset)\n",
    "    chunk_futures.append((chunk_idx, future))\n",
    "    chunk_size = blob_id_field_unique.chunks[0][chunk_idx]\n",
    "    next_id_offset += len(chunk_blobs) * chunk_size  # Conservative estimate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3351fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dask_compute(*[f[1] for f in chunk_futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3185b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6ed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1b10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48adb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_temp_ids = set()\n",
    "for _, (_, _, _, _, _, id_mapping) in zip(chunk_futures, results):\n",
    "    all_temp_ids.update(id_mapping.keys())\n",
    "\n",
    "# Create global ID mapping\n",
    "global_id_mapping = {temp_id: global_id_counter + i \n",
    "                for i, temp_id in enumerate(sorted(all_temp_ids))}\n",
    "global_id_counter += len(all_temp_ids)\n",
    "\n",
    "# Second pass: process results and update global state\n",
    "new_merging_blobs = set()\n",
    "new_blob_props_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ad11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass: process results and update global state\n",
    "new_merging_blobs = set()\n",
    "new_blob_props_list = []\n",
    "\n",
    "for (chunk_idx, _), (chunk_data, new_props, _, chunk_new_merging, chunk_merge_events, id_mapping) in zip(chunk_futures, results):\n",
    "    # Update IDs in chunk data\n",
    "    chunk_mask = np.isin(chunk_data, list(id_mapping.keys()))\n",
    "    for temp_id, global_id in global_id_mapping.items():\n",
    "        chunk_data = xr.where(chunk_data == temp_id, global_id, chunk_data)\n",
    "    \n",
    "    # Update chunk in main array\n",
    "    chunk_start = chunk_boundaries[chunk_idx]\n",
    "    chunk_end = chunk_boundaries[chunk_idx + 1]\n",
    "    blob_id_field_unique[{tracker.timedim: slice(chunk_start, chunk_end)}] = chunk_data[:-1]\n",
    "    \n",
    "    # Update merge events with new global IDs\n",
    "    times, child_ids, parent_ids, areas = chunk_merge_events\n",
    "    updated_child_ids = []\n",
    "    for ids in child_ids:\n",
    "        updated_ids = np.array([global_id_mapping.get(id_, id_) for id_ in ids])\n",
    "        updated_child_ids.append(updated_ids)\n",
    "    \n",
    "    merge_times.extend(times)\n",
    "    merge_child_ids.extend(updated_child_ids)\n",
    "    merge_parent_ids.extend(parent_ids)\n",
    "    merge_areas.extend(areas)\n",
    "    \n",
    "    # Update blob properties with new global IDs\n",
    "    for old_id, props in new_props.items():\n",
    "        if old_id in global_id_mapping:\n",
    "            props['ID'] = global_id_mapping[old_id]\n",
    "        new_blob_props_list.append(props)\n",
    "    \n",
    "    # Track new merging blobs (using updated global IDs)\n",
    "    new_merging_blobs.update(\n",
    "        global_id_mapping.get(blob_id, blob_id) \n",
    "        for blob_id in chunk_new_merging\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473723f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb8569f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed251824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(np.random.random(size=(200, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16e85e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=20).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b5f547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=21).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f651837",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=22).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dde57c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=23).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68b74068",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=24).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=25).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc522047",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=26).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique.isel(time=27).pyic.plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142093d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked_unstruct.zarr'\n",
    "blobs.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
