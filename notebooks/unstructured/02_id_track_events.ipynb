{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves on _Unstructured Grid_ using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill spatial holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Fill gaps in time -- permitting up to `T_fill` missing time slices, while keeping the same blob ID.\n",
    "3. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "4. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "5. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "6. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "7. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 5km resolution on an Unstructured Grid (15 million cells) using 32 cores takes \n",
    "- Full Split/Merge Thresholding & Merge Tracking:  ~40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks={}).isel(time=slice(0,32)).chunk(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.8  # Remove the smallest 80% of the identified blobs\n",
    "hole_filling_radius = 32  # Fill small holes with radius < 32 elements, i.e. ~100 km\n",
    "time_gap_fill = 2         # Allow gaps of 2 days and still continue the blob tracking with the same ID\n",
    "allow_merging = True      # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5   # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True    # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpOt & Track the Blobs & Merger Events\n",
    "\n",
    "tracker = blob.Spotter(ds.extreme_events, ds.mask, R_fill=hole_filling_radius, T_fill = time_gap_fill, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning, \n",
    "                       xdim='ncells',                 # Need to tell spot_the_blOb the new Unstructured dimension\n",
    "                       unstructured_grid=True,        # Use Unstructured Grid\n",
    "                       neighbours=ds.neighbours,      # Connectivity array for the Unstructured Grid Cells\n",
    "                       cell_areas=ds.cell_areas,      # Cell areas for each Unstructured Grid Cell\n",
    "                       debug=0,                       # Choose Debugging Level (max=2)\n",
    "                       verbosity=3)                   # Choose Verbosity Level (0=None, 1=Basic, 2=Timing)\n",
    "\n",
    "# blobs = tracker.run(return_merges=False)\n",
    "\n",
    "# blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f9cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask.distributed import wait\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing as binary_closing_dask\n",
    "from dask_image.ndmorph import binary_opening as binary_opening_dask\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from scipy.sparse import coo_matrix, csr_matrix, eye\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask import delayed\n",
    "from dask import compute as dask_compute\n",
    "import dask.array as dsa\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, njit, prange\n",
    "import jax.numpy as jnp\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Area of Initial Binary Data\n",
    "raw_area = tracker.compute_area(tracker.data_bin)  # This is e.g. the initial Hobday area\n",
    "\n",
    "# Fill Small Holes & Gaps between Objects\n",
    "data_bin_filled = tracker.fill_holes(tracker.data_bin).persist()\n",
    "wait(data_bin_filled)\n",
    "if tracker.verbosity > 0:    print('Finished Filling Spatial Holes')\n",
    "\n",
    "# Fill Small Time-Gaps between Objects\n",
    "data_bin_gap_filled = tracker.fill_time_gaps(data_bin_filled).persist()\n",
    "wait(data_bin_gap_filled)\n",
    "if tracker.verbosity > 0:    print('Finished Filling Spatio-temporal Holes.')\n",
    "\n",
    "# Remove Small Objects\n",
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_prefiltered = tracker.filter_small_blobs(data_bin_gap_filled)\n",
    "if tracker.verbosity > 0:    print('Finished Filtering Small Blobs.')\n",
    "\n",
    "# Clean Up & Persist Preprocessing (This helps avoid block-wise task fusion run_spec issues with dask)\n",
    "data_bin_filtered = data_bin_filtered.persist()\n",
    "wait(data_bin_filtered)\n",
    "del data_bin_filled\n",
    "del data_bin_gap_filled\n",
    "\n",
    "# Compute Area of Morphologically-Processed & Filtered Data\n",
    "processed_area = tracker.compute_area(data_bin_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin = data_bin_filtered\n",
    "\n",
    "blob_id_field, _ = tracker.identify_blobs(data_bin, time_connectivity=False)\n",
    "blob_id_field = blob_id_field.persist()\n",
    "if tracker.verbosity > 0:    print('Finished Blob Identification.')\n",
    "\n",
    "\n",
    "if tracker.unstructured_grid:\n",
    "    # Make the blob_id_field unique across time\n",
    "    cumsum_ids = (blob_id_field.max(dim=tracker.xdim)).cumsum(tracker.timedim).shift({tracker.timedim: 1}, fill_value=0)\n",
    "    blob_id_field = xr.where(blob_id_field > 0, blob_id_field + cumsum_ids, 0)\n",
    "    blob_id_field = blob_id_field.persist()\n",
    "    wait(blob_id_field)\n",
    "    if tracker.verbosity > 0:    print('Finished Making Blobs Globally Unique.')\n",
    "\n",
    "# Calculate Properties of each Blob\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n",
    "blob_props = blob_props.persist()\n",
    "wait(blob_props)\n",
    "if tracker.verbosity > 0:    print('Finished Calculating Blob Properties.')\n",
    "\n",
    "blob_id_field_unique = blob_id_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292386d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b271e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_blob_field(blob_id_field_unique, id_lookup, updates):\n",
    "    \"\"\"Update the blob field with chunk results using xarray operations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    blob_id_field_unique : xarray.DataArray\n",
    "        The full blob field to update\n",
    "    id_lookup : Dictionary\n",
    "        Dictionary mapping temporary IDs to new IDs\n",
    "    updates : xarray.DataArray\n",
    "        DataArray of Dictionaries containing updates: 'spatial_indices' for each 'new_label'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        Updated blob field\n",
    "    \"\"\"\n",
    "    \n",
    "    def apply_updates(data, updates):\n",
    "        \"\"\"Apply updates to a single chunk of data.\"\"\"\n",
    "        result = data.copy()\n",
    "        for update in updates:\n",
    "            \n",
    "            spatial_indices = update['spatial_indices']\n",
    "            new_label = id_lookup[update['new_label']]\n",
    "            result[spatial_indices] = new_label\n",
    "        \n",
    "        return result\n",
    "\n",
    "    result = xr.apply_ufunc(apply_updates,\n",
    "                            blob_id_field_unique,\n",
    "                            updates,\n",
    "                            input_core_dims=[[tracker.xdim], []],\n",
    "                            output_core_dims=[[tracker.xdim]],\n",
    "                            dask='parallelized',\n",
    "                            output_dtypes=[blob_id_field_unique.dtype],\n",
    "                            vectorize=True).persist()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e195a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##################################\n",
    "### Optimised Helper Functions ###\n",
    "##################################\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def wrapped_euclidian_parallel(mask_values, parent_centroids_values, Nx):\n",
    "    \"\"\"\n",
    "    Optimised function for computing wrapped Euclidean distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_values : np.ndarray\n",
    "        2D boolean array where True indicates points to calculate distances for\n",
    "    parent_centroids_values : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (y, x) coordinates of parent centroids\n",
    "    Nx : int\n",
    "        Size of the x-dimension for wrapping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    distances : np.ndarray\n",
    "        Array of shape (n_true_points, n_parents) with minimum distances\n",
    "    \"\"\"\n",
    "    n_parents = len(parent_centroids_values)\n",
    "    half_Nx = Nx / 2\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(mask_values)\n",
    "    n_true = len(y_indices)\n",
    "    \n",
    "    distances = np.empty((n_true, n_parents), dtype=np.float64)\n",
    "    \n",
    "    # Precompute for faster access\n",
    "    parent_y = parent_centroids_values[:, 0]\n",
    "    parent_x = parent_centroids_values[:, 1]\n",
    "    \n",
    "    # Parallel loop over true positions\n",
    "    for idx in prange(n_true):\n",
    "        y, x = y_indices[idx], x_indices[idx]\n",
    "        \n",
    "        # Pre-compute y differences for all parents\n",
    "        dy = y - parent_y\n",
    "        \n",
    "        # Pre-compute x differences for all parents\n",
    "        dx = x - parent_x\n",
    "        \n",
    "        # Wrapping correction\n",
    "        dx = np.where(dx > half_Nx, dx - Nx, dx)\n",
    "        dx = np.where(dx < -half_Nx, dx + Nx, dx)\n",
    "        \n",
    "        distances[idx] = np.sqrt(dy * dy + dx * dx)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def create_grid_index_arrays(points_y, points_x, grid_size, ny, nx):\n",
    "    \"\"\"\n",
    "    Creates a grid-based spatial index using numpy arrays.\n",
    "    \"\"\"\n",
    "    n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "    n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "    max_points_per_cell = len(points_y)\n",
    "    \n",
    "    grid_points = np.full((n_grids_y, n_grids_x, max_points_per_cell), -1, dtype=np.int32)\n",
    "    grid_counts = np.zeros((n_grids_y, n_grids_x), dtype=np.int32)\n",
    "    \n",
    "    for idx in range(len(points_y)):\n",
    "        grid_y = min(points_y[idx] // grid_size, n_grids_y - 1)\n",
    "        grid_x = min(points_x[idx] // grid_size, n_grids_x - 1)\n",
    "        count = grid_counts[grid_y, grid_x]\n",
    "        if count < max_points_per_cell:\n",
    "            grid_points[grid_y, grid_x, count] = idx\n",
    "            grid_counts[grid_y, grid_x] += 1\n",
    "    \n",
    "    return grid_points, grid_counts\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def calculate_wrapped_distance(y1, x1, y2, x2, nx, half_nx):\n",
    "    \"\"\"\n",
    "    Calculate distance with periodic boundary conditions in x dimension.\n",
    "    \"\"\"\n",
    "    dy = y1 - y2\n",
    "    dx = x1 - x2\n",
    "    \n",
    "    if dx > half_nx:\n",
    "        dx -= nx\n",
    "    elif dx < -half_nx:\n",
    "        dx += nx\n",
    "        \n",
    "    return np.sqrt(dy * dy + dx * dx)\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def get_nearest_parent_labels(child_mask, parent_masks, child_ids, parent_centroids, Nx, max_distance=20):\n",
    "    \"\"\"\n",
    "    Assigns labels based on nearest parent blob points.\n",
    "    This is quite computationally-intensive, so we utilise many optimisations here...\n",
    "    \"\"\"\n",
    "    \n",
    "    ny, nx = child_mask.shape\n",
    "    half_Nx = Nx / 2\n",
    "    n_parents = len(parent_masks)\n",
    "    grid_size = max(2, max_distance // 4)\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(child_mask)\n",
    "    n_child_points = len(y_indices)\n",
    "    \n",
    "    min_distances = np.full(n_child_points, np.inf)\n",
    "    parent_assignments = np.zeros(n_child_points, dtype=np.int32)\n",
    "    found_close = np.zeros(n_child_points, dtype=np.bool_)\n",
    "    \n",
    "    for parent_idx in range(n_parents):\n",
    "        py, px = np.nonzero(parent_masks[parent_idx])\n",
    "        \n",
    "        if len(py) == 0:  # Skip empty parents\n",
    "            continue\n",
    "            \n",
    "        # Create grid index for this parent\n",
    "        n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "        n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "        grid_points, grid_counts = create_grid_index_arrays(py, px, grid_size, ny, nx)\n",
    "        \n",
    "        # Process child points in parallel\n",
    "        for child_idx in prange(n_child_points):\n",
    "            if found_close[child_idx]:  # Skip if we already found an exact match\n",
    "                continue\n",
    "                \n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            grid_y = min(child_y // grid_size, n_grids_y - 1)\n",
    "            grid_x = min(child_x // grid_size, n_grids_x - 1)\n",
    "            \n",
    "            min_dist_to_parent = np.inf\n",
    "            \n",
    "            # Check nearby grid cells\n",
    "            for dy in range(-1, 2):\n",
    "                grid_y_check = (grid_y + dy) % n_grids_y\n",
    "                \n",
    "                for dx in range(-1, 2):\n",
    "                    grid_x_check = (grid_x + dx) % n_grids_x\n",
    "                    \n",
    "                    # Process points in this grid cell\n",
    "                    n_points = grid_counts[grid_y_check, grid_x_check]\n",
    "                    \n",
    "                    for p_idx in range(n_points):\n",
    "                        point_idx = grid_points[grid_y_check, grid_x_check, p_idx]\n",
    "                        if point_idx == -1:\n",
    "                            break\n",
    "                        \n",
    "                        dist = calculate_wrapped_distance(\n",
    "                            child_y, child_x,\n",
    "                            py[point_idx], px[point_idx],\n",
    "                            Nx, half_Nx\n",
    "                        )\n",
    "                        \n",
    "                        if dist > max_distance:\n",
    "                            continue\n",
    "                        \n",
    "                        if dist < min_dist_to_parent:\n",
    "                            min_dist_to_parent = dist\n",
    "                            \n",
    "                        if dist < 1e-6:  # Found exact same point (within numerical precision)\n",
    "                            min_dist_to_parent = dist\n",
    "                            found_close[child_idx] = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_close[child_idx]:\n",
    "                        break\n",
    "                \n",
    "                if found_close[child_idx]:\n",
    "                    break\n",
    "            \n",
    "            # Update assignment if this parent is closer\n",
    "            if min_dist_to_parent < min_distances[child_idx]:\n",
    "                min_distances[child_idx] = min_dist_to_parent\n",
    "                parent_assignments[child_idx] = parent_idx\n",
    "    \n",
    "    # Handle any unassigned points using centroids\n",
    "    unassigned = min_distances == np.inf\n",
    "    if np.any(unassigned):\n",
    "        for child_idx in np.nonzero(unassigned)[0]:\n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            min_dist = np.inf\n",
    "            best_parent = 0\n",
    "            \n",
    "            for parent_idx in range(n_parents):\n",
    "                # Calculate distance to centroid with periodic boundary conditions\n",
    "                dist = calculate_wrapped_distance(\n",
    "                    child_y, child_x,\n",
    "                    parent_centroids[parent_idx, 0],\n",
    "                    parent_centroids[parent_idx, 1],\n",
    "                    Nx, half_Nx\n",
    "                )\n",
    "                \n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_parent = parent_idx\n",
    "                    \n",
    "            parent_assignments[child_idx] = best_parent\n",
    "    \n",
    "    # Convert from parent indices to child_ids\n",
    "    new_labels = child_ids[parent_assignments]\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def get_nearest_parent_labels_unstructured(child_mask, parent_masks, child_ids, parent_centroids, neighbours_int, lat, lon, max_distance=20):\n",
    "    \"\"\"\n",
    "    Optimised version of nearest parent label assignment for unstructured grids.\n",
    "    Uses numpy arrays throughout to ensure Numba compatibility.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    child_mask : np.ndarray\n",
    "        1D boolean array where True indicates points in the child blob\n",
    "    parent_masks : np.ndarray\n",
    "        2D boolean array of shape (n_parents, n_points) where True indicates points in each parent blob\n",
    "    child_ids : np.ndarray\n",
    "        1D array containing the IDs to assign to each partition of the child blob\n",
    "    parent_centroids : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (lat, lon) coordinates of parent centroids in degrees\n",
    "    neighbours_int : np.ndarray\n",
    "        2D array of shape (3, n_points) containing indices of neighboring cells for each point\n",
    "    lat / lon : np.ndarray\n",
    "        Latitude/Longitude in degrees\n",
    "    max_distance : int, optional\n",
    "        Maximum number of edge hops to search for parent points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_labels : np.ndarray\n",
    "        1D array containing the assigned child_ids for each True point in child_mask\n",
    "    \"\"\"\n",
    "    \n",
    "    # Force contiguous arrays in memory for optimal vectorised performance (from indexing)\n",
    "    child_mask = np.ascontiguousarray(child_mask)\n",
    "    parent_masks = np.ascontiguousarray(parent_masks)\n",
    "    \n",
    "    n_points = len(child_mask)\n",
    "    n_parents = len(parent_masks)\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    distances = np.full(n_points, np.inf, dtype=np.float32)\n",
    "    parent_assignments = np.full(n_points, -1, dtype=np.int32)\n",
    "    visited = np.zeros((n_parents, n_points), dtype=np.bool_)\n",
    "    \n",
    "    # Initialise with direct overlaps\n",
    "    for parent_idx in range(n_parents):\n",
    "        overlap_mask = parent_masks[parent_idx] & child_mask\n",
    "        if np.any(overlap_mask):\n",
    "            visited[parent_idx, overlap_mask] = True\n",
    "            unclaimed_overlap = distances[overlap_mask] == np.inf\n",
    "            if np.any(unclaimed_overlap):\n",
    "                overlap_points = np.where(overlap_mask)[0]\n",
    "                valid_points = overlap_points[unclaimed_overlap]\n",
    "                distances[valid_points] = 0\n",
    "                parent_assignments[valid_points] = parent_idx\n",
    "    \n",
    "    # Pre-compute trig values\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    cos_lat = np.cos(lat_rad)\n",
    "    \n",
    "    # Graph traversal for remaining points\n",
    "    current_distance = 0\n",
    "    any_unassigned = np.any(child_mask & (parent_assignments == -1))\n",
    "    \n",
    "    while current_distance < max_distance and any_unassigned:\n",
    "        current_distance += 1\n",
    "        updates_made = False\n",
    "        \n",
    "        for parent_idx in range(n_parents):\n",
    "            # Get current frontier points\n",
    "            frontier_mask = visited[parent_idx]\n",
    "            if not np.any(frontier_mask):\n",
    "                continue\n",
    "            \n",
    "            # Process neighbors\n",
    "            for i in range(3):  # For each neighbor direction\n",
    "                neighbors = neighbours_int[i, frontier_mask]\n",
    "                valid_neighbors = neighbors >= 0\n",
    "                if not np.any(valid_neighbors):\n",
    "                    continue\n",
    "                    \n",
    "                valid_points = neighbors[valid_neighbors]\n",
    "                unvisited = ~visited[parent_idx, valid_points]\n",
    "                new_points = valid_points[unvisited]\n",
    "                \n",
    "                if len(new_points) > 0:\n",
    "                    visited[parent_idx, new_points] = True\n",
    "                    update_mask = distances[new_points] > current_distance\n",
    "                    if np.any(update_mask):\n",
    "                        points_to_update = new_points[update_mask]\n",
    "                        distances[points_to_update] = current_distance\n",
    "                        parent_assignments[points_to_update] = parent_idx\n",
    "                        updates_made = True\n",
    "        \n",
    "        if not updates_made:\n",
    "            break\n",
    "            \n",
    "        any_unassigned = np.any(child_mask & (parent_assignments == -1))\n",
    "    \n",
    "    # Handle remaining unassigned points using great circle distances\n",
    "    unassigned_mask = child_mask & (parent_assignments == -1)\n",
    "    if np.any(unassigned_mask):\n",
    "        parent_lat_rad = np.deg2rad(parent_centroids[:, 0])\n",
    "        parent_lon_rad = np.deg2rad(parent_centroids[:, 1])\n",
    "        cos_parent_lat = np.cos(parent_lat_rad)\n",
    "        \n",
    "        unassigned_points = np.where(unassigned_mask)[0]\n",
    "        for point in unassigned_points:\n",
    "            # Vectorised haversine calculation\n",
    "            dlat = parent_lat_rad - lat_rad[point]\n",
    "            dlon = parent_lon_rad - lon_rad[point]\n",
    "            a = np.sin(dlat/2)**2 + cos_lat[point] * cos_parent_lat * np.sin(dlon/2)**2\n",
    "            dist = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "            parent_assignments[point] = np.argmin(dist)\n",
    "    \n",
    "    # Return only the assignments for points in child_mask\n",
    "    child_points = np.where(child_mask)[0]\n",
    "    return child_ids[parent_assignments[child_points]]\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def unstructured_centroid_partition(child_mask, parent_centroids, child_ids, lat, lon):\n",
    "    \"\"\"\n",
    "    Assigns labels to child cells based on closest parent centroid using great circle distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    child_mask : np.ndarray\n",
    "        1D boolean array indicating which cells belong to the child blob\n",
    "    parent_centroids : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (lat, lon) coordinates of parent centroids in degrees\n",
    "    child_ids : np.ndarray\n",
    "        Array of IDs to assign to each partition of the child blob\n",
    "    lat / lon : np.ndarray\n",
    "        Latitude/Longitude in degrees\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    new_labels : np.ndarray\n",
    "        1D array containing assigned child_ids for cells in child_mask\n",
    "    \"\"\"\n",
    "    n_cells = len(child_mask)\n",
    "    n_parents = len(parent_centroids)\n",
    "    \n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    parent_coords_rad = np.deg2rad(parent_centroids)\n",
    "    \n",
    "    new_labels = np.zeros(n_cells, dtype=child_ids.dtype)\n",
    "    \n",
    "    # Process each child cell in parallel\n",
    "    for i in prange(n_cells):\n",
    "        if not child_mask[i]:\n",
    "            continue\n",
    "            \n",
    "        min_dist = np.inf\n",
    "        closest_parent = 0\n",
    "        \n",
    "        # Calculate great circle distance to each parent centroid\n",
    "        for j in range(n_parents):\n",
    "            dlat = parent_coords_rad[j, 0] - lat_rad[i]\n",
    "            dlon = parent_coords_rad[j, 1] - lon_rad[i]\n",
    "            \n",
    "            # Use haversine formula for great circle distance\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat_rad[i]) * np.cos(parent_coords_rad[j, 0]) * np.sin(dlon/2)**2\n",
    "            dist = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_parent = j\n",
    "        \n",
    "        new_labels[i] = child_ids[closest_parent]\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Helper Function for Super Fast Sparse Bool Multiply (*without the scipy+Dask Memory Leak*)\n",
    "@njit(fastmath=True, parallel=True)\n",
    "def sparse_bool_power(vec, sp_data, indices, indptr, exponent):\n",
    "    vec = vec.T\n",
    "    num_rows = indptr.size - 1\n",
    "    num_cols = vec.shape[1]\n",
    "    result = vec.copy()\n",
    "\n",
    "    for _ in range(exponent):\n",
    "        temp_result = np.zeros((num_rows, num_cols), dtype=np.bool_)\n",
    "\n",
    "        for i in prange(num_rows):\n",
    "            for j in range(indptr[i], indptr[i + 1]):\n",
    "                if sp_data[j]:\n",
    "                    for k in range(num_cols):\n",
    "                        if result[indices[j], k]:\n",
    "                            temp_result[i, k] = True\n",
    "\n",
    "        result = temp_result\n",
    "\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "79ec0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_data, chunk_data_m1, chunk_data_p1, merging_blobs, next_id_start, lat, lon, area, neighbours_int):\n",
    "    \"\"\"Process a single chunk of merging blobs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_data : numpy.ndarray\n",
    "        Array of shape (n_time, ncells) for unstructured or (n_time, ny, nx) for structured\n",
    "    chunk_data_m1 & chunk_data_p1 : numpy.ndarray\n",
    "        Same as chunk_data but shifted by 1 in time\n",
    "    merging_blobs : numpy.ndarray\n",
    "        Array of shape (n_time, max_merges) containing merging blob IDs (0=none)\n",
    "    next_id_start : numpy.ndarray\n",
    "        Array of shape (n_time, max_merges) containing ID offsets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing updates for each timestep\n",
    "    \"\"\"\n",
    "    n_time = chunk_data.shape[0]\n",
    "    updates_by_time = []\n",
    "    merge_events_by_time = []\n",
    "    new_merging_blobs_just_end = []\n",
    "    id_mappings_by_time = []\n",
    "    \n",
    "    # Pre-Convert lat/lon to Cartesian\n",
    "    x = np.cos(np.radians(lat)) * np.cos(np.radians(lon))\n",
    "    y = np.cos(np.radians(lat)) * np.sin(np.radians(lon))\n",
    "    z = np.sin(np.radians(lat))\n",
    "\n",
    "    # Process each timestep\n",
    "    for t in range(n_time):\n",
    "        # Initialise tracking variables\n",
    "        updates = []\n",
    "        merge_events = {\n",
    "            'child_ids': [],\n",
    "            'parent_ids': [],\n",
    "            'areas': []\n",
    "        }\n",
    "        new_merging_blobs = set()\n",
    "        id_mapping = {}\n",
    "        next_new_id = next_id_start[t]  # Use the offset for this timestep\n",
    "        \n",
    "        # Get current time slice data\n",
    "        if t == 0:\n",
    "            data_m1 = chunk_data_m1[t]\n",
    "            data_t = chunk_data[t]\n",
    "        else:\n",
    "            data_m1 = data_t\n",
    "            data_t = data_p1\n",
    "        data_p1 = chunk_data_p1[t]\n",
    "        \n",
    "        # Get non-zero merging blobs for this timestep\n",
    "        merging_blobs_t = merging_blobs[t]\n",
    "        merging_blobs_t = merging_blobs_t[merging_blobs_t > 0]\n",
    "        \n",
    "        if len(merging_blobs_t) == 0:\n",
    "            # Store empty results for this timestep\n",
    "            updates_by_time.append(updates)\n",
    "            merge_events_by_time.append(merge_events)\n",
    "            id_mappings_by_time.append(id_mapping)\n",
    "            new_merging_blobs_just_end.append(set())\n",
    "            continue\n",
    "        \n",
    "        # Process each merging blob at this timestep\n",
    "        blobs_to_process = list(merging_blobs_t)\n",
    "        \n",
    "        \n",
    "        while blobs_to_process:\n",
    "            child_id = blobs_to_process.pop(0)\n",
    "            \n",
    "            # Get child mask and find overlapping parents\n",
    "            child_mask_2d = (data_t == child_id)\n",
    "            \n",
    "            # Find parent blobs that overlap with this child\n",
    "            parent_masks = []\n",
    "            parent_centroids = []\n",
    "            parent_ids = []\n",
    "            parent_areas = []\n",
    "            overlap_areas = []\n",
    "            \n",
    "            # Find all unique parent IDs that overlap with the child\n",
    "            potential_parents = np.unique(data_m1[child_mask_2d])\n",
    "            for parent_id in potential_parents[potential_parents > 0]:\n",
    "                parent_mask = (data_m1 == parent_id)\n",
    "                if np.any(parent_mask & child_mask_2d):\n",
    "                    \n",
    "                    # Check if overlap area is large enough\n",
    "                    area_0 = area[parent_mask].sum()  # Parent area\n",
    "                    area_1 = area[child_mask_2d].sum()\n",
    "                    min_area = np.minimum(area_0, area_1)\n",
    "                    overlap_area = area[parent_mask & child_mask_2d].sum()\n",
    "                    overlap_fraction = overlap_area / min_area\n",
    "                    \n",
    "                    if overlap_fraction < tracker.overlap_threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    overlap_areas.append(overlap_area)\n",
    "                    parent_masks.append(parent_mask)\n",
    "                    parent_ids.append(parent_id)\n",
    "                    \n",
    "                    # Calculate centroid for this parent\n",
    "                    mask_area = area[parent_mask]\n",
    "                    weighted_x = np.sum(mask_area * x[parent_mask])\n",
    "                    weighted_y = np.sum(mask_area * y[parent_mask])\n",
    "                    weighted_z = np.sum(mask_area * z[parent_mask])\n",
    "                    \n",
    "                    norm = np.sqrt(weighted_x**2 + weighted_y**2 + weighted_z**2)\n",
    "                    \n",
    "                    # Convert back to lat/lon\n",
    "                    centroid_lat = np.degrees(np.arcsin(weighted_z/norm))\n",
    "                    centroid_lon = np.degrees(np.arctan2(weighted_y, weighted_x))\n",
    "                    \n",
    "                    # Fix longitude range to [-180, 180]\n",
    "                    if centroid_lon > 180:\n",
    "                        centroid_lon -= 360\n",
    "                    elif centroid_lon < -180:\n",
    "                        centroid_lon += 360\n",
    "                    \n",
    "                    parent_centroids.append([centroid_lat, centroid_lon])\n",
    "                    parent_areas.append(area_0) \n",
    "            \n",
    "            if len(parent_ids) < 2:  # Need at least 2 parents for merging\n",
    "                continue\n",
    "                \n",
    "            parent_masks = np.array(parent_masks)\n",
    "            parent_centroids = np.array(parent_centroids, dtype=np.float32)\n",
    "            parent_ids = np.array(parent_ids)\n",
    "            parent_areas = np.array(parent_areas)\n",
    "            overlap_areas = np.array(overlap_areas)\n",
    "            \n",
    "            # Create new IDs for each partition\n",
    "            new_child_ids = np.arange(next_new_id, next_new_id + (len(parent_ids) - 1), dtype=np.int32)\n",
    "            child_ids = np.concatenate((np.array([child_id], dtype=np.int32), new_child_ids))\n",
    "            \n",
    "            # Update ID tracking\n",
    "            for new_id in child_ids[1:]:\n",
    "                id_mapping[new_id] = None\n",
    "            next_new_id += len(parent_ids) - 1\n",
    "            \n",
    "            # Get new labels based on partitioning method\n",
    "            if tracker.nn_partitioning:\n",
    "                # Estimate max_area from number of cells\n",
    "                max_area = parent_areas.max() / tracker.mean_cell_area\n",
    "                max_distance = int(np.sqrt(max_area) * 2.0)\n",
    "                \n",
    "                new_labels = get_nearest_parent_labels_unstructured(\n",
    "                    child_mask_2d,\n",
    "                    parent_masks,\n",
    "                    child_ids,\n",
    "                    parent_centroids,\n",
    "                    neighbours_int,\n",
    "                    lat,\n",
    "                    lon,\n",
    "                    max_distance=max(max_distance, 20)*2\n",
    "                )\n",
    "            else:\n",
    "                new_labels = unstructured_centroid_partition(\n",
    "                    child_mask_2d,\n",
    "                    parent_centroids,\n",
    "                    child_ids,\n",
    "                    lat,\n",
    "                    lon\n",
    "                )\n",
    "            \n",
    "            # Update slice data\n",
    "            data_t[child_mask_2d] = new_labels\n",
    "            spatial_indices_all = np.where(child_mask_2d)[0]\n",
    "            \n",
    "            for new_id in child_ids[1:]:\n",
    "                # Get spatial indices where we need to update\n",
    "                new_id_mask = (new_labels == new_id)\n",
    "                spatial_indices = spatial_indices_all[new_id_mask]\n",
    "                \n",
    "                # Store the updates\n",
    "                updates.append({\n",
    "                    'spatial_indices': spatial_indices,\n",
    "                    'new_label': new_id\n",
    "                })\n",
    "            \n",
    "            # Record merge event\n",
    "            merge_events['child_ids'].append(child_ids)\n",
    "            merge_events['parent_ids'].append(parent_ids)\n",
    "            merge_events['areas'].append(overlap_areas)\n",
    "            \n",
    "            # Find all child blobs in the next timestep that overlap with our newly labeled regions\n",
    "            new_merging = []\n",
    "            for new_id in child_ids:\n",
    "                parent_mask = (data_t == new_id)                        \n",
    "                potential_children = np.unique(data_p1[parent_mask])\n",
    "                area_0 = area[parent_mask].sum()\n",
    "                \n",
    "                for potential_child in potential_children:\n",
    "                    # Check if overlap area is large enough\n",
    "                    potential_child_mask = (data_p1==potential_child)\n",
    "                    area_1 = area[potential_child_mask].sum()\n",
    "                    min_area = np.minimum(area_0, area_1)\n",
    "                    overlap_area = area[parent_mask & potential_child_mask].sum()\n",
    "                    overlap_fraction = overlap_area / min_area\n",
    "                    \n",
    "                    if overlap_fraction > tracker.overlap_threshold:\n",
    "                        new_merging.append(potential_child)                        \n",
    "            \n",
    "            # Add to new merging blobs set\n",
    "            new_merging_blobs.update(new_merging)\n",
    "            \n",
    "            # Add to processing queue if not already processed\n",
    "            for new_blob_id in new_merging:\n",
    "                if new_blob_id not in blobs_to_process and new_blob_id not in merging_blobs_t:\n",
    "                    blobs_to_process.append(new_blob_id)\n",
    "        \n",
    "\n",
    "        # Store results for this timestep\n",
    "        updates_by_time.append(updates)\n",
    "        merge_events_by_time.append(merge_events)\n",
    "        id_mappings_by_time.append(id_mapping)\n",
    "        if t < n_time - 1:\n",
    "            new_merging_blobs_just_end.append(set())\n",
    "        else:\n",
    "            new_merging_blobs_just_end.append(new_merging_blobs)\n",
    "    \n",
    "    # Outputs need to be an array with dimension n_time:\n",
    "    \n",
    "    results_dict = {\n",
    "        'merge_events': merge_events_by_time,\n",
    "        'id_mappings': id_mappings_by_time,\n",
    "        'next_chunk_merge': new_merging_blobs_just_end\n",
    "    }\n",
    "\n",
    "    # Create a list to hold dictionaries for each time step\n",
    "    time_step_dicts = []\n",
    "\n",
    "    for t in range(n_time):\n",
    "        time_step_dict = {key: value[t] for key, value in results_dict.items()}\n",
    "        time_step_dicts.append(time_step_dict)\n",
    "\n",
    "    time_step_array = np.array(time_step_dicts, dtype=object)\n",
    "    updates_by_time_array = np.array(updates_by_time, dtype=object)\n",
    "    \n",
    "    return time_step_array, updates_by_time_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e96d2655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:31:10,962 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.93 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:10,978 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 5.98 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:11,097 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.54 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:11,114 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 5.48 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:11,670 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 5.98 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:12,056 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 5.93 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:12,061 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.54 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:12,161 - distributed.worker.memory - WARNING - Worker is at 27% memory usage. Resuming worker. Process memory: 2.02 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,155 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.03 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,218 - distributed.worker.memory - WARNING - Worker is at 24% memory usage. Resuming worker. Process memory: 1.80 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,346 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 5.99 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,448 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 5.68 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,626 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.03 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:13,650 - distributed.worker.memory - WARNING - Worker is at 41% memory usage. Resuming worker. Process memory: 3.02 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:15,885 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 5.98 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:16,035 - distributed.worker.memory - WARNING - Worker is at 28% memory usage. Resuming worker. Process memory: 2.07 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:17,581 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 6.42 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:17,659 - distributed.worker.memory - WARNING - Worker is at 42% memory usage. Resuming worker. Process memory: 3.14 GiB -- Worker memory limit: 7.36 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Finding Overlapping Blobs.\n"
     ]
    }
   ],
   "source": [
    "# Compile List of Overlapping Blob ID Pairs Across Time\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field_unique, blob_props)  # List blob pairs that overlap by at least overlap_threshold percent\n",
    "if tracker.verbosity > 0:    print('Finished Finding Overlapping Blobs.')\n",
    "\n",
    "# Find initial merging blobs\n",
    "unique_children, children_counts = np.unique(overlap_blobs_list[:, 1], return_counts=True)\n",
    "merging_blobs = set(unique_children[children_counts > 1])\n",
    "\n",
    "\n",
    "## Process chunks iteratively until no new merging blobs remain\n",
    "iteration = 0\n",
    "max_iterations = 10\n",
    "processed_chunks = set()\n",
    "global_id_counter = blob_props.ID.max().item() + 1\n",
    "\n",
    "# Initialise global merge event tracking\n",
    "all_merge_events = {\n",
    "    'times': [],\n",
    "    'child_ids': [],\n",
    "    'parent_ids': [],\n",
    "    'areas': []\n",
    "}\n",
    "\n",
    "n_time = len(blob_id_field_unique[tracker.timedim])\n",
    "time_indices = xr.DataArray(np.arange(n_time),dims=[tracker.timedim],coords={tracker.timedim: blob_id_field_unique[tracker.timedim]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7927b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parallel Iteration 1 with 6 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:31:25,171 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.01 GiB -- Worker memory limit: 7.36 GiB\n",
      "2025-01-31 11:31:25,246 - distributed.worker.memory - WARNING - Worker is at 31% memory usage. Resuming worker. Process memory: 2.32 GiB -- Worker memory limit: 7.36 GiB\n"
     ]
    }
   ],
   "source": [
    "# WHILE merging_blobs:::::\n",
    "\n",
    "if tracker.verbosity > 0:    print(f\"Processing Parallel Iteration {iteration + 1} with {len(merging_blobs)} Merging Blobs...\")\n",
    "\n",
    "# Pre-compute the child_time_idx for merging_blobs\n",
    "time_index_map = tracker.compute_id_time_dict(blob_id_field_unique, list(merging_blobs), global_id_counter)\n",
    "\n",
    "# Create the uniform merging blobs array\n",
    "max_merges = max(len([b for b in merging_blobs if time_index_map.get(b, -1) == t]) for t in range(n_time))\n",
    "\n",
    "uniform_merging_blobs_array = np.array([\n",
    "    np.pad([b for b in merging_blobs if time_index_map.get(b, -1) == t], (0, max_merges - len([b for b in merging_blobs if time_index_map.get(b, -1) == t])), 'constant')\n",
    "    for t in range(n_time)\n",
    "])\n",
    "merging_blobs_da = xr.DataArray(\n",
    "    uniform_merging_blobs_array,\n",
    "    dims=[tracker.timedim, 'merges'],\n",
    "    coords={tracker.timedim: blob_id_field_unique[tracker.timedim]})\n",
    "\n",
    "next_id_offsets = np.arange(n_time) * max_merges\n",
    "next_id_offsets_da = xr.DataArray(next_id_offsets,\n",
    "                                dims=[tracker.timedim],\n",
    "                                coords={tracker.timedim: blob_id_field_unique[tracker.timedim]})\n",
    "\n",
    "blob_id_field_unique_p1 = blob_id_field_unique.shift({tracker.timedim: -1}, fill_value=0)\n",
    "blob_id_field_unique_m1 = blob_id_field_unique.shift({tracker.timedim: 1}, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1a0f3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 4  # Or whatever size is appropriate\n",
    "blob_id_field_unique = blob_id_field_unique.chunk({tracker.timedim: chunk_size})\n",
    "blob_id_field_unique_m1 = blob_id_field_unique_m1.chunk({tracker.timedim: chunk_size})\n",
    "blob_id_field_unique_p1 = blob_id_field_unique_p1.chunk({tracker.timedim: chunk_size})\n",
    "merging_blobs_da = merging_blobs_da.chunk({tracker.timedim: chunk_size})\n",
    "next_id_offsets_da = next_id_offsets_da.chunk({tracker.timedim: chunk_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, updates = xr.apply_ufunc(process_chunk,\n",
    "                                 blob_id_field_unique,\n",
    "                                 blob_id_field_unique_m1,\n",
    "                                 blob_id_field_unique_p1,\n",
    "                                 merging_blobs_da,\n",
    "                                 next_id_offsets_da,\n",
    "                                 blob_id_field_unique.lat,\n",
    "                                 blob_id_field_unique.lon,\n",
    "                                 tracker.cell_area,\n",
    "                                 tracker.neighbours_int,\n",
    "                                 input_core_dims=[[tracker.xdim], [tracker.xdim], [tracker.xdim], [], [], [tracker.xdim], [tracker.xdim], [tracker.xdim], ['nv', tracker.xdim]],\n",
    "                                 output_core_dims=[[], []],\n",
    "                                 output_dtypes=[object, object],\n",
    "                                 vectorize=False,\n",
    "                                 dask='parallelized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, updates = persist(results, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "994b63d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resultsc \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:1207\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:1175\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1175\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataset.py:899\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 899\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/namedarray/daskmanager.py:85\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:655\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m()\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m'\u001b[39m, axes)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "resultsc = results.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aafd5469",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id_mappings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:892\u001b[0m, in \u001b[0;36mDataArray._getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_mappings'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2517465/3746930420.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_mappings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_coord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;31m# xarray-style array indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_key_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mdim_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0msplit_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mref_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_mappings'"
     ]
    }
   ],
   "source": [
    "results.isel(time=1).isel(merges=0)['id_mappings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "782e44e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id_mappings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:892\u001b[0m, in \u001b[0;36mDataArray._getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_mappings'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2517465/2285747416.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m temp_id_arrays = [\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_mappings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# time dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# merges dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2517465/2285747416.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m temp_id_arrays = [\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_coord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;31m# xarray-style array indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_key_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mdim_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_virtual_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0msplit_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mref_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_mappings'"
     ]
    }
   ],
   "source": [
    "temp_id_arrays = [\n",
    "    np.array(list(res['id_mappings'].keys()), dtype=np.int64) \n",
    "    for t in range(results.sizes[results.dims[0]])     # time dimension\n",
    "    for m in range(results.sizes[results.dims[1]])     # merges dimension\n",
    "    for res in [results.isel({results.dims[0]: t, results.dims[1]: m})]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_id_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blobs = blobs.compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c88a6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked_unstruct.zarr'\n",
    "blobs.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
