{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves on _Unstructured Grid_ using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill spatial holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Fill gaps in time -- permitting up to `T_fill` missing time slices, while keeping the same blob ID.\n",
    "3. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "4. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "5. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "6. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "7. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 5km resolution on an Unstructured Grid (15 million cells) using 32 cores takes \n",
    "- Full Split/Merge Thresholding & Merge Tracking:  ~40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 7.87 GB\n",
      "Hostname is  l40329\n",
      "Forward Port = l40329:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:20:04,770 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35407 (pid=4008375) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,098 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38275 (pid=4008172) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,125 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:06,197 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40111 (pid=4008184) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,201 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44557 (pid=4008196) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,265 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39505 (pid=4008277) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,272 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38483 (pid=4008293) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,276 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41663 (pid=4008318) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,283 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36197 (pid=4008342) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,289 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45899 (pid=4008369) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,327 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42539 (pid=4008345) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,490 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46041 (pid=4008136) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:06,501 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36817 (pid=4008201) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:07,457 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46705 (pid=4008228) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:07,752 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40577 (pid=4008221) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:09,096 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38771 (pid=4008156) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:09,111 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,159 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,162 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,165 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,214 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,255 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35011 (pid=4008246) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:20:09,279 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,282 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,326 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,329 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,332 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,340 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,386 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:09,551 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:10,534 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:20:10,554 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:24:20,557 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41885 (pid=4008288) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:24:22,531 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45459 (pid=4008357) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:24:22,690 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:24:23,405 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:24:23,669 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:34759 (pid=4008330) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:24:24,720 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:24:31,702 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40061 (pid=4008212) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:24:32,949 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:24:38,719 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35983 (pid=4008302) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:24:40,356 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:25:54,905 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38397 (pid=4008262) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:25:56,174 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:26:12,181 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39327 (pid=4008307) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:26:15,211 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:28:29,767 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43731 (pid=4008354) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:28:30,305 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40677 (pid=4008285) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:28:30,568 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40985 (pid=4008168) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:28:30,661 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:28:31,395 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:28:31,845 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:28:33,232 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46301 (pid=4008177) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:28:34,476 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:29:21,495 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36671 (pid=4008326) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:29:23,372 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:32:23,702 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46403 (pid=4008237) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:32:24,554 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:33:44,007 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43881 (pid=4008241) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:33:46,337 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:35:15,948 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44779 (pid=4015621) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:35:16,932 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:36:32,441 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38019 (pid=4015623) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:36:33,456 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:36:43,069 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35361 (pid=4008224) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:36:45,502 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:36:56,554 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42841 (pid=4008297) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:36:59,061 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:37:59,147 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41015 (pid=4008191) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:38:01,161 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:38:44,510 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43315 (pid=4015611) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:38:45,860 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:40:01,495 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43699 (pid=4008148) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:40:02,707 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:40:03,240 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35137 (pid=4008152) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:40:05,084 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:40:23,806 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44849 (pid=4008349) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:40:25,664 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:41:35,017 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:34931 (pid=4015599) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:41:36,227 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:43:32,712 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40733 (pid=4008322) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:43:33,582 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:44:08,510 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40351 (pid=4015633) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:44:09,603 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:44:12,962 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33451 (pid=4008365) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:44:14,086 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:44:53,837 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33473 (pid=4008131) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:44:54,815 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:44:59,222 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42461 (pid=4008257) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:45:00,864 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:45:09,422 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44037 (pid=4015565) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:45:11,074 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:45:42,979 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40005 (pid=4008314) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:45:44,933 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:45:45,632 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39857 (pid=4008233) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:45:47,853 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:45:59,697 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36941 (pid=4015716) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:46:03,345 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:46:05,562 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36797 (pid=4008274) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:46:07,858 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:46:28,455 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45677 (pid=4008269) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:46:32,132 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:49:42,349 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38937 (pid=4015639) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:49:43,302 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:50:38,303 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:37251 (pid=4019902) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:50:39,265 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:50:42,988 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35079 (pid=4008128) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:50:43,042 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44603 (pid=4020045) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:50:46,085 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:50:46,197 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:51:09,345 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41213 (pid=4008189) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:51:12,103 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:52:00,229 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:34407 (pid=4008209) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:52:02,221 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:54:22,616 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36469 (pid=4022501) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:54:23,781 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:55:04,666 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46079 (pid=4008335) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:55:05,532 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:56:58,331 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39881 (pid=4019869) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:56:59,374 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:57:30,405 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40093 (pid=4020331) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:57:32,253 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 12:59:51,440 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45653 (pid=4022011) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 12:59:52,427 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:00:01,064 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45759 (pid=4023120) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:00:02,206 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:00:05,911 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38633 (pid=4008340) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:00:06,889 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:00:08,566 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42397 (pid=4019886) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:00:08,803 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:37239 (pid=4024082) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:00:09,366 - distributed.scheduler - ERROR - Task ('process_chunk-5d53519d6c79f33fbd7c89a0c1c6136d', 19) marked as failed because 4 workers died while trying to run it\n",
      "2025-02-13 13:00:09,582 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:00:10,099 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:00:33,051 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43335 (pid=4015721) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:00:33,992 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:01:09,148 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35333 (pid=4015648) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:01:10,139 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:01:58,408 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:37677 (pid=4022646) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:01:59,246 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-13 13:03:35,788 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:40367 (pid=4008140) exceeded 95% memory budget. Restarting...\n",
      "2025-02-13 13:03:36,757 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "#  N.B.: Need ~ 8 GB per worker (for 5km data // 15 million points)\n",
    "client = hpc.StartLocalCluster(n_workers=64, n_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary_unstruct.zarr'\n",
    "chunk_size = {'time': 4, 'ncells': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks={}).isel(time=slice(0, 128)).chunk(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.8  # Remove the smallest 80% of the identified blobs\n",
    "hole_filling_radius = 32  # Fill small holes with radius < 32 elements, i.e. ~100 km\n",
    "time_gap_fill = 2         # Allow gaps of 4 days and still continue the blob tracking with the same ID\n",
    "allow_merging = True      # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5   # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True    # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Constructing the Sparse Dilation Matrix.\n",
      "Finished Filling Spatial Holes\n",
      "Finished Filling Spatio-temporal Holes.\n",
      "Finished Filtering Small Blobs.\n",
      "Finished Blob Identification.\n",
      "Finished Making Blobs Globally Unique.\n",
      "Finished Calculating Blob Properties.\n",
      "Finished Finding Overlapping Blobs.\n",
      "Processing Parallel Iteration 1 with 27 Merging Blobs...\n",
      "Processing Parallel Iteration 2 with 20 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:12:22,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.61 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parallel Iteration 3 with 5 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:14:31,781 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:32,729 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:32,881 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:33,244 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:33,282 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.02 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:33,713 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,017 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,107 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.99 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,187 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,293 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,609 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,693 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,709 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,768 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,859 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,908 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,924 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:34,993 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,016 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,073 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,240 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.96 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,333 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,438 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,565 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,626 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:14:35,726 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parallel Iteration 4 with 4 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:17:13,663 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:17:14,137 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.64 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:17:15,060 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:17:15,609 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.66 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parallel Iteration 5 with 1 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:19:14,832 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:14,932 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:14,941 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:15,032 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:15,804 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:15,850 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,023 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,361 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,381 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,461 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,749 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:16,917 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,074 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,173 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,366 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,450 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,569 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,649 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,670 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,750 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,768 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,772 - distributed.worker.memory - WARNING - Worker is at 43% memory usage. Resuming worker. Process memory: 3.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:17,850 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,005 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,291 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,352 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,396 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,473 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.00 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,490 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,504 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,516 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,575 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.61 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,590 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,595 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,651 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,710 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,774 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,828 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,890 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,895 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,902 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,907 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.00 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,935 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,952 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:18,954 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,009 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,014 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,031 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,034 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,074 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,076 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.00 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,135 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,146 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,151 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,153 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,214 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,218 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,244 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,246 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,256 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.54 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,287 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,288 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,313 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,330 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,359 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,373 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,431 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,444 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,472 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,530 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,563 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,624 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,657 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.50 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,697 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,957 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.55 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:19,995 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,096 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,185 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,342 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.50 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,356 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,371 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,495 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,529 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,639 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.72 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,646 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,742 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,782 - distributed.worker.memory - WARNING - Worker is at 43% memory usage. Resuming worker. Process memory: 3.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,887 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.49 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:20,937 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:21,015 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.63 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:21,135 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:21,209 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.74 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:19:25,540 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Parallel Iteration 6 with 1 Merging Blobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:20:01,204 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,013 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,051 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,088 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,163 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.34 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,238 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,312 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.42 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,337 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,342 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,549 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,720 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,835 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,942 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:02,988 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,061 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.42 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,292 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,307 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,387 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,554 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,650 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.77 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,777 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,811 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:03,929 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,036 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,047 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,108 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,121 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,191 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,346 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.44 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,376 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,504 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,629 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,760 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.43 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,848 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,911 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,967 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.83 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:04,969 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,011 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,082 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,124 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,126 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.45 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,155 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,158 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,166 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,168 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,199 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.43 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,249 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,344 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.43 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,355 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,581 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.80 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:05,827 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,187 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,706 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,744 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,773 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,835 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,857 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:06,929 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,017 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,033 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,115 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.79 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,178 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,256 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,262 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,277 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,337 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 6.68 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,343 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,345 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 4.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:07,749 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41663\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52220 remote=tcp://127.0.0.1:41663>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,750 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41663\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52204 remote=tcp://127.0.0.1:41663>: Stream is closed\n",
      "2025-02-13 12:20:07,750 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41663\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52216 remote=tcp://127.0.0.1:41663>: Stream is closed\n",
      "2025-02-13 12:20:07,750 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41663\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:52226 remote=tcp://127.0.0.1:41663>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,750 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41663\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:52238 remote=tcp://127.0.0.1:41663>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,794 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46041\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60228 remote=tcp://127.0.0.1:46041>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,795 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46041\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39846 remote=tcp://127.0.0.1:46041>: Stream is closed\n",
      "2025-02-13 12:20:07,795 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46041\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53058 remote=tcp://127.0.0.1:46041>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,795 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46041\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:34380 remote=tcp://127.0.0.1:46041>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:07,837 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36197\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52118 remote=tcp://127.0.0.1:36197>: Stream is closed\n",
      "2025-02-13 12:20:07,881 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42539\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52960 remote=tcp://127.0.0.1:42539>: Stream is closed\n",
      "2025-02-13 12:20:08,044 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36817\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:57704 remote=tcp://127.0.0.1:36817>: Stream is closed\n",
      "2025-02-13 12:20:08,043 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36817\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:57718 remote=tcp://127.0.0.1:36817>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:08,124 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh/storage/%28%27xarray-%3Cthis-array%3E-d5104d069f726701ad2c3a1360a4f065%27%2C%204%2C%200%29#45' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-d5104d069f726701ad2c3a1360a4f065%27%2C%204%2C%200%29#45'\n",
      "2025-02-13 12:20:08,124 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh/storage/%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%204%29#46' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%204%29#46'\n",
      "2025-02-13 12:20:08,124 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh/storage/%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%204%2C%200%29#35' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%204%2C%200%29#35'\n",
      "2025-02-13 12:20:08,125 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-02-13 12:20:08,125 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-uj2a9cxh'\n",
      "2025-02-13 12:20:08,150 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44557\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38078 remote=tcp://127.0.0.1:44557>: Stream is closed\n",
      "2025-02-13 12:20:08,302 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38275\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:46022 remote=tcp://127.0.0.1:38275>: Stream is closed\n",
      "2025-02-13 12:20:08,379 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39505\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:51076 remote=tcp://127.0.0.1:39505>: Stream is closed\n",
      "2025-02-13 12:20:08,383 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38483\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:41092 remote=tcp://127.0.0.1:38483>: Stream is closed\n",
      "2025-02-13 12:20:08,418 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:08,421 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40111\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44766 remote=tcp://127.0.0.1:40111>: Stream is closed\n",
      "2025-02-13 12:20:08,510 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46705\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45376 remote=tcp://127.0.0.1:46705>: Stream is closed\n",
      "2025-02-13 12:20:08,511 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46705\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45388 remote=tcp://127.0.0.1:46705>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:08,537 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27xarray-%3Cthis-array%3E-d5104d069f726701ad2c3a1360a4f065%27%2C%2014%2C%200%29#37' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-d5104d069f726701ad2c3a1360a4f065%27%2C%2014%2C%200%29#37'\n",
      "2025-02-13 12:20:08,537 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%2014%29#44' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%2014%29#44'\n",
      "2025-02-13 12:20:08,537 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27rechunk-merge-f80d88ff563745c1318c485ce3bd8f58%27%2C%2014%2C%200%29#43' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27rechunk-merge-f80d88ff563745c1318c485ce3bd8f58%27%2C%2014%2C%200%29#43'\n",
      "2025-02-13 12:20:08,538 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27open_dataset-cell_areas-5693b328a00b3fed6d23cc7e5d38b52e%27%2C%200%29#39' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27open_dataset-cell_areas-5693b328a00b3fed6d23cc7e5d38b52e%27%2C%200%29#39'\n",
      "2025-02-13 12:20:08,558 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2014%2C%200%29#34' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2014%2C%200%29#34'\n",
      "2025-02-13 12:20:08,558 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27transpose-c909079ddf50e0712d7a862da7d85569%27%2C%2014%2C%200%29#1' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27transpose-c909079ddf50e0712d7a862da7d85569%27%2C%2014%2C%200%29#1'\n",
      "2025-02-13 12:20:08,558 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27where-00380fdb0770e3f85c9378d0ea49bf6c%27%2C%2014%2C%200%29#3' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27where-00380fdb0770e3f85c9378d0ea49bf6c%27%2C%2014%2C%200%29#3'\n",
      "2025-02-13 12:20:08,558 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-32fi_vq0/storage/%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#40' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#40'\n",
      "2025-02-13 12:20:08,614 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45899\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:42820 remote=tcp://127.0.0.1:45899>: Stream is closed\n",
      "2025-02-13 12:20:08,615 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45899\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42840 remote=tcp://127.0.0.1:45899>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:08,615 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45899\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42844 remote=tcp://127.0.0.1:45899>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:08,615 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45899\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42830 remote=tcp://127.0.0.1:45899>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:08,889 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40577\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59772 remote=tcp://127.0.0.1:40577>: Stream is closed\n",
      "2025-02-13 12:20:08,899 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:08,905 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46041\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1554b3caad40>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:20:08,905 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40577\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59770 remote=tcp://127.0.0.1:40577>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:09,149 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 5.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:09,243 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40577\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516e9aa10>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:20:09,527 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:09,900 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36817\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155505aec190>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:20:09,992 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36817\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1554e7d9c1f0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:20:10,030 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40577\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15537f97bc70>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:20:10,338 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38771\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35788 remote=tcp://127.0.0.1:38771>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:20:10,339 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38771\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:35774 remote=tcp://127.0.0.1:38771>: Stream is closed\n",
      "2025-02-13 12:20:10,364 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idck9cte/storage/%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2026%2C%200%29#33' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2026%2C%200%29#33'\n",
      "2025-02-13 12:20:10,364 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idck9cte/storage/%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%2026%29#43' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-76b75037163d6253ec78f9e5e5156479%27%2C%2026%29#43'\n",
      "2025-02-13 12:20:10,364 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idck9cte/storage/%28%27astype-75b2298f05d583ec75a8ceae29975ee2%27%2C%200%29#37' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27astype-75b2298f05d583ec75a8ceae29975ee2%27%2C%200%29#37'\n",
      "2025-02-13 12:20:10,381 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idck9cte/storage/%28%27sub-3a4ac2730881955d1204d2fd1dde1c7f%27%2C%200%2C%200%29#36' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27sub-3a4ac2730881955d1204d2fd1dde1c7f%27%2C%200%2C%200%29#36'\n",
      "2025-02-13 12:20:10,417 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-_u_69nuv/storage/%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#39' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#39'\n",
      "2025-02-13 12:20:10,444 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-_u_69nuv/storage/%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2031%2C%200%29#33' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27%3Cthis-array%3E-apply_updates_block-7b8f5ae00f8c762bf81331316c09652d%27%2C%2031%2C%200%29#33'\n",
      "2025-02-13 12:20:10,444 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-_u_69nuv/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-02-13 12:20:10,445 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-_u_69nuv' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-_u_69nuv'\n",
      "2025-02-13 12:20:10,449 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:19,074 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:21,997 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:38,910 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42539\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516dac2e0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:42539 after 30 s\n",
      "2025-02-13 12:20:38,917 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38275\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1554b82bc070>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:38275 after 30 s\n",
      "2025-02-13 12:20:38,918 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42539\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15551582df30>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:42539 after 30 s\n",
      "2025-02-13 12:20:38,931 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39505\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516e9d840>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:39505 after 30 s\n",
      "2025-02-13 12:20:38,939 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44557\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155524157dc0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:44557 after 30 s\n",
      "2025-02-13 12:20:38,932 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46705\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155517001f90>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:46705 after 30 s\n",
      "2025-02-13 12:20:38,944 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44557\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516a66230>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:44557 after 30 s\n",
      "2025-02-13 12:20:38,937 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38275\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516c31d80>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:38275 after 30 s\n",
      "2025-02-13 12:20:38,950 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39505\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1555243bf730>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:39505 after 30 s\n",
      "2025-02-13 12:20:38,956 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42539\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1554b82e0e80>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:42539 after 30 s\n",
      "2025-02-13 12:20:38,952 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40111\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516ed93c0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:40111 after 30 s\n",
      "2025-02-13 12:20:38,920 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46705\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15552d517e20>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:46705 after 30 s\n",
      "2025-02-13 12:20:39,020 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38275\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15551300a3b0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:38275 after 30 s\n",
      "2025-02-13 12:20:39,608 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38275\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1554cf7e50f0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:38275 after 30 s\n",
      "2025-02-13 12:20:40,526 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44557\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/utils.py\", line 1915, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155516b1f250>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://127.0.0.1:44557 after 30 s\n",
      "2025-02-13 12:20:51,950 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:54,054 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:56,871 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:20:59,659 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:19,983 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.46 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:20,770 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:21,234 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41885\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43338 remote=tcp://127.0.0.1:41885>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:24:21,264 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41885\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43336 remote=tcp://127.0.0.1:41885>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:24:22,286 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:22,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:24,437 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34759\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:34876 remote=tcp://127.0.0.1:34759>: Stream is closed\n",
      "2025-02-13 12:24:24,438 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34759\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:34888 remote=tcp://127.0.0.1:34759>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:24:28,739 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.47 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:30,477 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 5.63 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:31,293 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:31,782 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:32,551 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40061\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:36540 remote=tcp://127.0.0.1:40061>: Stream is closed\n",
      "2025-02-13 12:24:33,514 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40061\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:24:37,667 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:38,536 - distributed.worker.memory - WARNING - Worker is at 91% memory usage. Pausing worker.  Process memory: 6.99 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:38,709 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:40,023 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35983\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59806 remote=tcp://127.0.0.1:35983>: Stream is closed\n",
      "2025-02-13 12:24:40,032 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35983\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:59794 remote=tcp://127.0.0.1:35983>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:24:40,216 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:24:42,722 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:25:14,818 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.42 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:25:15,166 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:25:54,297 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.34 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:25:57,615 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:01,092 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:11,441 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 6.64 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:26,126 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:26,499 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:30,400 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:43,697 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:50,482 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.92 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:26:52,160 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:27:47,839 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.81 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:14,193 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.95 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:14,568 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.94 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:14,883 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.85 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:29,000 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:29,899 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:29,974 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.44 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:31,231 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40677\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38582 remote=tcp://127.0.0.1:40677>: Stream is closed\n",
      "2025-02-13 12:28:31,232 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40677\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:46644 remote=tcp://127.0.0.1:40677>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:28:31,232 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40677\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:46656 remote=tcp://127.0.0.1:40677>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:28:31,372 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:32,598 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:33,656 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:28:34,197 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46301\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37502 remote=tcp://127.0.0.1:46301>: Stream is closed\n",
      "2025-02-13 12:28:34,333 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8/storage/%28%27xarray-%3Cthis-array%3E-7db36952126a2c2d3dd6b209d5e69aa0%27%2C%2014%2C%200%29#85' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27xarray-%3Cthis-array%3E-7db36952126a2c2d3dd6b209d5e69aa0%27%2C%2014%2C%200%29#85'\n",
      "2025-02-13 12:28:34,333 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8/storage/%28%27transpose-c909079ddf50e0712d7a862da7d85569%27%2C%2029%2C%200%29#59' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27transpose-c909079ddf50e0712d7a862da7d85569%27%2C%2029%2C%200%29#59'\n",
      "2025-02-13 12:28:34,333 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8/storage/%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#40' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27astype-821072a2f9568509c27023901a05822e%27%2C%200%29#40'\n",
      "2025-02-13 12:28:34,334 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8/storage/%28%27sub-3a4ac2730881955d1204d2fd1dde1c7f%27%2C%200%2C%200%29#81' (failed in <built-in function unlink>): [Errno 2] No such file or directory: '%28%27sub-3a4ac2730881955d1204d2fd1dde1c7f%27%2C%200%2C%200%29#81'\n",
      "2025-02-13 12:28:34,345 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-02-13 12:28:34,345 - distributed.diskutils - ERROR - Failed to remove '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/scratch/b/b382615/clients/tmphw35nsv2/dask-scratch-space/worker-idaayxw8'\n",
      "2025-02-13 12:29:20,501 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:31:39,361 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.92 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:31:58,647 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.73 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:32:22,911 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:36,722 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.47 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:38,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:39,951 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 6.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:40,774 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:41,058 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 5.91 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:41,317 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.83 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:41,543 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.60 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:42,654 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:42,705 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.72 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:42,723 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:42,822 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:43,401 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:43,839 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:33:45,969 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:36,450 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:37,899 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:38,293 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:39,199 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:39,723 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:41,843 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:44,906 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.50 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:45,123 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:49,193 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:49,362 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:49,629 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:49,797 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:50,700 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.61 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:50,839 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.84 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:51,827 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:53,036 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:53,872 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:54,722 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:54,872 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:56,233 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.72 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:59,758 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:34:59,810 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 5.96 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:00,479 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.64 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:00,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:09,415 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:10,726 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 5.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:11,985 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:12,514 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:15,267 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:15,824 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:16,632 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.46 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:16,839 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:17,880 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:35:45,154 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:01,016 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:01,208 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:01,311 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:01,408 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:01,642 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:02,248 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:11,117 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:12,513 - distributed.worker.memory - WARNING - Worker is at 51% memory usage. Resuming worker. Process memory: 3.90 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:12,987 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:19,345 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:20,715 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:21,969 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.95 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:25,404 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:25,877 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:26,335 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:31,039 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:31,074 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:31,139 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:31,684 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:31,702 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:32,167 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.74 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:32,684 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.66 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:41,780 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.38 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:41,869 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:43,609 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:53,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:36:58,638 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42841\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58140 remote=tcp://127.0.0.1:42841>: Stream is closed\n",
      "2025-02-13 12:37:13,395 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:37:13,928 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.36 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:37:48,065 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:37:58,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:00,064 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41015\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37706 remote=tcp://127.0.0.1:41015>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:38:32,429 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:32,530 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:44,238 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:44,329 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:53,077 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:53,325 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:38:55,847 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 4.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:02,430 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:10,430 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:11,427 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:54,707 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:56,555 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:58,969 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,119 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.43 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,241 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 5.88 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,250 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,396 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,869 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:39:59,959 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:00,244 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:00,326 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.38 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:00,453 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:00,477 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.77 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:01,122 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.65 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:01,538 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.73 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:02,068 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:04,667 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35137\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60436 remote=tcp://127.0.0.1:35137>: Stream is closed\n",
      "2025-02-13 12:40:04,668 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35137\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60454 remote=tcp://127.0.0.1:35137>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:40:04,670 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35137\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60438 remote=tcp://127.0.0.1:35137>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:40:04,670 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35137\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44782 remote=tcp://127.0.0.1:35137>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:40:05,014 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:16,071 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:16,149 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:16,247 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:16,458 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 5.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:21,961 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:23,874 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:23,976 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:24,103 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:24,139 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:25,679 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:28,425 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44849\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:40:28,832 - distributed.worker.memory - WARNING - Worker is at 54% memory usage. Resuming worker. Process memory: 4.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:38,757 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:41,634 - distributed.worker.memory - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 3.83 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:42,727 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:45,658 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:45,727 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:45,924 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:46,248 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:47,288 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:48,023 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:50,368 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:54,837 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:55,148 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:56,952 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 4.90 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:58,209 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:58,350 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.68 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:40:58,610 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:03,527 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:06,051 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.96 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:12,441 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.95 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:12,683 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.91 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:13,451 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.49 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:17,376 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.59 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:19,182 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:19,910 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:32,673 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.72 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:34,305 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:35,619 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:58,434 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:58,820 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:59,024 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:59,121 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:41:59,461 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 5.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:10,237 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:10,454 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:17,268 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:37,508 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:37,516 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:38,998 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:41,520 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:42,079 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.46 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:43,700 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:47,202 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:48,154 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:48,154 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:49,388 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.68 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:42:49,741 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 5.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:05,299 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:06,862 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:07,740 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:08,037 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:10,352 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:11,327 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:21,289 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:22,587 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:22,682 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.69 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:23,537 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:23,950 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.36 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:24,631 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.85 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:25,577 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 4.92 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:26,441 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:31,094 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:32,010 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:32,562 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:38,205 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:39,471 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 4.89 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:40,429 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:42,225 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:42,324 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:42,426 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:43:42,526 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:02,125 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:02,232 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:02,325 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:02,460 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:03,193 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.56 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:03,797 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:05,515 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:05,802 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:06,962 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 5.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:08,039 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:09,785 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:10,917 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:11,075 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:40005 -> tcp://127.0.0.1:40351\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 1797, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40005 remote=tcp://127.0.0.1:51896>: Stream is closed\n",
      "2025-02-13 12:44:11,075 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:11,158 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.97 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:11,553 - distributed.worker.memory - WARNING - Worker is at 52% memory usage. Resuming worker. Process memory: 3.98 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:12,307 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.95 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:13,663 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33451\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:51214 remote=tcp://127.0.0.1:33451>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:44:13,664 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33451\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:51224 remote=tcp://127.0.0.1:33451>: Stream is closed\n",
      "2025-02-13 12:44:16,477 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:16,609 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:17,683 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.88 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:19,255 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:21,096 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:22,588 - distributed.worker.memory - WARNING - Worker is at 52% memory usage. Resuming worker. Process memory: 4.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:48,224 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:53,213 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:44:58,330 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:08,138 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.77 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:10,772 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44037\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:51136 remote=tcp://127.0.0.1:44037>: Stream is closed\n",
      "2025-02-13 12:45:10,775 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44037\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60386 remote=tcp://127.0.0.1:44037>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:45:20,802 - distributed.worker.memory - WARNING - Worker is at 45% memory usage. Resuming worker. Process memory: 3.43 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:39,011 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:39,012 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:41,024 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.73 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:41,067 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:41,577 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:42,020 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 5.56 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:42,031 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:42,783 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:46,803 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39857\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:35344 remote=tcp://127.0.0.1:39857>: Stream is closed\n",
      "2025-02-13 12:45:46,804 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39857\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37558 remote=tcp://127.0.0.1:39857>: Stream is closed\n",
      "2025-02-13 12:45:46,806 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39857\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:58162 remote=tcp://127.0.0.1:39857>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:45:48,508 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:51,152 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 6.79 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:52,505 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.90 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:53,275 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:57,900 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.82 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:45:58,769 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:00,177 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:00,286 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:00,286 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:00,407 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:01,138 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.77 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:03,110 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:03,795 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:04,407 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:08,295 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:09,536 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 5.81 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:11,944 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:12,688 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:12,873 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:13,149 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:26,556 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:39,975 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:46:40,104 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 5.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:20,702 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:20,840 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:22,040 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:22,295 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:22,348 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:47:22,745 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 5.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:48:06,679 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:48:07,966 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.72 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:48:35,626 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:48:59,485 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.99 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:01,205 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.88 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:05,393 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:05,412 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:05,471 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:06,499 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:06,778 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:27,269 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:29,829 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:29,864 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:38,610 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.61 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:39,599 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:39,638 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 4.92 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:39,679 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:41,679 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:44,082 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:45,310 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.86 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:47,108 - distributed.worker.memory - WARNING - Worker is at 45% memory usage. Resuming worker. Process memory: 3.49 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:48,144 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:48,877 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.88 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:49,607 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:49,795 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:51,068 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.02 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:55,334 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:55,475 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.54 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:56,402 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:56,874 - distributed.worker.memory - WARNING - Worker is at 42% memory usage. Resuming worker. Process memory: 3.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:58,807 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.97 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:49:58,836 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.66 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:00,932 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:01,178 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:01,378 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:01,625 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:03,849 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.86 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:11,624 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 6.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:13,224 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:18,008 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:21,122 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 5.68 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:22,473 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:23,272 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:23,466 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:23,783 - distributed.worker.memory - WARNING - Worker is at 40% memory usage. Resuming worker. Process memory: 3.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:29,293 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:30,738 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:31,573 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:37,661 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:38,238 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.46 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:40,720 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:41,667 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:44,264 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35079\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:55540 remote=tcp://127.0.0.1:35079>: Stream is closed\n",
      "2025-02-13 12:50:44,265 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35079\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:32874 remote=tcp://127.0.0.1:35079>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:50:44,368 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44603\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:55544 remote=tcp://127.0.0.1:44603>: Stream is closed\n",
      "2025-02-13 12:50:50,009 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:50,717 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.78 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:53,582 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:57,074 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:50:58,733 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 4.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:08,098 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:11,752 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41213\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:40852 remote=tcp://127.0.0.1:41213>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:51:11,753 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41213\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:41022 remote=tcp://127.0.0.1:41213>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:51:40,739 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:42,438 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:43,161 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 4.89 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:51,754 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:52,372 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:51:58,929 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 6.55 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:01,681 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34407\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:55606 remote=tcp://127.0.0.1:34407>: Stream is closed\n",
      "2025-02-13 12:52:05,566 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:05,635 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.06 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:05,731 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:05,824 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:05,927 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:06,154 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 5.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/contextlib.py:142: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.24s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n",
      "2025-02-13 12:52:15,354 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 6.47 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:23,435 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 5.64 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:40,444 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:52:42,278 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 5.59 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:01,314 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:01,988 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:02,254 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.02 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:02,325 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:02,425 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:02,526 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:12,998 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:14,440 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 5.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:18,174 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 5.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:21,757 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:23,335 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36469\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:54720 remote=tcp://127.0.0.1:36469>: Stream is closed\n",
      "2025-02-13 12:54:25,104 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 4.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:29,978 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:34,825 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:54:35,314 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.31 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:00,772 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:03,105 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:03,809 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:03,952 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:05,277 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46079\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:60946 remote=tcp://127.0.0.1:46079>: Stream is closed\n",
      "2025-02-13 12:55:15,810 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.86 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:16,144 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.74 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:31,944 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.81 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:55:45,957 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.98 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:16,574 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.41 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:46,926 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:48,245 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:49,757 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:50,546 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:52,634 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.42 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:53,105 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.97 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:53,768 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:54,509 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:56,058 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.63 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:57,702 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.16 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:57,917 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:58,621 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:56:59,048 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39881\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44858 remote=tcp://127.0.0.1:39881>: Stream is closed\n",
      "2025-02-13 12:56:59,390 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39881\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43162 remote=tcp://127.0.0.1:39881>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:57:08,277 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.93 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:08,444 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:08,801 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:09,600 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.70 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:09,610 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.52 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:09,630 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:11,268 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.68 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:11,502 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.88 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:11,671 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:13,087 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.02 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:13,244 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 5.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:14,977 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.34 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:16,358 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:16,835 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:22,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.93 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:22,925 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:23,025 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:23,124 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:23,321 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 4.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:28,913 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:31,189 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40093\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52396 remote=tcp://127.0.0.1:40093>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:57:32,910 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40093\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:57:33,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40093\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1542, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-02-13 12:57:35,360 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:42,064 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:57:42,700 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:05,128 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:06,490 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 5.81 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:22,090 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:26,714 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.75 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:26,894 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:28,708 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:29,725 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:32,554 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 5.45 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:36,923 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:37,942 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:38,629 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.59 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:58:52,528 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:00,300 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:06,114 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:07,229 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.59 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:07,517 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.66 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:08,078 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:08,882 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:08,910 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:08,968 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:09,169 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:09,333 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.33 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:09,369 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 5.98 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:09,846 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:10,354 - distributed.worker.memory - WARNING - Worker is at 49% memory usage. Resuming worker. Process memory: 3.79 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:11,707 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:12,821 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.81 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:14,446 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:15,568 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.62 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:19,605 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.28 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:20,770 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:28,444 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.02 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:36,609 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:37,468 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.61 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:37,477 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.98 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:37,716 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:38,649 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 4.21 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:38,714 - distributed.worker.memory - WARNING - Worker is at 48% memory usage. Resuming worker. Process memory: 3.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:43,521 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:44,346 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:45,101 - distributed.worker.memory - WARNING - Worker is at 51% memory usage. Resuming worker. Process memory: 3.93 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:45,667 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:45,787 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:48,378 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:49,383 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:49,556 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:50,587 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.50 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:50,690 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.37 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:50,853 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:52,093 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45653\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53928 remote=tcp://127.0.0.1:45653>: Stream is closed\n",
      "2025-02-13 12:59:52,093 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45653\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:46130 remote=tcp://127.0.0.1:45653>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 12:59:55,279 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:56,273 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 5.73 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 12:59:59,602 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:02,490 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.35 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:03,790 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.98 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:05,003 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.13 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:05,102 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:05,203 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:05,336 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.36 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:05,402 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.27 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:06,636 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38633\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47986 remote=tcp://127.0.0.1:38633>: Stream is closed\n",
      "2025-02-13 13:00:07,797 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:07,801 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:07,888 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.25 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:09,341 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 5.26 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:09,364 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42397\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:35084 remote=tcp://127.0.0.1:42397>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 13:00:09,777 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37239\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:52750 remote=tcp://127.0.0.1:37239>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 13:00:11,489 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37239\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:35150 remote=tcp://127.0.0.1:37239>: Stream is closed\n",
      "2025-02-13 13:00:13,642 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('process_chunk-5d53519d6c79f33fbd7c89a0c1c6136d', 19) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42397. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SpOt & Track the Blobs & Merger Events\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tracker \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mSpotter(ds\u001b[38;5;241m.\u001b[39mextreme_events, ds\u001b[38;5;241m.\u001b[39mmask, R_fill\u001b[38;5;241m=\u001b[39mhole_filling_radius, T_fill \u001b[38;5;241m=\u001b[39m time_gap_fill, area_filter_quartile\u001b[38;5;241m=\u001b[39mdrop_area_quartile, \n\u001b[1;32m      4\u001b[0m                        allow_merging\u001b[38;5;241m=\u001b[39mallow_merging, overlap_threshold\u001b[38;5;241m=\u001b[39moverlap_threshold, nn_partitioning\u001b[38;5;241m=\u001b[39mnn_partitioning, \n\u001b[1;32m      5\u001b[0m                        xdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mncells\u001b[39m\u001b[38;5;124m'\u001b[39m,                 \u001b[38;5;66;03m# Need to tell spot_the_blOb the new Unstructured dimension\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                        cell_areas\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mcell_areas,      \u001b[38;5;66;03m# Cell areas for each Unstructured Grid Cell\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                        verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)                   \u001b[38;5;66;03m# Choose Verbosity Level (0=None, 1=Basic, 2=Advanced/Timing)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m blobs \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_merges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m blobs\n",
      "File \u001b[0;32m~/opt/spot_the_blOb/spot_the_blOb/spot_the_blOb.py:227\u001b[0m, in \u001b[0;36mSpotter.run\u001b[0;34m(self, return_merges)\u001b[0m\n\u001b[1;32m    223\u001b[0m processed_area \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_area(data_bin_filtered)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_merging \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_grid:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Track Blobs _with_ Merging & Splitting\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     blObs_ds, merges_ds, N_blobs_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_blObs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bin_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Track Blobs without any special Merging or Splitting\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     blObs_ds, N_blobs_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midentify_blobs(data_bin_filtered, time_connectivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/spot_the_blOb/spot_the_blOb/spot_the_blOb.py:2178\u001b[0m, in \u001b[0;36mSpotter.track_blObs\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;66;03m# Apply Splitting & Merging Logic to `overlap_blobs`\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;66;03m#   N.B. This is the longest step due to loop-wise dependencies... \u001b[39;00m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;66;03m#          In v2.0 unstructured, this loop has been painstakingly parallelised\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m split_and_merge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_and_merge_blobs_parallel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_grid \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_and_merge_blobs\n\u001b[0;32m-> 2178\u001b[0m blob_id_field, blob_props, blobs_list, merge_events \u001b[38;5;241m=\u001b[39m \u001b[43msplit_and_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_id_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblob_props\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Splitting and Merging Blobs.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;66;03m# Persist Together (This helps avoid block-wise task fusion run_spec issues with dask)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/spot_the_blOb/spot_the_blOb/spot_the_blOb.py:1996\u001b[0m, in \u001b[0;36mSpotter.split_and_merge_blobs_parallel\u001b[0;34m(self, blob_id_field_unique, blob_props)\u001b[0m\n\u001b[1;32m   1987\u001b[0m (merge_child_ids, merge_parent_ids, merge_areas, merge_counts,\n\u001b[1;32m   1988\u001b[0m     has_merge, updates_array, updates_ids, final_merging_blobs) \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m   1990\u001b[0m merge_child_ids, merge_parent_ids, merge_areas, merge_counts, \\\n\u001b[1;32m   1991\u001b[0m     has_merge, updates_array, updates_ids, final_merging_blobs \u001b[38;5;241m=\u001b[39m persist(\n\u001b[1;32m   1992\u001b[0m         merge_child_ids, merge_parent_ids, merge_areas, merge_counts,\n\u001b[1;32m   1993\u001b[0m         has_merge, updates_array, updates_ids, final_merging_blobs\n\u001b[1;32m   1994\u001b[0m     )\n\u001b[0;32m-> 1996\u001b[0m has_merge \u001b[38;5;241m=\u001b[39m \u001b[43mhas_merge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1997\u001b[0m time_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(has_merge)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;66;03m# Clean up temporary arrays\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:1207\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataarray.py:1175\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1175\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/core/dataset.py:899\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 899\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/xarray/namedarray/daskmanager.py:85\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:2427\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     exception \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m   2426\u001b[0m     traceback \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtraceback\n\u001b[0;32m-> 2427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2429\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[0;31mKilledWorker\u001b[0m: Attempted to run task ('process_chunk-5d53519d6c79f33fbd7c89a0c1c6136d', 19) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42397. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 13:00:22,216 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:22,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:23,391 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.84 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:25,630 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:26,124 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.49 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:32,018 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 6.29 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:32,536 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 6.48 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:33,729 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43335\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45730 remote=tcp://127.0.0.1:43335>: Stream is closed\n",
      "2025-02-13 13:00:44,179 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:44,179 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:44,258 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.07 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:44,358 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:45,143 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:00:53,624 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 5.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:08,737 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.47 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:33,591 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.65 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:41,153 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.45 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:50,378 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:58,252 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 7.03 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:01:59,041 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37677\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:35566 remote=tcp://127.0.0.1:37677>: Stream is closed\n",
      "2025-02-13 13:01:59,041 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37677\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1539, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1429, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:43364 remote=tcp://127.0.0.1:37677>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-02-13 13:02:11,413 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.40 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:18,389 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:24,763 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:25,951 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.86 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:31,929 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.09 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:32,034 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.04 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:32,129 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:33,660 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.96 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:52,885 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:02:56,366 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:03:34,189 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:03:35,594 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.79 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:03:36,485 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40367\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2075, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/core.py\", line 1018, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:42100 remote=tcp://127.0.0.1:40367>: Stream is closed\n",
      "2025-02-13 13:03:41,715 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:03:46,230 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:02,477 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:02,572 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.05 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:02,673 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:03,863 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:06,931 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:06,931 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:08,119 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.51 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:09,968 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:22,181 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.38 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:24,542 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:25,076 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 4.77 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:25,892 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 5.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:32,213 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 6.39 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:33,464 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:39,772 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.24 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:05:43,917 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.32 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:31,367 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.18 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:35,239 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:35,242 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:36,479 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.96 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:40,166 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:41,181 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 4.71 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:41,593 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:41,632 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.08 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:41,731 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:42,582 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.50 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:42,709 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.12 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:06:43,061 - distributed.worker.memory - WARNING - Worker is at 46% memory usage. Resuming worker. Process memory: 3.54 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:45,927 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.15 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:45,959 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.22 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:46,295 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:46,840 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.53 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:50,378 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.99 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:50,683 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.10 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:51,187 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 4.30 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:51,910 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 6.36 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:08:52,888 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 4.86 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:09:12,664 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.17 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:09:13,825 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 4.55 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:14,293 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.14 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:15,205 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 4.34 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:18,100 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.20 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:19,276 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 4.57 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:19,429 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.11 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:19,469 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.19 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:19,550 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.23 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:20,636 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 4.95 GiB -- Worker memory limit: 7.61 GiB\n",
      "2025-02-13 13:10:20,657 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 5.01 GiB -- Worker memory limit: 7.61 GiB\n"
     ]
    }
   ],
   "source": [
    "# SpOt & Track the Blobs & Merger Events\n",
    "\n",
    "tracker = blob.Spotter(ds.extreme_events, ds.mask, R_fill=hole_filling_radius, T_fill = time_gap_fill, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning, \n",
    "                       xdim='ncells',                 # Need to tell spot_the_blOb the new Unstructured dimension\n",
    "                       unstructured_grid=True,        # Use Unstructured Grid\n",
    "                       neighbours=ds.neighbours,      # Connectivity array for the Unstructured Grid Cells\n",
    "                       cell_areas=ds.cell_areas,      # Cell areas for each Unstructured Grid Cell\n",
    "                       verbosity=1)                   # Choose Verbosity Level (0=None, 1=Basic, 2=Advanced/Timing)\n",
    "\n",
    "blobs = tracker.run(return_merges=False)\n",
    "\n",
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df39de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = blobs.compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs to `zarr` for more efficient parallel I/O\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked_unstruct.zarr'\n",
    "blobs.to_zarr(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
