{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "3. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "4. Connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "5. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "6. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 0.25Â° resolution on 32 cores takes \n",
    "- Standard (i.e. Scannell et al., which involves no merge/split criteria or tracking):  ~2 minutes\n",
    "- Full Split/Merge Thresholding & Merge Tracking:  ~1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 15.74 GB\n",
      "Hostname is  l40095\n",
      "Forward Port = l40095:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary.zarr'\n",
    "chunk_size = {'time': 25, 'lat': -1, 'lon': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93364dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Binary Features and Modify Mask\n",
    "\n",
    "extreme_bin = ds.extreme_events#.isel(time=slice(0, 2000))\n",
    "mask = ds.mask.where((ds.lat<85) & (ds.lat>-90), other=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.5\n",
    "filling_radius = 8\n",
    "allow_merging = True     # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5  # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True   # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filling holes.\n",
      "Finished filtering small blobs.\n",
      "Finished blob identification.\n",
      "Finished calculating blob properties.\n",
      "Finished finding overlapping blobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/dask/array/core.py:4918: PerformanceWarning: Increasing number of chunks by factor of 15\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing splitting and merging in chunk 0 of 556\n",
      "Processing splitting and merging in chunk 25 of 556\n",
      "Missing newly created child_ids {151981} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 50 of 556\n",
      "Missing newly created child_ids {153469} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 75 of 556\n",
      "Missing newly created child_ids {154794} because parents have split/morphed in the meantime...\n"
     ]
    }
   ],
   "source": [
    "# Spot the Blobs\n",
    "\n",
    "tracker = blob.Spotter(extreme_bin, mask, R_fill=filling_radius, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning)\n",
    "blobs = tracker.run(return_merges=False)\n",
    "\n",
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked.nc'\n",
    "blobs.to_netcdf(file_name, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12d13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes()\n",
    "print('Finished filling holes.')\n",
    "\n",
    "# Remove Small Objects\n",
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_filled)\n",
    "print('Finished filtering small blobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin = data_bin_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field, _ = tracker.identify_blobs(data_bin, time_connectivity=False)\n",
    "print('Finished blob identification.')\n",
    "\n",
    "# Calculate Properties of each Blob\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n",
    "print('Finished calculating blob properties.')\n",
    "\n",
    "# Compile List of Overlapping Blob ID Pairs Across Time\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field)  # List of overlapping blob pairs\n",
    "print('Finished finding overlapping blobs.')\n",
    "\n",
    "\n",
    "# Apply Splitting & Merging Logic to `overlap_blobs`\n",
    "#   N.B. This is the longest step due to loop-wise dependencies... but many sub-steps are highly threaded so we're okay-ish in the end\n",
    "split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list, merge_events = tracker.split_and_merge_blobs(blob_id_field, blob_props, overlap_blobs_list)\n",
    "print('Finished splitting and merging blobs.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd101227",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = split_merged_blob_id_field_unique\n",
    "blobs_props = merged_blobs_props\n",
    "overlap_blobs_list = split_merged_blobs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing as binary_closing_dask\n",
    "from dask_image.ndmorph import binary_opening as binary_opening_dask\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, prange\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = np.unique(overlap_blobs_list) # 1D sorted unique\n",
    "        \n",
    "# Create a mapping from ID to indices\n",
    "ID_to_index = {ID: index for index, ID in enumerate(IDs)}\n",
    "\n",
    "# Convert overlap pairs to indices\n",
    "overlap_pairs_indices = np.array([(ID_to_index[pair[0]], ID_to_index[pair[1]]) for pair in overlap_blobs_list])\n",
    "\n",
    "# Create a sparse matrix representation of the graph\n",
    "n = len(IDs)\n",
    "row_indices, col_indices = overlap_pairs_indices.T\n",
    "data = np.ones(len(overlap_pairs_indices))\n",
    "graph = csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "\n",
    "# Solve the graph to determine connected components\n",
    "num_components, component_IDs = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "\n",
    "# Group IDs by their component index\n",
    "ID_clusters = [[] for _ in range(num_components)]\n",
    "for ID, component_ID in zip(IDs, component_IDs):\n",
    "    ID_clusters[component_ID].append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int32 = np.iinfo(np.int32).min\n",
    "max_old_ID = blob_id_field_unique.max().compute().data\n",
    "ID_to_cluster_index_array = np.full(max_old_ID + 1, min_int32, dtype=np.int32)\n",
    "\n",
    "# Fill the lookup array with cluster indices\n",
    "for index, cluster in enumerate(ID_clusters):\n",
    "    for ID in cluster:\n",
    "        ID_to_cluster_index_array[ID] = np.int32(index+1) # Because these are the connected IDs, there are many fewer!\n",
    "                                                            #  Add 1 so that ID = 0 is still invalid/no object\n",
    "\n",
    "# N.B.: **Need to pass da into apply_ufunc, otherwise it doesn't manage the memory correctly with large shared-mem numpy arrays**\n",
    "ID_to_cluster_index_da = xr.DataArray(ID_to_cluster_index_array, dims='ID', coords={'ID': np.arange(max_old_ID + 1)})\n",
    "\n",
    "def map_IDs_to_indices(block, ID_to_cluster_index_array):\n",
    "    mask = block > 0\n",
    "    new_block = np.zeros_like(block, dtype=np.int32)\n",
    "    new_block[mask] = ID_to_cluster_index_array[block[mask]]\n",
    "    return new_block\n",
    "\n",
    "split_merged_relabeled_blob_id_field = xr.apply_ufunc(\n",
    "    map_IDs_to_indices,\n",
    "    blob_id_field_unique, \n",
    "    ID_to_cluster_index_da,\n",
    "    input_core_dims=[[tracker.ydim, tracker.xdim],['ID']],\n",
    "    output_core_dims=[[tracker.ydim, tracker.xdim]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[np.int32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24996b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_merged_relabeled_blob_id_field = split_merged_relabeled_blob_id_field.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee039ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_ID = num_components + 1  # New IDs range from 0 to max_new_ID...\n",
    "new_ids = np.arange(1, max_new_ID+1, dtype=np.int32)\n",
    "\n",
    "# New blobs_props DataSet Structure\n",
    "blobs_props_extended = xr.Dataset(coords={\n",
    "    'ID': new_ids,\n",
    "    tracker.timedim: blob_id_field_unique[tracker.timedim]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960731ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_new_ids = (split_merged_relabeled_blob_id_field > 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ids_field = blob_id_field_unique.where(valid_new_ids).stack(z=(tracker.ydim, tracker.xdim), create_index=False)\n",
    "new_ids_field = split_merged_relabeled_blob_id_field.where(valid_new_ids).stack(z=(tracker.ydim, tracker.xdim), create_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionary for new IDs\n",
    "new_id_to_idx = {id_val: idx for idx, id_val in enumerate(new_ids)}\n",
    "\n",
    "def process_timestep(orig_ids, new_ids_t):\n",
    "    \"\"\"Process a single timestep to create ID mapping.\"\"\"\n",
    "    result = np.zeros(len(new_id_to_idx), dtype=np.int32)\n",
    "    \n",
    "    valid_mask = new_ids_t > 0\n",
    "    \n",
    "    # Get valid points for this timestep\n",
    "    if not valid_mask.any():\n",
    "        return result\n",
    "        \n",
    "    # Extract valid IDs\n",
    "    orig_valid = orig_ids[valid_mask]\n",
    "    new_valid = new_ids_t[valid_mask]\n",
    "    \n",
    "    if len(orig_valid) == 0:\n",
    "        return result\n",
    "        \n",
    "    # Find unique pairs efficiently\n",
    "    unique_pairs = np.unique(np.column_stack((orig_valid, new_valid)), axis=0)\n",
    "    \n",
    "    # Create mapping\n",
    "    for orig_id, new_id in unique_pairs:\n",
    "        if new_id in new_id_to_idx:\n",
    "            result[new_id_to_idx[new_id]] = orig_id\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "# Use apply_ufunc to parallelize the computation\n",
    "result = xr.apply_ufunc(\n",
    "    process_timestep,\n",
    "    original_ids_field,\n",
    "    new_ids_field,\n",
    "    input_core_dims=[['z'], ['z']],\n",
    "    output_core_dims=[['ID']],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[np.int32],\n",
    "    dask_gufunc_kwargs={'output_sizes': {'ID': len(new_ids)}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6277706",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_mapping = xr.apply_ufunc(\n",
    "                    process_timestep,\n",
    "                    original_ids_field,\n",
    "                    new_ids_field,\n",
    "                    input_core_dims=[['z'], ['z']],\n",
    "                    output_core_dims=[['ID']],\n",
    "                    vectorize=True,\n",
    "                    dask='parallelized',\n",
    "                    output_dtypes=[np.int32],\n",
    "                    dask_gufunc_kwargs={'output_sizes': {'ID': len(new_ids)}}\n",
    "            ).assign_coords(ID=new_ids).compute()\n",
    "        \n",
    "\n",
    "blobs_props_extended['global_ID'] = global_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "result = xr.where(new_ids_field.isel(z=first_match_idx) == new_ids, \n",
    "                    original_ids_field.isel(z=first_match_idx), 0)\n",
    "\n",
    "global_id_mapping = (result\n",
    "    .assign_coords(new_id=new_ids)\n",
    "    .rename({'new_id': 'ID'})\n",
    "    .astype(np.int32)\n",
    "    .compute())\n",
    "\n",
    "blobs_props_extended['global_ID'] = global_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = blobs_props.isel(ID=0) * np.nan\n",
    "blobs_props = xr.concat([dummy.assign_coords(ID=0), blobs_props], dim='ID')\n",
    "\n",
    "\n",
    "for var_name in blobs_props.data_vars:\n",
    "    \n",
    "    temp = (blobs_props[var_name]\n",
    "                        .sel(ID=global_id_mapping.rename({'ID':'new_id'}))\n",
    "                        .drop_vars('ID').rename({'new_id':'ID'}))\n",
    "    \n",
    "    if var_name == 'ID':\n",
    "        temp = temp.astype(np.int32)\n",
    "    else:\n",
    "        temp = temp.astype(np.float32)\n",
    "        \n",
    "    blobs_props_extended[var_name] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_parent_IDs = xr.where(merge_events.parent_IDs>0, merge_events.parent_IDs, 0)\n",
    "new_IDs_parents = ID_to_cluster_index_da.sel(ID=old_parent_IDs)\n",
    "\n",
    "# Replace the coordinate merge_ID in new_IDs_parents with merge_time.  merge_events.merge_time gives merge_time for each merge_ID\n",
    "new_IDs_parents_t = new_IDs_parents.assign_coords({'merge_time': merge_events.merge_time}).drop_vars('ID').swap_dims({'merge_ID': 'merge_time'})  # this now has coordinate merge_time and ID\n",
    "\n",
    "# Map new_IDs_parents_t into a new data array with dimensions time, ID, and sibling_ID\n",
    "merge_ledger = xr.full_like(global_id_mapping, fill_value=-1).expand_dims({'sibling_ID': new_IDs_parents_t.parent_idx.shape[0]}).copy() # dimesions are time, ID, sibling_ID\n",
    "\n",
    "for time_val in new_IDs_parents_t.merge_time.values:\n",
    "    IDs = new_IDs_parents_t.sel({'merge_time': time_val})\n",
    "    if IDs.ndim == 1:\n",
    "        IDs = IDs.values\n",
    "        for ID in IDs:\n",
    "            if ID > 0:\n",
    "                merge_ledger.loc[{tracker.timedim: time_val, 'ID': ID}] = IDs\n",
    "    else:  # There were multiple mergers at this time...\n",
    "        for merge_num, _ in enumerate(IDs.merge_time):\n",
    "            IDs_sub = IDs.isel(merge_time=merge_num).values\n",
    "            for ID in IDs_sub:\n",
    "                if ID > 0:\n",
    "                    merge_ledger.loc[{tracker.timedim: time_val, 'ID': ID}] = IDs_sub\n",
    "\n",
    "merge_ledger = merge_ledger.rename('merge_ledger').transpose(tracker.timedim, 'ID', 'sibling_ID').chunk({tracker.timedim: split_merged_relabeled_blob_id_field.data.chunksize[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31347c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c42b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
