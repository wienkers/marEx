{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Filter out small objects -- area less than the `area_filter_quartile` of the distribution of objects.\n",
    "3. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "4. Manually connect objects across time, applying Sun et al. 2023 criteria:\n",
    "    - Connected Blobs must overlap by at least `overlap_threshold=50%` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but split the blob based on parent centroid locality.\n",
    "5. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "\n",
    "N.B.: Exploits parallelised `Dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of Daily outputs at 0.25Â° resolution takes ~6 minutes on 128 total cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 15.74 GB\n",
      "Hostname is  l40128\n",
      "Forward Port = l40128:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary.zarr'\n",
    "chunk_size = {'time': 25, 'lat': -1, 'lon': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93364dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Binary Features and Modify Mask\n",
    "\n",
    "extreme_bin = ds.extreme_events\n",
    "mask = ds.mask.where((ds.lat<85) & (ds.lat>-90), other=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.5\n",
    "filling_radius = 8\n",
    "allow_merging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48aeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot the Blobs\n",
    "\n",
    "tracker = blob.Spotter(extreme_bin, mask, R_fill=filling_radius, area_filter_quartile=drop_area_quartile, allow_merging=allow_merging)\n",
    "#blobs = tracker.run()\n",
    "\n",
    "#blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02db4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 13.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin = data_bin_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field, _ = tracker.identify_blobs(data_bin, time_connectivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Properties of each Blob\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile List of Overlapping Blob ID Pairs Across Time\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field)  # List of overlapping blob pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list, merged_blobs_ledger = tracker.split_and_merge_blobs(blob_id_field, blob_props, overlap_blobs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ac411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = blob_id_field.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised computation of overlap fractions\n",
    "areas_0 = blob_props['area'].sel(ID=overlap_blobs_list[:, 0]).values\n",
    "areas_1 = blob_props['area'].sel(ID=overlap_blobs_list[:, 1]).values\n",
    "min_areas = np.minimum(areas_0, areas_1)\n",
    "overlap_fractions = overlap_blobs_list[:, 2].astype(float) / min_areas\n",
    "\n",
    "# Filter out the overlaps that are too small\n",
    "overlap_blobs_list = overlap_blobs_list[overlap_fractions >= tracker.overlap_threshold]\n",
    "\n",
    "\n",
    "\n",
    "##### Consider Merging Blobs\n",
    "\n",
    "# Initialise merge tracking structures\n",
    "merge_ledger = []                      # List of IDs of the 2 Merging Parents\n",
    "next_new_id = blob_props.ID.max().item() + 1  # Start new IDs after highest existing ID\n",
    "\n",
    "# Find all the Children (t+1 / RHS) elements that appear multiple times --> Indicates there are 2+ Parent Blobs...\n",
    "unique_children, children_counts = np.unique(overlap_blobs_list[:, 1], return_counts=True)\n",
    "merging_blobs = unique_children[children_counts > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-compute the child_time_idx for each child_blob\n",
    "\n",
    "search_ids = xr.DataArray(\n",
    "        merging_blobs,\n",
    "        dims=['search_id'],\n",
    "        coords={'search_id': merging_blobs}\n",
    "    )\n",
    "\n",
    "# Reduce boolean array in spatial dimensions for all IDs at once\n",
    "mask_4d = blob_id_field_unique == search_ids\n",
    "presence_by_time = mask_4d.any(dim=[tracker.ydim, tracker.xdim])\n",
    "\n",
    "# Find time index\n",
    "time_indices = presence_by_time.argmax(dim=tracker.timedim).compute()\n",
    "\n",
    "# Convert to dictionary for fast lookup\n",
    "time_index_map = {\n",
    "    int(id_val): int(idx.values) \n",
    "    for id_val, idx in time_indices.items()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#... then can persist the present time slice, and reuse it...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Add at the top with other imports\n",
    "\n",
    "for child_id in merging_blobs:\n",
    "    start_total = time.perf_counter()\n",
    "    print(f'Merging Child ID: {child_id}')\n",
    "    \n",
    "    # Find all pairs involving this Child Blob\n",
    "    t0 = time.perf_counter()\n",
    "    child_mask = overlap_blobs_list[:, 1] == child_id\n",
    "    child_where = np.where(overlap_blobs_list[:, 1] == child_id)[0]  # Needed for assignment\n",
    "    merge_group = overlap_blobs_list[child_mask]\n",
    "    parent_ids = merge_group[:, 0]\n",
    "    num_parents = len(parent_ids)\n",
    "    print(f'Parent IDs: {parent_ids}')\n",
    "    print(f'Time for finding pairs: {time.perf_counter() - t0:.4f}s')\n",
    "    \n",
    "    # Make a new ID for the other Half of the Child Blob & Record in the Merge Ledger\n",
    "    t0 = time.perf_counter()\n",
    "    new_blob_id = np.arange(next_new_id, next_new_id + (num_parents - 1), dtype=np.int32)\n",
    "    next_new_id += num_parents - 1\n",
    "    merge_ledger.append(parent_ids)\n",
    "    overlap_blobs_list[child_where[1:], 1] = new_blob_id\n",
    "    child_ids = np.concatenate((np.array([child_id]), new_blob_id))\n",
    "    print(f'Time for ID assignment: {time.perf_counter() - t0:.4f}s')\n",
    "    \n",
    "    # Detailed timing for distance calculations and relabeling\n",
    "    print(\"\\nDetailed timing for distance calculations:\")\n",
    "    \n",
    "    t_start = time.perf_counter()\n",
    "    parent_centroids = blob_props.sel(ID=parent_ids).centroid.isel(component=[1,0]).values.T\n",
    "    t1 = time.perf_counter()\n",
    "    print(f'  Getting parent centroids: {t1 - t_start:.4f}s')\n",
    "    \n",
    "    child_time_idx = (blob_id_field_unique == child_id).any(dim=[tracker.ydim, tracker.xdim]).argmax().compute().item()\n",
    "    t2 = time.perf_counter()\n",
    "    print(f'  Finding child time index (with compute): {t2 - t1:.4f}s')\n",
    "    \n",
    "    child_mask_2d = blob_id_field_unique.isel({tracker.timedim: child_time_idx}) == child_id\n",
    "    t3 = time.perf_counter()\n",
    "    print(f'  Creating 2D child mask: {t3 - t2:.4f}s')\n",
    "    \n",
    "    child_coords = np.stack(np.where(child_mask_2d), axis=1)\n",
    "    t4 = time.perf_counter()\n",
    "    print(f'  Getting child coordinates: {t4 - t3:.4f}s')\n",
    "    \n",
    "    # Break down the distance calculation\n",
    "    print(f'  Child coords shape: {child_coords.shape}, Parent centroids shape: {parent_centroids.shape}')\n",
    "    expanded_coords = child_coords[:, None]  # Broadcasting preparation\n",
    "    t5 = time.perf_counter()\n",
    "    print(f'  Broadcasting preparation: {t5 - t4:.4f}s')\n",
    "    \n",
    "    coord_diff = expanded_coords - parent_centroids\n",
    "    t6 = time.perf_counter()\n",
    "    print(f'  Coordinate differencing: {t6 - t5:.4f}s')\n",
    "    \n",
    "    distances = np.linalg.norm(coord_diff, axis=2)\n",
    "    t7 = time.perf_counter()\n",
    "    print(f'  Computing distances: {t7 - t6:.4f}s')\n",
    "    \n",
    "    new_labels = child_ids[np.argmin(distances, axis=1)]\n",
    "    t8 = time.perf_counter()\n",
    "    print(f'  Assigning new labels: {t8 - t7:.4f}s')\n",
    "    print(f'Total time for distance calculations: {t8 - t_start:.4f}s\\n')\n",
    "    \n",
    "    # Update blob field values\n",
    "    t0 = time.perf_counter()\n",
    "    tslice_child = blob_id_field_unique.isel({tracker.timedim: child_time_idx})\n",
    "    temp = np.zeros_like(tslice_child)\n",
    "    temp[child_mask_2d] = new_labels\n",
    "    blob_id_field_unique[{tracker.timedim: child_time_idx}] = tslice_child.where(~child_mask_2d, temp)\n",
    "    print(f'Time for field update: {time.perf_counter() - t0:.4f}s')\n",
    "    \n",
    "    # Update blob properties\n",
    "    t0 = time.perf_counter()\n",
    "    new_child_props = tracker.calculate_blob_properties(blob_id_field_unique.isel({tracker.timedim: child_time_idx}), properties=['area', 'centroid'])\n",
    "    blob_props.loc[dict(ID=child_id)] = new_child_props.sel(ID=child_id)\n",
    "    blob_props = xr.concat([blob_props, new_child_props.sel(ID=new_blob_id)], dim='ID')\n",
    "    print(f'Time for properties update: {time.perf_counter() - t0:.4f}s')\n",
    "    \n",
    "    # Re-assess overlaps\n",
    "    t0 = time.perf_counter()\n",
    "    new_overlaps = tracker.check_overlap_slice(blob_id_field_unique.isel({tracker.timedim: child_time_idx}).values, \n",
    "                                             blob_id_field_unique.isel({tracker.timedim: child_time_idx+1}).values)\n",
    "    new_child_overlaps_list = new_overlaps[(new_overlaps[:, 0] == child_id) | np.isin(new_overlaps[:, 0], new_blob_id)]\n",
    "    \n",
    "    areas_0 = blob_props['area'].sel(ID=new_child_overlaps_list[:, 0]).values\n",
    "    areas_1 = blob_props['area'].sel(ID=new_child_overlaps_list[:, 1]).values\n",
    "    min_areas = np.minimum(areas_0, areas_1)\n",
    "    overlap_fractions = new_child_overlaps_list[:, 2].astype(float) / min_areas\n",
    "    new_child_overlaps_list = new_child_overlaps_list[overlap_fractions >= tracker.overlap_threshold]\n",
    "    \n",
    "    child_mask_LHS = overlap_blobs_list[:, 0] == child_id\n",
    "    overlap_blobs_list = np.concatenate([overlap_blobs_list[~child_mask_LHS], new_child_overlaps_list])\n",
    "    print(f'Time for overlap reassessment: {time.perf_counter() - t0:.4f}s')\n",
    "    \n",
    "    print(f'Total time for this child_id: {time.perf_counter() - start_total:.4f}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f57f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f855b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Blobs List to Determine Globally Unique IDs & Update Blob ID Field\n",
    "split_merged_blobs_ds = tracker.cluster_rename_blobs_and_props(split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Merge Ledger to split_merged_blobs_ds\n",
    "split_merged_blobs_ds.attrs['merge_ledger'] = merged_blobs_ledger\n",
    "\n",
    "# Count Number of Blobs (This may have increased due to splitting)\n",
    "N_blobs = split_merged_blobs_ds.ID_field.max().compute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5517778",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked.nc'\n",
    "blobs.to_netcdf(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
