{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Filter out small objects -- area less than the `area_filter_quartile` of the distribution of objects.\n",
    "3. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "4. Manually connect objects across time, applying Sun et al. 2023 criteria:\n",
    "    - Connected Blobs must overlap by at least `overlap_threshold=50%` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but split the blob based on parent centroid locality.\n",
    "5. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "\n",
    "N.B.: Exploits parallelised `Dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of Daily outputs at 0.25Â° resolution takes ~6 minutes on 128 total cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 7.86 GB\n",
      "Hostname is  l20134\n",
      "Forward Port = l20134:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary.zarr'\n",
    "chunk_size = {'time': 25, 'lat': -1, 'lon': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93364dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Binary Features and Modify Mask\n",
    "\n",
    "extreme_bin = ds.extreme_events.isel(time=slice(0, 1000))\n",
    "mask = ds.mask.where((ds.lat<85) & (ds.lat>-90), other=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.5\n",
    "filling_radius = 8\n",
    "allow_merging = True\n",
    "centroid_partitioning = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48aeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot the Blobs\n",
    "\n",
    "tracker = blob.Spotter(extreme_bin, mask, R_fill=filling_radius, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, centroid_partitioning=centroid_partitioning)\n",
    "# blobs = tracker.run()\n",
    "\n",
    "# blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9acaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes()\n",
    "\n",
    "# Remove Small Objects\n",
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd10daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing, binary_opening\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232f778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def wrapped_euclidian_parallel(mask_values, parent_centroids_values, Nx):\n",
    "    \"\"\"\n",
    "    Optimised function for computing wrapped Euclidean distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask_values : np.ndarray\n",
    "        2D boolean array where True indicates points to calculate distances for\n",
    "    parent_centroids_values : np.ndarray\n",
    "        Array of shape (n_parents, 2) containing (y, x) coordinates of parent centroids\n",
    "    Nx : int\n",
    "        Size of the x-dimension for wrapping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    distances : np.ndarray\n",
    "        Array of shape (n_true_points, n_parents) with minimum distances\n",
    "    \"\"\"\n",
    "    n_parents = len(parent_centroids_values)\n",
    "    half_Nx = Nx / 2\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(mask_values)\n",
    "    n_true = len(y_indices)\n",
    "    \n",
    "    distances = np.empty((n_true, n_parents), dtype=np.float64)\n",
    "    \n",
    "    # Precompute for faster access\n",
    "    parent_y = parent_centroids_values[:, 0]\n",
    "    parent_x = parent_centroids_values[:, 1]\n",
    "    \n",
    "    # Parallel loop over true positions\n",
    "    for idx in prange(n_true):\n",
    "        y, x = y_indices[idx], x_indices[idx]\n",
    "        \n",
    "        # Pre-compute y differences for all parents\n",
    "        dy = y - parent_y\n",
    "        \n",
    "        # Pre-compute x differences for all parents\n",
    "        dx = x - parent_x\n",
    "        \n",
    "        # Wrapping correction\n",
    "        dx = np.where(dx > half_Nx, dx - Nx, dx)\n",
    "        dx = np.where(dx < -half_Nx, dx + Nx, dx)\n",
    "        \n",
    "        distances[idx] = np.sqrt(dy * dy + dx * dx)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def create_grid_index_arrays(points_y, points_x, grid_size, ny, nx):\n",
    "    \"\"\"\n",
    "    Creates a grid-based spatial index using numpy arrays.\n",
    "    \"\"\"\n",
    "    n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "    n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "    max_points_per_cell = len(points_y)\n",
    "    \n",
    "    grid_points = np.full((n_grids_y, n_grids_x, max_points_per_cell), -1, dtype=np.int32)\n",
    "    grid_counts = np.zeros((n_grids_y, n_grids_x), dtype=np.int32)\n",
    "    \n",
    "    for idx in range(len(points_y)):\n",
    "        grid_y = min(points_y[idx] // grid_size, n_grids_y - 1)\n",
    "        grid_x = min(points_x[idx] // grid_size, n_grids_x - 1)\n",
    "        count = grid_counts[grid_y, grid_x]\n",
    "        if count < max_points_per_cell:\n",
    "            grid_points[grid_y, grid_x, count] = idx\n",
    "            grid_counts[grid_y, grid_x] += 1\n",
    "    \n",
    "    return grid_points, grid_counts\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def calculate_wrapped_distance(y1, x1, y2, x2, nx, half_nx):\n",
    "    \"\"\"\n",
    "    Calculate distance with periodic boundary conditions in x dimension.\n",
    "    \"\"\"\n",
    "    dy = y1 - y2\n",
    "    dx = x1 - x2\n",
    "    \n",
    "    if dx > half_nx:\n",
    "        dx -= nx\n",
    "    elif dx < -half_nx:\n",
    "        dx += nx\n",
    "        \n",
    "    return np.sqrt(dy * dy + dx * dx)\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def get_nearest_parent_labels(child_mask, parent_masks, child_ids, parent_centroids, Nx, max_distance=20):\n",
    "    \"\"\"\n",
    "    Assigns labels based on nearest parent blob points.\n",
    "    This is quite computationally-intensive, so we utilise many optimisations here...\n",
    "    \"\"\"\n",
    "    ny, nx = child_mask.shape\n",
    "    half_Nx = Nx / 2\n",
    "    n_parents = len(parent_masks)\n",
    "    grid_size = max(2, max_distance // 4)\n",
    "    \n",
    "    y_indices, x_indices = np.nonzero(child_mask)\n",
    "    n_child_points = len(y_indices)\n",
    "    \n",
    "    min_distances = np.full(n_child_points, np.inf)\n",
    "    parent_assignments = np.zeros(n_child_points, dtype=np.int32)\n",
    "    found_close = np.zeros(n_child_points, dtype=np.bool_)\n",
    "    \n",
    "    for parent_idx in range(n_parents):\n",
    "        py, px = np.nonzero(parent_masks[parent_idx])\n",
    "        \n",
    "        if len(py) == 0:  # Skip empty parents\n",
    "            continue\n",
    "            \n",
    "        # Create grid index for this parent\n",
    "        n_grids_y = (ny + grid_size - 1) // grid_size\n",
    "        n_grids_x = (nx + grid_size - 1) // grid_size\n",
    "        grid_points, grid_counts = create_grid_index_arrays(py, px, grid_size, ny, nx)\n",
    "        \n",
    "        # Process child points in parallel\n",
    "        for child_idx in prange(n_child_points):\n",
    "            if found_close[child_idx]:  # Skip if we already found a very close match\n",
    "                continue\n",
    "                \n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            grid_y = min(child_y // grid_size, n_grids_y - 1)\n",
    "            grid_x = min(child_x // grid_size, n_grids_x - 1)\n",
    "            \n",
    "            min_dist_to_parent = np.inf\n",
    "            \n",
    "            # Check nearby grid cells\n",
    "            for dy in range(-1, 2):\n",
    "                grid_y_check = (grid_y + dy) % n_grids_y\n",
    "                \n",
    "                for dx in range(-1, 2):\n",
    "                    grid_x_check = (grid_x + dx) % n_grids_x\n",
    "                    \n",
    "                    # Process points in this grid cell\n",
    "                    n_points = grid_counts[grid_y_check, grid_x_check]\n",
    "                    \n",
    "                    for p_idx in range(n_points):\n",
    "                        point_idx = grid_points[grid_y_check, grid_x_check, p_idx]\n",
    "                        if point_idx == -1:\n",
    "                            break\n",
    "                        \n",
    "                        dist = calculate_wrapped_distance(\n",
    "                            child_y, child_x,\n",
    "                            py[point_idx], px[point_idx],\n",
    "                            Nx, half_Nx\n",
    "                        )\n",
    "                        \n",
    "                        if dist > max_distance:\n",
    "                            continue\n",
    "                        \n",
    "                        if dist < min_dist_to_parent:\n",
    "                            min_dist_to_parent = dist\n",
    "                            \n",
    "                        if dist < 2:  # Very close match found\n",
    "                            min_dist_to_parent = dist\n",
    "                            found_close[child_idx] = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_close[child_idx]:\n",
    "                        break\n",
    "                \n",
    "                if found_close[child_idx]:\n",
    "                    break\n",
    "            \n",
    "            # Update assignment if this parent is closer\n",
    "            if min_dist_to_parent < min_distances[child_idx]:\n",
    "                min_distances[child_idx] = min_dist_to_parent\n",
    "                parent_assignments[child_idx] = parent_idx\n",
    "                \n",
    "                if min_dist_to_parent < 2:\n",
    "                    found_close[child_idx] = True\n",
    "    \n",
    "    # Handle any unassigned points using centroids\n",
    "    unassigned = min_distances == np.inf\n",
    "    if np.any(unassigned):\n",
    "        for child_idx in np.nonzero(unassigned)[0]:\n",
    "            child_y, child_x = y_indices[child_idx], x_indices[child_idx]\n",
    "            min_dist = np.inf\n",
    "            best_parent = 0\n",
    "            \n",
    "            for parent_idx in range(n_parents):\n",
    "                # Calculate distance to centroid with periodic boundary conditions\n",
    "                dist = calculate_wrapped_distance(\n",
    "                    child_y, child_x,\n",
    "                    parent_centroids[parent_idx, 0],\n",
    "                    parent_centroids[parent_idx, 1],\n",
    "                    Nx, half_Nx\n",
    "                )\n",
    "                \n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_parent = parent_idx\n",
    "                    \n",
    "            parent_assignments[child_idx] = best_parent\n",
    "    \n",
    "    # Convert from parent indices to child_ids\n",
    "    new_labels = child_ids[parent_assignments]\n",
    "    \n",
    "    return new_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401c6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin = data_bin_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f11cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field, _ = tracker.identify_blobs(data_bin, time_connectivity=False)\n",
    "        \n",
    "# Calculate Properties of each Blob\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n",
    "\n",
    "# Compile List of Overlapping Blob ID Pairs Across Time\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field)  # List of overlapping blob pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10131195",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = blob_id_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30efcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b382615/opt/spot_the_blOb/spot_the_blOb/spot_the_blOb.py:645: FutureWarning: ``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.\n",
      "  unique_ids_by_time = xr.apply_ufunc(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 99.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "areas_0 = blob_props['area'].sel(ID=overlap_blobs_list[:, 0]).values\n",
    "areas_1 = blob_props['area'].sel(ID=overlap_blobs_list[:, 1]).values\n",
    "min_areas = np.minimum(areas_0, areas_1)\n",
    "overlap_fractions = overlap_blobs_list[:, 2].astype(float) / min_areas\n",
    "\n",
    "## Filter out the overlaps that are too small\n",
    "overlap_blobs_list = overlap_blobs_list[overlap_fractions >= tracker.overlap_threshold]\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "##### Consider Merging Blobs ####\n",
    "#################################\n",
    "\n",
    "## Initialize merge tracking lists to build DataArray later\n",
    "merge_times = []      # When the merge occurred\n",
    "merge_child_ids = []  # Resulting child ID\n",
    "merge_parent_ids = [] # List of parent IDs that merged\n",
    "merge_areas = []      # Areas of overlap\n",
    "next_new_id = blob_props.ID.max().item() + 1  # Start new IDs after highest existing ID\n",
    "\n",
    "# Find all the Children (t+1 / RHS) elements that appear multiple times --> Indicates there are 2+ Parent Blobs...\n",
    "unique_children, children_counts = np.unique(overlap_blobs_list[:, 1], return_counts=True)\n",
    "merging_blobs = unique_children[children_counts > 1]\n",
    "\n",
    "# Pre-compute the child_time_idx & 2d_mask_id for each child_blob\n",
    "time_index_map = tracker.compute_id_time_dict(blob_id_field_unique, merging_blobs, next_new_id)\n",
    "Nx = blob_id_field_unique[tracker.xdim].size\n",
    "\n",
    "# Group blobs by time-chunk\n",
    "# -- Pre-condition: Blob IDs should be monotonically increasing in time...\n",
    "chunk_boundaries = np.cumsum([0] + list(blob_id_field_unique.chunks[0] ))\n",
    "blobs_by_chunk = {}\n",
    "for blob_id in merging_blobs:\n",
    "    # Find which chunk this time index belongs to\n",
    "    chunk_idx = np.searchsorted(chunk_boundaries, time_index_map[blob_id], side='right') - 1\n",
    "    blobs_by_chunk.setdefault(chunk_idx, []).append(blob_id)\n",
    "\n",
    "\n",
    "future_chunk_merges = []\n",
    "for chunk_idx, chunk_blobs in blobs_by_chunk.items(): # Loop over each time-chunk\n",
    "    # We do this to avoid repetetively re-computing and injecting tiny changes into the full dask-backed DataArray blob_id_field_unique\n",
    "    \n",
    "    ## Extract and Load an entire chunk into memory\n",
    "    \n",
    "    chunk_start = sum(blob_id_field_unique.chunks[0][:chunk_idx])\n",
    "    chunk_end = chunk_start + blob_id_field_unique.chunks[0][chunk_idx] + 1  #  We also want access to the blob_id_time_p1...  But need to remember to remove the last time later\n",
    "    \n",
    "    chunk_data = blob_id_field_unique.isel({tracker.timedim: slice(chunk_start, chunk_end)}).compute()\n",
    "    \n",
    "    # Create a working queue of blobs to process\n",
    "    blobs_to_process = chunk_blobs.copy()\n",
    "    # Combine only the future_chunk_merges that don't already appear in blobs_to_process\n",
    "    blobs_to_process = blobs_to_process + [blob_id for blob_id in future_chunk_merges if blob_id not in blobs_to_process]  # First, assess the new blobs from the end of the previous chunk...\n",
    "    future_chunk_merges = []\n",
    "    \n",
    "    #for child_id in chunk_blobs: # Process each blob in this chunk\n",
    "    while blobs_to_process:  # Process until queue is empty\n",
    "        child_id = blobs_to_process.pop(0)  # Get next blob to process\n",
    "        \n",
    "        child_time_idx = time_index_map[child_id]\n",
    "        relative_time_idx = child_time_idx - chunk_start\n",
    "        \n",
    "        blob_id_time = chunk_data.isel({tracker.timedim: relative_time_idx})\n",
    "        blob_id_time_p1 = chunk_data.isel({tracker.timedim: relative_time_idx+1})\n",
    "        if relative_time_idx-1 >= 0:\n",
    "            blob_id_time_m1 = chunk_data.isel({tracker.timedim: relative_time_idx-1})\n",
    "        elif chunk_idx > 0:\n",
    "            blob_id_time_m1 = blob_id_field_unique.isel({tracker.timedim: chunk_start-1})\n",
    "        else:\n",
    "            blob_id_time_m1 = xr.full_like(blob_id_time, 0)\n",
    "        \n",
    "        child_mask_2d  = (blob_id_time == child_id).values\n",
    "        \n",
    "        # Find all pairs involving this Child Blob\n",
    "        child_mask = overlap_blobs_list[:, 1] == child_id\n",
    "        child_where = np.where(overlap_blobs_list[:, 1] == child_id)[0]  # Needed for assignment\n",
    "        merge_group = overlap_blobs_list[child_mask]\n",
    "        \n",
    "        # Get all Parents (LHS) Blobs that overlap with this Child Blob -- N.B. This is now generalised for N-parent merging !\n",
    "        parent_ids = merge_group[:, 0]\n",
    "        num_parents = len(parent_ids)\n",
    "        \n",
    "        # Make a new ID for the other Half of the Child Blob & Record in the Merge Ledger\n",
    "        new_blob_id = np.arange(next_new_id, next_new_id + (num_parents - 1), dtype=np.int32)\n",
    "        next_new_id += num_parents - 1\n",
    "        \n",
    "        # Replace the 2nd+ Child in the Overlap Blobs List with the new Child ID\n",
    "        overlap_blobs_list[child_where[1:], 1] = new_blob_id    #overlap_blobs_list[child_mask, 1][1:] = new_blob_id\n",
    "        child_ids = np.concatenate((np.array([child_id]), new_blob_id))    #np.array([child_id, new_blob_id])\n",
    "        \n",
    "        # Record merge event data\n",
    "        merge_times.append(chunk_data.isel({tracker.timedim: relative_time_idx}).time.values)\n",
    "        merge_child_ids.append(child_ids)\n",
    "        merge_parent_ids.append(parent_ids)\n",
    "        merge_areas.append(overlap_blobs_list[child_mask, 2])\n",
    "        \n",
    "        ### Relabel the Original Child Blob ID Field to account for the New ID:\n",
    "        parent_centroids = blob_props.sel(ID=parent_ids).centroid.values.T  # (y, x), [:,0] are the y's\n",
    "        \n",
    "        if tracker.centroid_partitioning:\n",
    "            # --> For every (Original) Child Cell in the ID Field, Measure the Distance to the Centroids of the Parents\n",
    "            distances = wrapped_euclidian_parallel(child_mask_2d, parent_centroids, Nx)  # **Deals with wrapping**\n",
    "            \n",
    "            # Assign the new ID to each cell based on the closest parent\n",
    "            new_labels = child_ids[np.argmin(distances, axis=1)]\n",
    "        \n",
    "        else:\n",
    "            # --> For every (Original) Child Cell in the ID Field, Find the closest parent cell\n",
    "            parent_masks = np.zeros((len(parent_ids), blob_id_time.shape[0], blob_id_time.shape[1]), dtype=bool)\n",
    "            for idx, parent_id in enumerate(parent_ids):\n",
    "                parent_masks[idx] = (blob_id_time_m1 == parent_id).values\n",
    "            \n",
    "            # Calculate typical blob size to set max_distance\n",
    "            max_area = np.max(blob_props.sel(ID=parent_ids).area.values)\n",
    "            max_distance = int(np.sqrt(max_area) * 2.0)  # Use 2x the max blob radius\n",
    "            max_distance = max(max_distance, 20)  # Set minimum threshold...\n",
    "            \n",
    "            new_labels = get_nearest_parent_labels(\n",
    "                child_mask_2d,\n",
    "                parent_masks, \n",
    "                child_ids,\n",
    "                parent_centroids,\n",
    "                Nx,\n",
    "                max_distance=max_distance\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ## Update values in child_time_idx and assign the updated slice back to the original DataArray\n",
    "        temp = np.zeros_like(blob_id_time)\n",
    "        temp[child_mask_2d] = new_labels\n",
    "        blob_id_time = blob_id_time.where(~child_mask_2d, temp)\n",
    "        ## ** Update directly into the chunk\n",
    "        chunk_data[{tracker.timedim: relative_time_idx}] = blob_id_time\n",
    "        \n",
    "        ## Add new entries to time_index_map for each of new_blob_id corresponding to the current time index\n",
    "        time_index_map.update({new_id: child_time_idx for new_id in new_blob_id})\n",
    "        \n",
    "        ## Update the Properties of the N Children Blobs\n",
    "        new_child_props = tracker.calculate_blob_properties(blob_id_time, properties=['area', 'centroid'])\n",
    "        \n",
    "        # Update the blob_props DataArray:  (but first, check if the original Children still exists)\n",
    "        if child_id in new_child_props.ID:  # Update the entry\n",
    "            blob_props.loc[dict(ID=child_id)] = new_child_props.sel(ID=child_id)\n",
    "        else:  # Delte child_id:  The blob has split/morphed such that it doesn't get a partition of this child...\n",
    "            blob_props = blob_props.drop_sel(ID=child_id)  # N.B.: This means that the IDs are no longer continuous...\n",
    "            print(f\"Deleted child_id {child_id} because parents have split/morphed in the meantime...\")\n",
    "        # Add the properties for the N-1 other new child ID\n",
    "        new_blob_ids_still = new_child_props.ID.where(new_child_props.ID.isin(new_blob_id), drop=True).ID\n",
    "        blob_props = xr.concat([blob_props, new_child_props.sel(ID=new_blob_ids_still)], dim='ID')\n",
    "        missing_ids = set(new_blob_id) - set(new_blob_ids_still.values)\n",
    "        if len(missing_ids) > 0:\n",
    "            print(f\"Missing newly created child_ids {missing_ids} because parents have split/morphed in the meantime...\")\n",
    "\n",
    "        \n",
    "        ## Finally, Re-assess all of the Parent IDs (LHS) equal to the (original) child_id\n",
    "        \n",
    "        # Look at the overlap IDs between the original child_id and the next time-step, and also the new_blob_id and the next time-step\n",
    "        new_overlaps = tracker.check_overlap_slice(blob_id_time.values, blob_id_time_p1.values)\n",
    "        new_child_overlaps_list = new_overlaps[(new_overlaps[:, 0] == child_id) | np.isin(new_overlaps[:, 0], new_blob_id)]\n",
    "        \n",
    "        # _Before_ replacing the overlap_blobs_list, we need to re-assess the overlap fractions of just the new_child_overlaps_list\n",
    "        areas_0 = blob_props['area'].sel(ID=new_child_overlaps_list[:, 0]).values\n",
    "        areas_1 = blob_props['area'].sel(ID=new_child_overlaps_list[:, 1]).values\n",
    "        min_areas = np.minimum(areas_0, areas_1)\n",
    "        overlap_fractions = new_child_overlaps_list[:, 2].astype(float) / min_areas\n",
    "        new_child_overlaps_list = new_child_overlaps_list[overlap_fractions >= tracker.overlap_threshold]\n",
    "        \n",
    "        # Replace the lines in the overlap_blobs_list where (original) child_id is on the LHS, with these new pairs in new_child_overlaps_list\n",
    "        child_mask_LHS = overlap_blobs_list[:, 0] == child_id\n",
    "        overlap_blobs_list = np.concatenate([overlap_blobs_list[~child_mask_LHS], new_child_overlaps_list])\n",
    "        \n",
    "        \n",
    "        ## Finally, _FINALLY_, we need to ensure that of the new children blobs we made, they only overlap with their respective parent...\n",
    "        new_unique_children, new_children_counts = np.unique(new_child_overlaps_list[:, 1], return_counts=True)\n",
    "        new_merging_blobs = new_unique_children[new_children_counts > 1]\n",
    "        if new_merging_blobs.size > 0:\n",
    "            \n",
    "            if relative_time_idx + 1 < chunk_data.sizes[tracker.timedim]-1:  # If there is a next time-step in this chunk\n",
    "                for new_child_id in new_merging_blobs:\n",
    "                    if new_child_id not in blobs_to_process: # We aren't already going to assess this blob\n",
    "                        blobs_to_process.insert(0, new_child_id)\n",
    "            \n",
    "            else: # This is out of our current jurisdiction: Defer this reassessment to the beginning of the next chunk\n",
    "                future_chunk_merges.extend(new_merging_blobs)\n",
    "        \n",
    "    \n",
    "    # Update the full dask DataArray with this processed chunk\n",
    "    blob_id_field_unique[{\n",
    "        tracker.timedim: slice(chunk_start, chunk_end-1)  # cf. above definition of chunk_end for why we need -1\n",
    "    }] = chunk_data[:(chunk_end-1-chunk_start)]\n",
    "    \n",
    "    \n",
    "    ### Process the Merge Events\n",
    "    max_parents = max(len(ids) for ids in merge_parent_ids)\n",
    "    max_children = max(len(ids) for ids in merge_child_ids)\n",
    "    \n",
    "    # Convert lists to padded numpy arrays\n",
    "    parent_ids_array = np.full((len(merge_parent_ids), max_parents), -1, dtype=np.int32)\n",
    "    child_ids_array = np.full((len(merge_child_ids), max_children), -1, dtype=np.int32)\n",
    "    overlap_areas_array = np.full((len(merge_areas), max_parents), -1, dtype=np.int32)\n",
    "\n",
    "    for i, parents in enumerate(merge_parent_ids):\n",
    "        parent_ids_array[i, :len(parents)] = parents\n",
    "    \n",
    "    for i, children in enumerate(merge_child_ids):\n",
    "        child_ids_array[i, :len(children)] = children\n",
    "    \n",
    "    for i, areas in enumerate(merge_areas):\n",
    "        overlap_areas_array[i, :len(areas)] = areas\n",
    "    \n",
    "    # Create merge events dataset\n",
    "    merge_events = xr.Dataset(\n",
    "        {\n",
    "            'merge_parent_IDs': (('merge_ID', 'parent_idx'), parent_ids_array),\n",
    "            'merge_child_IDs': (('merge_ID', 'child_idx'), child_ids_array),\n",
    "            'merge_overlap_areas': (('merge_ID', 'parent_idx'), overlap_areas_array),\n",
    "            'merge_time': ('merge_ID', merge_times),\n",
    "            'merge_n_parents': ('merge_ID', np.array([len(p) for p in merge_parent_ids])),\n",
    "            'merge_n_children': ('merge_ID', np.array([len(c) for c in merge_child_ids]))\n",
    "        },\n",
    "        attrs={\n",
    "            'fill_value': -1\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked.nc'\n",
    "blobs.to_netcdf(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
