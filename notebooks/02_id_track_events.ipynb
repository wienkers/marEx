{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Filter out small objects -- area less than the `area_filter_quartile` of the distribution of objects.\n",
    "3. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "4. Manually connect objects across time, applying Sun et al. 2023 criteria:\n",
    "    - Connected Blobs must overlap by at least `overlap_threshold=50%` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but split the blob based on parent centroid locality.\n",
    "5. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "\n",
    "N.B.: Exploits parallelised `Dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of Daily outputs at 0.25Â° resolution takes ~6 minutes on 128 total cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63fce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~14 minutes per 1000 time steps on 32 workers (2 threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 15.74 GB\n",
      "Hostname is  l40200\n",
      "Forward Port = l40200:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary.zarr'\n",
    "chunk_size = {'time': 25, 'lat': -1, 'lon': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93364dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Binary Features and Modify Mask\n",
    "\n",
    "extreme_bin = ds.extreme_events.isel(time=slice(0, 500))\n",
    "mask = ds.mask.where((ds.lat<85) & (ds.lat>-90), other=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.5\n",
    "filling_radius = 8\n",
    "allow_merging = True\n",
    "nn_partitioning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filling holes.\n",
      "Finished filtering small blobs.\n",
      "Finished blob identification.\n",
      "Finished calculating blob properties.\n",
      "Finished finding overlapping blobs.\n",
      "Processed chunk 0 of 20\n"
     ]
    }
   ],
   "source": [
    "# Spot the Blobs\n",
    "\n",
    "tracker = blob.Spotter(extreme_bin, mask, R_fill=filling_radius, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, nn_partitioning=nn_partitioning)\n",
    "blobs = tracker.run()\n",
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked.nc'\n",
    "blobs.to_netcdf(file_name, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb78322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_filled = tracker.fill_holes()\n",
    "print('Finished filling holes.')\n",
    "\n",
    "# Remove Small Objects\n",
    "data_bin_filtered, area_threshold, blob_areas, N_blobs_unfiltered = tracker.filter_small_blobs(data_bin_filled)\n",
    "print('Finished filtering small blobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin = data_bin_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field, _ = tracker.identify_blobs(data_bin, time_connectivity=False)\n",
    "print('Finished blob identification.')\n",
    "\n",
    "# Calculate Properties of each Blob\n",
    "blob_props = tracker.calculate_blob_properties(blob_id_field, properties=['area', 'centroid'])\n",
    "print('Finished calculating blob properties.')\n",
    "\n",
    "# Compile List of Overlapping Blob ID Pairs Across Time\n",
    "overlap_blobs_list = tracker.find_overlapping_blobs(blob_id_field)  # List of overlapping blob pairs\n",
    "print('Finished finding overlapping blobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_merged_blob_id_field_unique, merged_blobs_props, split_merged_blobs_list, merge_events = tracker.split_and_merge_blobs(blob_id_field, blob_props, overlap_blobs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask_image.ndmeasure import label\n",
    "from skimage.measure import regionprops_table\n",
    "from dask_image.ndmorph import binary_closing as binary_closing_dask\n",
    "from dask_image.ndmorph import binary_opening as binary_opening_dask\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from dask import persist\n",
    "from dask.base import is_dask_collection\n",
    "from numba import jit, prange\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_id_field_unique = split_merged_blob_id_field_unique\n",
    "blobs_props = merged_blobs_props\n",
    "overlap_blobs_list = split_merged_blobs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = np.unique(overlap_blobs_list) # 1D sorted unique\n",
    "\n",
    "# Create a mapping from ID to indices\n",
    "ID_to_index = {ID: index for index, ID in enumerate(IDs)}\n",
    "\n",
    "# Convert overlap pairs to indices\n",
    "overlap_pairs_indices = np.array([(ID_to_index[pair[0]], ID_to_index[pair[1]]) for pair in overlap_blobs_list])\n",
    "\n",
    "# Create a sparse matrix representation of the graph\n",
    "n = len(IDs)\n",
    "row_indices, col_indices = overlap_pairs_indices.T\n",
    "data = np.ones(len(overlap_pairs_indices))\n",
    "graph = csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "\n",
    "# Solve the graph to determine connected components\n",
    "num_components, component_IDs = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "\n",
    "# Group IDs by their component index\n",
    "ID_clusters = [[] for _ in range(num_components)]\n",
    "for ID, component_ID in zip(IDs, component_IDs):\n",
    "    ID_clusters[component_ID].append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f43ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int32 = np.iinfo(np.int32).min\n",
    "max_old_ID = blob_id_field_unique.max().compute().data\n",
    "ID_to_cluster_index_array = np.full(max_old_ID + 1, min_int32, dtype=np.int32)\n",
    "\n",
    "# Fill the lookup array with cluster indices\n",
    "for index, cluster in enumerate(ID_clusters):\n",
    "    for ID in cluster:\n",
    "        ID_to_cluster_index_array[ID] = np.int32(index+1) # Because these are the connected IDs, there are many fewer!\n",
    "                                                            #  Add 1 so that ID = 0 is still invalid/no object\n",
    "\n",
    "# N.B.: **Need to pass da into apply_ufunc, otherwise it doesn't manage the memory correctly with large shared-mem numpy arrays**\n",
    "ID_to_cluster_index_da = xr.DataArray(ID_to_cluster_index_array, dims='ID', coords={'ID': np.arange(max_old_ID + 1)})\n",
    "\n",
    "def map_IDs_to_indices(block, ID_to_cluster_index_array):\n",
    "    mask = block > 0\n",
    "    new_block = np.zeros_like(block, dtype=np.int32)\n",
    "    new_block[mask] = ID_to_cluster_index_array[block[mask]]\n",
    "    return new_block\n",
    "\n",
    "split_merged_relabeled_blob_id_field = xr.apply_ufunc(\n",
    "    map_IDs_to_indices,\n",
    "    blob_id_field_unique, \n",
    "    ID_to_cluster_index_da,\n",
    "    input_core_dims=[[tracker.ydim, tracker.xdim],['ID']],\n",
    "    output_core_dims=[[tracker.ydim, tracker.xdim]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[np.int32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c74618",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_merged_relabeled_blob_id_field = split_merged_relabeled_blob_id_field.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relabel the blobs_props to match the new IDs (and add time dimension!)\n",
    "\n",
    "max_new_ID = num_components + 1  # New IDs range from 0 to max_new_ID...\n",
    "new_ids = np.arange(1, max_new_ID+1, dtype=np.int32)\n",
    "\n",
    "# New blobs_props DataSet Structure\n",
    "blobs_props_extended = xr.Dataset(coords={\n",
    "    'ID': new_ids,\n",
    "    tracker.timedim: blob_id_field_unique[tracker.timedim]\n",
    "})\n",
    "\n",
    "## Create a mapping from new IDs to the original IDs _at the corresponding time_\n",
    "valid_new_ids = (split_merged_relabeled_blob_id_field > 0)      \n",
    "original_ids = blob_id_field_unique.where(valid_new_ids).stack(z=(tracker.ydim, tracker.xdim), create_index=False)\n",
    "new_ids_field = split_merged_relabeled_blob_id_field.where(valid_new_ids).stack(z=(tracker.ydim, tracker.xdim), create_index=False)\n",
    "\n",
    "id_mapping = xr.Dataset({\n",
    "    'original_id': original_ids,\n",
    "    'new_id': new_ids_field\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad660eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ THIS MAKES MANY MANY TASKSSSSSSSS\n",
    "\n",
    "\n",
    "\n",
    "# transformed_arrays = []\n",
    "# for new_id in new_ids:\n",
    "    \n",
    "#     mask = id_mapping.new_id == new_id\n",
    "#     mask_time = mask.any('z')\n",
    "    \n",
    "#     original_ids = id_mapping.original_id.where(mask, 0).max(dim='z').where(mask_time, 0)\n",
    "    \n",
    "#     transformed_arrays.append(original_ids)\n",
    "\n",
    "# global_id_mapping = xr.concat(transformed_arrays, dim='new_id').assign_coords(new_id=new_ids).rename({'new_id': 'ID'}).astype(np.int32).compute()\n",
    "# blobs_props_extended['global_ID'] = global_id_mapping\n",
    "# # N.B.: Now, e.g. global_id_mapping.sel(ID=10) --> Given the new ID (10), returns corresponding original_id at every time\n",
    "\n",
    "\n",
    "\n",
    "def map_ids_chunk(original_ids, new_ids_field, target_new_id):\n",
    "    \"\"\"Process a single chunk/new_id combination.\"\"\"\n",
    "    mask = new_ids_field == target_new_id\n",
    "    if not np.any(mask):\n",
    "        return np.zeros(1, dtype=np.int32)\n",
    "    return np.array([np.max(original_ids[mask])], dtype=np.int32)\n",
    "\n",
    "# Replace your loop with this:\n",
    "result = xr.apply_ufunc(\n",
    "    map_ids_chunk,\n",
    "    id_mapping.original_id,\n",
    "    id_mapping.new_id,\n",
    "    xr.DataArray(new_ids, dims='new_id'),\n",
    "    input_core_dims=[['z'], ['z'], []],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[np.int32],\n",
    "    dask_gufunc_kwargs={'allow_rechunk': True}\n",
    ")\n",
    "\n",
    "# Convert to the expected format\n",
    "global_id_mapping = (result\n",
    "    .assign_coords(new_id=new_ids)\n",
    "    .rename({'new_id': 'ID'})\n",
    "    .astype(np.int32)\n",
    "    .compute())\n",
    "\n",
    "blobs_props_extended['global_ID'] = global_id_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6dd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e71e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Transfer and transform all variables from original blobs_props:\n",
    "        \n",
    "# Add a value of ID = 0 to this coordinate ID\n",
    "dummy = blobs_props.isel(ID=0) * np.nan\n",
    "blobs_props = xr.concat([dummy.assign_coords(ID=0), blobs_props], dim='ID')\n",
    "\n",
    "\n",
    "for var_name in blobs_props.data_vars:\n",
    "    \n",
    "    temp = (blobs_props[var_name]\n",
    "                        .sel(ID=global_id_mapping.rename({'ID':'new_id'}))\n",
    "                        .drop_vars('ID').rename({'new_id':'ID'}))\n",
    "    \n",
    "    if var_name == 'ID':\n",
    "        temp = temp.astype(np.int32)\n",
    "    else:\n",
    "        temp = temp.astype(np.float32)\n",
    "        \n",
    "    blobs_props_extended[var_name] = temp\n",
    "\n",
    "\n",
    "# Add start and end time indices for each ID\n",
    "valid_presence = blobs_props_extended['global_ID'] > 0  # Where we have valid data\n",
    "\n",
    "blobs_props_extended['presence'] = valid_presence\n",
    "blobs_props_extended['time_start'] = valid_presence.time[valid_presence.argmax(dim=tracker.timedim)]\n",
    "blobs_props_extended['time_end'] = valid_presence.time[(valid_presence.sizes[tracker.timedim] - 1) - (valid_presence[::-1]).argmax(dim=tracker.timedim)]\n",
    "\n",
    "# Combine blobs_props_extended with split_merged_relabeled_blob_id_field\n",
    "split_merged_relabeled_blobs_ds = xr.merge([split_merged_relabeled_blob_id_field.rename('ID_field'), \n",
    "                                            blobs_props_extended])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_merged_relabeled_blobs_ds = split_merged_relabeled_blobs_ds.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
