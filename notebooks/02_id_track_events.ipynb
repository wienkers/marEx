{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d3a1a",
   "metadata": {},
   "source": [
    "# Identify & Track Marine Heatwaves using `spot_the_blOb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61028733",
   "metadata": {},
   "source": [
    "## Processing Steps:\n",
    "1. Fill holes in the binary data, using `dask_image.ndmorph` -- up to `R_fill` cells in radius.\n",
    "2. Filter out small objects -- area less than the bottom `area_filter_quartile` of the size distribution of objects.\n",
    "3. Identify objects in the binary data, using `dask_image.ndmeasure`.\n",
    "4. Manually connect objects across time, applying the following criteria for splitting, merging, and persistence:\n",
    "    - Connected Blobs must overlap by at least fraction `overlap_threshold` of the smaller blob.\n",
    "    - Merged Blobs retain their original ID, but partition the child blob based on the parent of the _nearest-neighbour_ cell. \n",
    "5. Cluster and reduce the final object ID graph using `scipy.sparse.csgraph.connected_components`.\n",
    "6. Map the tracked objects into ID-time space for convenient analysis.\n",
    "\n",
    "N.B.: Exploits parallelised `dask` operations with optimised chunking using `flox` for memory efficiency and speed \\\n",
    "N.N.B.: This example using 40 years of _daily_ outputs at 0.25Â° resolution on 32 cores takes \n",
    "- Standard (i.e. Scannell et al., which involves no merge/split criteria or tracking):  ~2 minutes\n",
    "- Full Split/Merge Thresholding & Merger Tracking:  ~1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b337539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from getpass import getuser\n",
    "from pathlib import Path\n",
    "\n",
    "import spot_the_blOb as blob\n",
    "import spot_the_blOb.helper as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per Worker: 15.74 GB\n",
      "Hostname is  l40225\n",
      "Forward Port = l40225:8787\n",
      "Dashboard Link: localhost:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start Dask Cluster\n",
    "client = hpc.StartLocalCluster(n_workers=32, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a21e23-427a-4f6f-9a91-fa2ae628f148",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Pre-processed Data (cf. `01_preprocess_extremes.ipynb`)\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'extreme_events_binary.zarr'\n",
    "chunk_size = {'time': 25, 'lat': -1, 'lon': -1}\n",
    "ds = xr.open_zarr(str(file_name), chunks=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93364dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Binary Features and Modify Mask\n",
    "\n",
    "extreme_bin = ds.extreme_events#.isel(time=slice(0, 2000))\n",
    "mask = ds.mask.where((ds.lat<85) & (ds.lat>-90), other=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7e8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Parameters\n",
    "\n",
    "drop_area_quartile = 0.5\n",
    "filling_radius = 8\n",
    "allow_merging = True     # Allow blobs to split/merge. Keeps track of merge events & unique IDs.\n",
    "overlap_threshold = 0.5  # Overlap threshold for merging blobs. If overlap < threshold, blobs keep independent IDs.\n",
    "nn_partitioning = True   # Use new NN method to partition merged children blobs. If False, reverts to old method of Di Sun et al. 2023..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7934cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filling holes.\n",
      "Finished filtering small blobs.\n",
      "Finished blob identification.\n",
      "Finished calculating blob properties.\n",
      "Finished finding overlapping blobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/b382615/opt/anaconda3/lib/python3.10/site-packages/dask/array/core.py:4918: PerformanceWarning: Increasing number of chunks by factor of 15\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing splitting and merging in chunk 0 of 556\n",
      "Processing splitting and merging in chunk 25 of 556\n",
      "Missing newly created child_ids {151981} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 50 of 556\n",
      "Missing newly created child_ids {153469} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 75 of 556\n",
      "Missing newly created child_ids {154794} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 100 of 556\n",
      "Processing splitting and merging in chunk 125 of 556\n",
      "Missing newly created child_ids {157565} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 150 of 556\n",
      "Processing splitting and merging in chunk 175 of 556\n",
      "Missing newly created child_ids {160172} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 200 of 556\n",
      "Processing splitting and merging in chunk 225 of 556\n",
      "Missing newly created child_ids {163076} because parents have split/morphed in the meantime...\n",
      "Processing splitting and merging in chunk 250 of 556\n",
      "Processing splitting and merging in chunk 275 of 556\n",
      "Processing splitting and merging in chunk 300 of 556\n",
      "Processing splitting and merging in chunk 325 of 556\n",
      "Missing newly created child_ids {166814} because parents have split/morphed in the meantime...\n",
      "Missing newly created child_ids {167054} because parents have split/morphed in the meantime...\n"
     ]
    }
   ],
   "source": [
    "# Spot the Blobs\n",
    "\n",
    "tracker = blob.Spotter(extreme_bin, mask, R_fill=filling_radius, area_filter_quartile=drop_area_quartile, \n",
    "                       allow_merging=allow_merging, overlap_threshold=overlap_threshold, nn_partitioning=nn_partitioning)\n",
    "blobs = tracker.run()\n",
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tracked Blobs\n",
    "\n",
    "file_name = Path('/scratch') / getuser()[0] / getuser() / 'mhws' / 'MHWs_tracked.nc'\n",
    "blobs.to_netcdf(file_name, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
